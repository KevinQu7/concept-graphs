[
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "Controller",
        "importPath": "ai2thor.controller",
        "description": "ai2thor.controller",
        "isExtraImport": true,
        "detail": "ai2thor.controller",
        "documentation": {}
    },
    {
        "label": "Controller",
        "importPath": "ai2thor.controller",
        "description": "ai2thor.controller",
        "isExtraImport": true,
        "detail": "ai2thor.controller",
        "documentation": {}
    },
    {
        "label": "Controller",
        "importPath": "ai2thor.controller",
        "description": "ai2thor.controller",
        "isExtraImport": true,
        "detail": "ai2thor.controller",
        "documentation": {}
    },
    {
        "label": "Controller",
        "importPath": "ai2thor.controller",
        "description": "ai2thor.controller",
        "isExtraImport": true,
        "detail": "ai2thor.controller",
        "documentation": {}
    },
    {
        "label": "parse_object_receptacle_mapping",
        "importPath": "conceptgraph.ai2thor.utils",
        "description": "conceptgraph.ai2thor.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.ai2thor.utils",
        "documentation": {}
    },
    {
        "label": "compute_pose",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_posrot",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_agent_pose_from_event",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_camera_pose_from_event",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_top_down_frame",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_intrinsics",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_pose",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_scene",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "sample_pose_uniform",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "sample_pose_random",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "make_subplots",
        "importPath": "plotly.subplots",
        "description": "plotly.subplots",
        "isExtraImport": true,
        "detail": "plotly.subplots",
        "documentation": {}
    },
    {
        "label": "plotly.graph_objects",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "plotly.graph_objects",
        "description": "plotly.graph_objects",
        "detail": "plotly.graph_objects",
        "documentation": {}
    },
    {
        "label": "abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "abc",
        "description": "abc",
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "imageio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imageio",
        "description": "imageio",
        "detail": "imageio",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "natsort",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "natsort",
        "description": "natsort",
        "detail": "natsort",
        "documentation": {}
    },
    {
        "label": "natsorted",
        "importPath": "natsort",
        "description": "natsort",
        "isExtraImport": true,
        "detail": "natsort",
        "documentation": {}
    },
    {
        "label": "natsorted",
        "importPath": "natsort",
        "description": "natsort",
        "isExtraImport": true,
        "detail": "natsort",
        "documentation": {}
    },
    {
        "label": "natsorted",
        "importPath": "natsort",
        "description": "natsort",
        "isExtraImport": true,
        "detail": "natsort",
        "documentation": {}
    },
    {
        "label": "conceptgraphs_datautils",
        "importPath": "conceptgraph.dataset",
        "description": "conceptgraph.dataset",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset",
        "documentation": {}
    },
    {
        "label": "relative_transformation",
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "rotation_matrix_to_quaternion",
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "rotation_matrix_to_quaternion",
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "rotation_matrix_to_quaternion",
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "quaternion_to_rotation_matrix",
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "rotation_matrix_to_quaternion",
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "RGBDImages",
        "importPath": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "description": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "prjson",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_det_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_exp_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_vis_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_hydra_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "ObjectClasses",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "ObjectClasses",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_det_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_exp_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "load_saved_detections",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "load_saved_hydra_json_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_detection_results",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_hydra_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_pointcloud",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "should_exit_early",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_vis_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "cfg_to_dict",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "check_run_detections",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "ObjectClasses",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "find_existing_image_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_det_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_exp_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_stream_data_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_vlm_annotated_image_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "handle_rerun_saving",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "load_saved_detections",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "load_saved_hydra_json_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "make_vlm_edges_and_captions",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_detection_results",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_hydra_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_objects_for_frame",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_pointcloud",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "should_exit_early",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "vis_render_image",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_vis_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "cfg_to_dict",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "check_run_detections",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "ObjectClasses",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_det_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_exp_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "load_saved_detections",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "load_saved_hydra_json_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_detection_results",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_hydra_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_pointcloud",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "should_exit_early",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_vis_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "cfg_to_dict",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "check_run_detections",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "ObjectClasses",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "find_existing_image_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_det_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_exp_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_vlm_annotated_image_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "handle_rerun_saving",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "load_saved_detections",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "load_saved_hydra_json_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "make_vlm_edges_and_captions",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_detection_results",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_edge_json",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_hydra_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_obj_json",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_objects_for_frame",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_pointcloud",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "should_exit_early",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "vis_render_image",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_vis_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "cfg_to_dict",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "check_run_detections",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "ObjectClasses",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_det_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_exp_out_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "load_saved_hydra_json_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_hydra_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "should_exit_early",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "find_existing_image_path",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "liblzfse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "liblzfse",
        "description": "liblzfse",
        "detail": "liblzfse",
        "documentation": {}
    },
    {
        "label": "png",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "png",
        "description": "png",
        "detail": "png",
        "documentation": {}
    },
    {
        "label": "tyro",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tyro",
        "description": "tyro",
        "detail": "tyro",
        "documentation": {}
    },
    {
        "label": "dataclasses",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dataclasses",
        "description": "dataclasses",
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "Record3DStream",
        "importPath": "record3d",
        "description": "record3d",
        "isExtraImport": true,
        "detail": "record3d",
        "documentation": {}
    },
    {
        "label": "Record3DStream",
        "importPath": "record3d",
        "description": "record3d",
        "isExtraImport": true,
        "detail": "record3d",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "matplotlib.patches",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.patches",
        "description": "matplotlib.patches",
        "detail": "matplotlib.patches",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "make_axes_locatable",
        "importPath": "mpl_toolkits.axes_grid1",
        "description": "mpl_toolkits.axes_grid1",
        "isExtraImport": true,
        "detail": "mpl_toolkits.axes_grid1",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPImageProcessor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPVisionModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoProcessor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlavaForConditionalGeneration",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BaseModelOutputWithPast",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "CausalLMOutputWithPast",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "SeparatorStyle",
        "importPath": "llava.conversation",
        "description": "llava.conversation",
        "isExtraImport": true,
        "detail": "llava.conversation",
        "documentation": {}
    },
    {
        "label": "conv_templates",
        "importPath": "llava.conversation",
        "description": "llava.conversation",
        "isExtraImport": true,
        "detail": "llava.conversation",
        "documentation": {}
    },
    {
        "label": "KeywordsStoppingCriteria",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "disable_torch_init",
        "importPath": "llava.utils",
        "description": "llava.utils",
        "isExtraImport": true,
        "detail": "llava.utils",
        "documentation": {}
    },
    {
        "label": "gc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gc",
        "description": "gc",
        "detail": "gc",
        "documentation": {}
    },
    {
        "label": "gzip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gzip",
        "description": "gzip",
        "detail": "gzip",
        "documentation": {}
    },
    {
        "label": "SimpleNamespace",
        "importPath": "types",
        "description": "types",
        "isExtraImport": true,
        "detail": "types",
        "documentation": {}
    },
    {
        "label": "wrap",
        "importPath": "textwrap",
        "description": "textwrap",
        "isExtraImport": true,
        "detail": "textwrap",
        "documentation": {}
    },
    {
        "label": "rich",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "rich",
        "description": "rich",
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "csr_matrix",
        "importPath": "scipy.sparse",
        "description": "scipy.sparse",
        "isExtraImport": true,
        "detail": "scipy.sparse",
        "documentation": {}
    },
    {
        "label": "connected_components",
        "importPath": "scipy.sparse.csgraph",
        "description": "scipy.sparse.csgraph",
        "isExtraImport": true,
        "detail": "scipy.sparse.csgraph",
        "documentation": {}
    },
    {
        "label": "minimum_spanning_tree",
        "importPath": "scipy.sparse.csgraph",
        "description": "scipy.sparse.csgraph",
        "isExtraImport": true,
        "detail": "scipy.sparse.csgraph",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "DetectionList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "DetectionList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapEdgeMapping",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapEdgeMapping",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapEdgeMapping",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "DetectionList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "to_tensor",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "create_or_load_colors",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_obj2_into_obj1",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "denoise_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "gobs_to_detection_list",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "get_bounding_box",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "init_process_pcd",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "make_detection_list_from_pcd_and_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "denoise_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "detections_to_obj_pcd_and_bbox",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "prepare_objects_save_vis",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_cfg",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_pcd",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "processing_needed",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "resize_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "compute_overlap_matrix_general",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_obj2_into_obj1",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "compute_overlap_matrix_2set",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "get_bounding_box",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "init_process_pcd",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "make_detection_list_from_pcd_and_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "denoise_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "detections_to_obj_pcd_and_bbox",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "prepare_objects_save_vis",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_cfg",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_edges",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_pcd",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "processing_needed",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "resize_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "get_bounding_box",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "init_process_pcd",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "make_detection_list_from_pcd_and_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "denoise_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "detections_to_obj_pcd_and_bbox",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "prepare_objects_save_vis",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_cfg",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_pcd",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "processing_needed",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "resize_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "get_bounding_box",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "init_process_pcd",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "make_detection_list_from_pcd_and_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "denoise_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "detections_to_obj_pcd_and_bbox",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "prepare_objects_save_vis",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_cfg",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_edges",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_pcd",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "processing_needed",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "resize_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "make_detection_list_from_pcd_and_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "denoise_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "detections_to_obj_pcd_and_bbox",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "prepare_objects_save_vis",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_cfg",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "processing_needed",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "resize_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "prepare_objects_save_vis",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "zlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "zlib",
        "description": "zlib",
        "detail": "zlib",
        "documentation": {}
    },
    {
        "label": "SensorData",
        "importPath": "SensorData",
        "description": "SensorData",
        "isExtraImport": true,
        "detail": "SensorData",
        "documentation": {}
    },
    {
        "label": "PyQt5",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PyQt5",
        "description": "PyQt5",
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "gzip,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gzip.",
        "description": "gzip.",
        "detail": "gzip.",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "open3d",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "open3d",
        "description": "open3d",
        "detail": "open3d",
        "documentation": {}
    },
    {
        "label": "better_camera_frustum",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "LineMesh",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_slow_caption",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "get_random_colors",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_slow_caption",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_detections",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "LineMesh",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "OnlineObjectRenderer",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "OnlineObjectRenderer",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_from_frames",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast_on_depth",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_detections",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "OnlineObjectRenderer",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_from_frames",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast_on_depth",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_for_vlm",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_detections",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "OnlineObjectRenderer",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_from_frames",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast_on_depth",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_detections",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "OnlineObjectRenderer",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_from_frames",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast_on_depth",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_for_vlm",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_detections",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "OnlineObjectRenderer",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_from_frames",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "distinctipy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "distinctipy",
        "description": "distinctipy",
        "detail": "distinctipy",
        "documentation": {}
    },
    {
        "label": "Detections",
        "importPath": "supervision.detection.core",
        "description": "supervision.detection.core",
        "isExtraImport": true,
        "detail": "supervision.detection.core",
        "documentation": {}
    },
    {
        "label": "Color",
        "importPath": "supervision.draw.color",
        "description": "supervision.draw.color",
        "isExtraImport": true,
        "detail": "supervision.draw.color",
        "documentation": {}
    },
    {
        "label": "ColorPalette",
        "importPath": "supervision.draw.color",
        "description": "supervision.draw.color",
        "isExtraImport": true,
        "detail": "supervision.draw.color",
        "documentation": {}
    },
    {
        "label": "Color",
        "importPath": "supervision.draw.color",
        "description": "supervision.draw.color",
        "isExtraImport": true,
        "detail": "supervision.draw.color",
        "documentation": {}
    },
    {
        "label": "ColorPalette",
        "importPath": "supervision.draw.color",
        "description": "supervision.draw.color",
        "isExtraImport": true,
        "detail": "supervision.draw.color",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "open_clip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "open_clip",
        "description": "open_clip",
        "detail": "open_clip",
        "documentation": {}
    },
    {
        "label": "knn_points",
        "importPath": "chamferdist.chamfer",
        "description": "chamferdist.chamfer",
        "isExtraImport": true,
        "detail": "chamferdist.chamfer",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "gradslam.structures.pointclouds",
        "description": "gradslam.structures.pointclouds",
        "isExtraImport": true,
        "detail": "gradslam.structures.pointclouds",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "gradslam.structures.pointclouds",
        "description": "gradslam.structures.pointclouds",
        "isExtraImport": true,
        "detail": "gradslam.structures.pointclouds",
        "documentation": {}
    },
    {
        "label": "REPLICA_EXISTING_CLASSES",
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_CLASSES",
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_SCENE_IDS",
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_SCENE_IDS_",
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "compute_confmatrix",
        "importPath": "conceptgraph.utils.eval",
        "description": "conceptgraph.utils.eval",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.eval",
        "documentation": {}
    },
    {
        "label": "compute_pred_gt_associations",
        "importPath": "conceptgraph.utils.eval",
        "description": "conceptgraph.utils.eval",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.eval",
        "documentation": {}
    },
    {
        "label": "compute_metrics",
        "importPath": "conceptgraph.utils.eval",
        "description": "conceptgraph.utils.eval",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.eval",
        "documentation": {}
    },
    {
        "label": "CloudRendering",
        "importPath": "ai2thor.platform",
        "description": "ai2thor.platform",
        "isExtraImport": true,
        "detail": "ai2thor.platform",
        "documentation": {}
    },
    {
        "label": "rearrange_objects",
        "importPath": "conceptgraph.ai2thor.rearrange",
        "description": "conceptgraph.ai2thor.rearrange",
        "isExtraImport": true,
        "detail": "conceptgraph.ai2thor.rearrange",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "YOLO",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "YOLO",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "SAM",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "YOLO",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "SAM",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "YOLO",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "SAM",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "YOLO",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "SAM",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "YOLO",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "SAM",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "supervision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "supervision",
        "description": "supervision",
        "detail": "supervision",
        "documentation": {}
    },
    {
        "label": "compute_clip_features",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_clip_features_batched",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_clip_features_batched",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_clip_features_batched",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_clip_features_batched",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_clip_features_batched",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "conceptgraph.utils.pointclouds",
        "description": "conceptgraph.utils.pointclouds",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.pointclouds",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "conceptgraph.utils.pointclouds",
        "description": "conceptgraph.utils.pointclouds",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.pointclouds",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "conceptgraph.utils.pointclouds",
        "description": "conceptgraph.utils.pointclouds",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.pointclouds",
        "documentation": {}
    },
    {
        "label": "PointFusion",
        "importPath": "gradslam.slam.pointfusion",
        "description": "gradslam.slam.pointfusion",
        "isExtraImport": true,
        "detail": "gradslam.slam.pointfusion",
        "documentation": {}
    },
    {
        "label": "RGBDImages",
        "importPath": "gradslam.structures.rgbdimages",
        "description": "gradslam.structures.rgbdimages",
        "isExtraImport": true,
        "detail": "gradslam.structures.rgbdimages",
        "documentation": {}
    },
    {
        "label": "profile",
        "importPath": "line_profiler",
        "description": "line_profiler",
        "isExtraImport": true,
        "detail": "line_profiler",
        "documentation": {}
    },
    {
        "label": "hydra",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hydra",
        "description": "hydra",
        "detail": "hydra",
        "documentation": {}
    },
    {
        "label": "omegaconf",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "omegaconf",
        "description": "omegaconf",
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "LLaVaChat",
        "importPath": "conceptgraph.llava.llava_model",
        "description": "conceptgraph.llava.llava_model",
        "isExtraImport": true,
        "detail": "conceptgraph.llava.llava_model",
        "documentation": {}
    },
    {
        "label": "crop_image_pil",
        "importPath": "conceptgraph.utils.image",
        "description": "conceptgraph.utils.image",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.image",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "compute_2d_box_contained_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "mask_subtract_contained",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_iou_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_giou_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_iou_accurate_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_giou_accurate_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "mask_subtract_contained",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "mask_subtract_contained",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "mask_subtract_contained",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "mask_subtract_contained",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_iou",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_iou_accurate_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_iou_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "mask_subtract_contained",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "mask_subtract_contained",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "mask_subtract_contained",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_spatial_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_visual_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "aggregate_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "merge_detections_to_objects",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_spatial_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_visual_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "aggregate_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "match_detections_to_objects",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "merge_obj_matches",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_spatial_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_visual_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "aggregate_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "match_detections_to_objects",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "merge_obj_matches",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_spatial_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_visual_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "aggregate_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "match_detections_to_objects",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "merge_obj_matches",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_spatial_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_visual_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "aggregate_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "match_detections_to_objects",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "merge_obj_matches",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_spatial_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_visual_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "aggregate_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "match_detections_to_objects",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "merge_obj_matches",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "OptionalWandB",
        "importPath": "conceptgraph.utils.optional_wandb_wrapper",
        "description": "conceptgraph.utils.optional_wandb_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_wandb_wrapper",
        "documentation": {}
    },
    {
        "label": "OptionalWandB",
        "importPath": "conceptgraph.utils.optional_wandb_wrapper",
        "description": "conceptgraph.utils.optional_wandb_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_wandb_wrapper",
        "documentation": {}
    },
    {
        "label": "OptionalWandB",
        "importPath": "conceptgraph.utils.optional_wandb_wrapper",
        "description": "conceptgraph.utils.optional_wandb_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_wandb_wrapper",
        "documentation": {}
    },
    {
        "label": "OptionalWandB",
        "importPath": "conceptgraph.utils.optional_wandb_wrapper",
        "description": "conceptgraph.utils.optional_wandb_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_wandb_wrapper",
        "documentation": {}
    },
    {
        "label": "OptionalWandB",
        "importPath": "conceptgraph.utils.optional_wandb_wrapper",
        "description": "conceptgraph.utils.optional_wandb_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_wandb_wrapper",
        "documentation": {}
    },
    {
        "label": "DenoisingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "MappingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "MappingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "DenoisingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "MappingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "DenoisingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "MappingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "DenoisingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "MappingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "DenoisingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "DenoisingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "MappingTracker",
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "read_pinhole_camera_parameters",
        "importPath": "open3d.io",
        "description": "open3d.io",
        "isExtraImport": true,
        "detail": "open3d.io",
        "documentation": {}
    },
    {
        "label": "read_pinhole_camera_parameters",
        "importPath": "open3d.io",
        "description": "open3d.io",
        "isExtraImport": true,
        "detail": "open3d.io",
        "documentation": {}
    },
    {
        "label": "read_pinhole_camera_parameters",
        "importPath": "open3d.io",
        "description": "open3d.io",
        "isExtraImport": true,
        "detail": "open3d.io",
        "documentation": {}
    },
    {
        "label": "read_pinhole_camera_parameters",
        "importPath": "open3d.io",
        "description": "open3d.io",
        "isExtraImport": true,
        "detail": "open3d.io",
        "documentation": {}
    },
    {
        "label": "read_pinhole_camera_parameters",
        "importPath": "open3d.io",
        "description": "open3d.io",
        "isExtraImport": true,
        "detail": "open3d.io",
        "documentation": {}
    },
    {
        "label": "open3d.visualization.gui",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "open3d.visualization.gui",
        "description": "open3d.visualization.gui",
        "detail": "open3d.visualization.gui",
        "documentation": {}
    },
    {
        "label": "DemoApp",
        "importPath": "conceptgraph.utils.record3d_utils",
        "description": "conceptgraph.utils.record3d_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.record3d_utils",
        "documentation": {}
    },
    {
        "label": "scipy.ndimage",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.ndimage",
        "description": "scipy.ndimage",
        "detail": "scipy.ndimage",
        "documentation": {}
    },
    {
        "label": "OptionalReRun",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_annotated_image",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_camera",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_depth_image",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_edges",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_objs_pcd_and_bbox",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_rgb_image",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_vlm_image",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "OptionalReRun",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_annotated_image",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_camera",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_depth_image",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_edges",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_objs_pcd_and_bbox",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_rgb_image",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_vlm_image",
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "get_obj_rel_from_image_gpt4v",
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "get_openai_client",
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "consolidate_captions",
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "get_obj_rel_from_image_gpt4v",
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "get_openai_client",
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "get_obj_captions_from_image_gpt4v",
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "get_obj_rel_from_image_gpt4v",
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "vlm_extract_object_captions",
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "faiss",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "faiss",
        "description": "faiss",
        "detail": "faiss",
        "documentation": {}
    },
    {
        "label": "prior",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "prior",
        "description": "prior",
        "detail": "prior",
        "documentation": {}
    },
    {
        "label": "compose_transformations",
        "importPath": "kornia.geometry.linalg",
        "description": "kornia.geometry.linalg",
        "isExtraImport": true,
        "detail": "kornia.geometry.linalg",
        "documentation": {}
    },
    {
        "label": "inverse_transformation",
        "importPath": "kornia.geometry.linalg",
        "description": "kornia.geometry.linalg",
        "isExtraImport": true,
        "detail": "kornia.geometry.linalg",
        "documentation": {}
    },
    {
        "label": "cosine",
        "importPath": "scipy.spatial.distance",
        "description": "scipy.spatial.distance",
        "isExtraImport": true,
        "detail": "scipy.spatial.distance",
        "documentation": {}
    },
    {
        "label": "projutils",
        "importPath": "conceptgraph.utils",
        "description": "conceptgraph.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils",
        "documentation": {}
    },
    {
        "label": "structutils",
        "importPath": "conceptgraph.utils",
        "description": "conceptgraph.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "rearrange_objects",
        "kind": 2,
        "importPath": "conceptgraph.ai2thor.rearrange",
        "description": "conceptgraph.ai2thor.rearrange",
        "peekOfCode": "def rearrange_objects(\n    controller: Controller,\n    pickupable_move_ratio: float,\n    moveable_move_ratio: float,\n    random_seed: int | None = None,\n    reset: bool = False,\n):\n    '''\n    Rearrange objects in the scene. \n    '''",
        "detail": "conceptgraph.ai2thor.rearrange",
        "documentation": {}
    },
    {
        "label": "parse_object_receptacle_mapping",
        "kind": 2,
        "importPath": "conceptgraph.ai2thor.utils",
        "description": "conceptgraph.ai2thor.utils",
        "peekOfCode": "def parse_object_receptacle_mapping(controller: Controller):\n    # Parse the receptacle-object relationships\n    obj_list = controller.last_event.metadata[\"objects\"]\n    obj2receptacle = {}\n    receptacle2obj = {}\n    for obj in obj_list:\n        obj_id = obj['objectId']\n        parents = obj['parentReceptacles']\n        if parents is None:\n            continue",
        "detail": "conceptgraph.ai2thor.utils",
        "documentation": {}
    },
    {
        "label": "compute_position_dist",
        "kind": 2,
        "importPath": "conceptgraph.ai2thor.utils",
        "description": "conceptgraph.ai2thor.utils",
        "peekOfCode": "def compute_position_dist(\n    p0: Mapping[str, Any],\n    p1: Mapping[str, Any],\n    ignore_y: bool = False,\n    l1_dist: bool = False,\n) -> float:\n    \"\"\"Distance between two points of the form {\"x\": x, \"y\":y, \"z\":z\"}.\"\"\"\n    if l1_dist:\n        return (\n            abs(p0[\"x\"] - p1[\"x\"])",
        "detail": "conceptgraph.ai2thor.utils",
        "documentation": {}
    },
    {
        "label": "compute_rotation_dist",
        "kind": 2,
        "importPath": "conceptgraph.ai2thor.utils",
        "description": "conceptgraph.ai2thor.utils",
        "peekOfCode": "def compute_rotation_dist(a: Dict[str, float], b: Dict[str, float]):\n    \"\"\"Distance between rotations.\"\"\"\n    def deg_dist(d0: float, d1: float):\n        dist = (d0 - d1) % 360\n        return min(dist, 360 - dist)\n    return sum(deg_dist(a[k], b[k]) for k in [\"x\", \"y\", \"z\"])\ndef compute_angle_between_rotations(a: Dict[str, float], b: Dict[str, float]):\n    return np.abs(\n        (180 / (2 * math.pi))\n        * (",
        "detail": "conceptgraph.ai2thor.utils",
        "documentation": {}
    },
    {
        "label": "compute_angle_between_rotations",
        "kind": 2,
        "importPath": "conceptgraph.ai2thor.utils",
        "description": "conceptgraph.ai2thor.utils",
        "peekOfCode": "def compute_angle_between_rotations(a: Dict[str, float], b: Dict[str, float]):\n    return np.abs(\n        (180 / (2 * math.pi))\n        * (\n            Rotation.from_euler(\"xyz\", [a[k] for k in \"xyz\"], degrees=True)\n            * Rotation.from_euler(\"xyz\", [b[k] for k in \"xyz\"], degrees=True).inv()\n        ).as_rotvec()\n    ).sum()",
        "detail": "conceptgraph.ai2thor.utils",
        "documentation": {}
    },
    {
        "label": "normalize_image",
        "kind": 2,
        "importPath": "conceptgraph.dataset.conceptgraphs_datautils",
        "description": "conceptgraph.dataset.conceptgraphs_datautils",
        "peekOfCode": "def normalize_image(rgb: Union[torch.Tensor, np.ndarray]):\n    r\"\"\"Normalizes RGB image values from :math:`[0, 255]` range to :math:`[0, 1]` range.\n    Args:\n        rgb (torch.Tensor or numpy.ndarray): RGB image in range :math:`[0, 255]`\n    Returns:\n        torch.Tensor or numpy.ndarray: Normalized RGB image in range :math:`[0, 1]`\n    Shape:\n        - rgb: :math:`(*)` (any shape)\n        - Output: Same shape as input :math:`(*)`\n    \"\"\"",
        "detail": "conceptgraph.dataset.conceptgraphs_datautils",
        "documentation": {}
    },
    {
        "label": "channels_first",
        "kind": 2,
        "importPath": "conceptgraph.dataset.conceptgraphs_datautils",
        "description": "conceptgraph.dataset.conceptgraphs_datautils",
        "peekOfCode": "def channels_first(rgb: Union[torch.Tensor, np.ndarray]):\n    r\"\"\"Converts from channels last representation :math:`(*, H, W, C)` to channels first representation\n    :math:`(*, C, H, W)`\n    Args:\n        rgb (torch.Tensor or numpy.ndarray): :math:`(*, H, W, C)` ordering `(*, height, width, channels)`\n    Returns:\n        torch.Tensor or numpy.ndarray: :math:`(*, C, H, W)` ordering\n    Shape:\n        - rgb: :math:`(*, H, W, C)`\n        - Output: :math:`(*, C, H, W)`",
        "detail": "conceptgraph.dataset.conceptgraphs_datautils",
        "documentation": {}
    },
    {
        "label": "scale_intrinsics",
        "kind": 2,
        "importPath": "conceptgraph.dataset.conceptgraphs_datautils",
        "description": "conceptgraph.dataset.conceptgraphs_datautils",
        "peekOfCode": "def scale_intrinsics(\n    intrinsics: Union[np.ndarray, torch.Tensor],\n    h_ratio: Union[float, int],\n    w_ratio: Union[float, int],\n):\n    r\"\"\"Scales the intrinsics appropriately for resized frames where\n    :math:`h_\\text{ratio} = h_\\text{new} / h_\\text{old}` and :math:`w_\\text{ratio} = w_\\text{new} / w_\\text{old}`\n    Args:\n        intrinsics (numpy.ndarray or torch.Tensor): Intrinsics matrix of original frame\n        h_ratio (float or int): Ratio of new frame's height to old frame's height",
        "detail": "conceptgraph.dataset.conceptgraphs_datautils",
        "documentation": {}
    },
    {
        "label": "pointquaternion_to_homogeneous",
        "kind": 2,
        "importPath": "conceptgraph.dataset.conceptgraphs_datautils",
        "description": "conceptgraph.dataset.conceptgraphs_datautils",
        "peekOfCode": "def pointquaternion_to_homogeneous(\n    pointquaternions: Union[np.ndarray, torch.Tensor], eps: float = 1e-12\n):\n    r\"\"\"Converts 3D point and unit quaternions :math:`(t_x, t_y, t_z, q_x, q_y, q_z, q_w)` to\n    homogeneous transformations [R | t] where :math:`R` denotes the :math:`(3, 3)` rotation matrix and :math:`T`\n    denotes the :math:`(3, 1)` translation matrix:\n    .. math::\n        \\left[\\begin{array}{@{}c:c@{}}\n        R & T \\\\ \\hdashline\n        \\begin{array}{@{}ccc@{}}",
        "detail": "conceptgraph.dataset.conceptgraphs_datautils",
        "documentation": {}
    },
    {
        "label": "poses_to_transforms",
        "kind": 2,
        "importPath": "conceptgraph.dataset.conceptgraphs_datautils",
        "description": "conceptgraph.dataset.conceptgraphs_datautils",
        "peekOfCode": "def poses_to_transforms(poses: Union[np.ndarray, List[np.ndarray]]):\n    r\"\"\"Converts poses to transformations w.r.t. the first frame in the sequence having identity pose\n    Args:\n        poses (numpy.ndarray or list of numpy.ndarray): Sequence of poses in `numpy.ndarray` format.\n    Returns:\n        numpy.ndarray or list of numpy.ndarray: Sequence of frame to frame transformations where initial\n            frame is transformed to have identity pose.\n    Shape:\n        - poses: Could be `numpy.ndarray` of shape :math:`(N, 4, 4)`, or list of `numpy.ndarray`s of shape\n          :math:`(4, 4)`",
        "detail": "conceptgraph.dataset.conceptgraphs_datautils",
        "documentation": {}
    },
    {
        "label": "create_label_image",
        "kind": 2,
        "importPath": "conceptgraph.dataset.conceptgraphs_datautils",
        "description": "conceptgraph.dataset.conceptgraphs_datautils",
        "peekOfCode": "def create_label_image(prediction: np.ndarray, color_palette: OrderedDict):\n    r\"\"\"Creates a label image, given a network prediction (each pixel contains class index) and a color palette.\n    Args:\n        prediction (numpy.ndarray): Predicted image where each pixel contains an integer,\n            corresponding to its class label.\n        color_palette (OrderedDict): Contains RGB colors (`uint8`) for each class.\n    Returns:\n        numpy.ndarray: Label image with the given color palette\n    Shape:\n        - prediction: :math:`(H, W)`",
        "detail": "conceptgraph.dataset.conceptgraphs_datautils",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "conceptgraph.dataset.conceptgraphs_datautils",
        "description": "conceptgraph.dataset.conceptgraphs_datautils",
        "peekOfCode": "__all__ = [\n    \"normalize_image\",\n    \"channels_first\",\n    \"scale_intrinsics\",\n    \"pointquaternion_to_homogeneous\",\n    \"poses_to_transforms\",\n    \"create_label_image\",\n]\ndef normalize_image(rgb: Union[torch.Tensor, np.ndarray]):\n    r\"\"\"Normalizes RGB image values from :math:`[0, 255]` range to :math:`[0, 1]` range.",
        "detail": "conceptgraph.dataset.conceptgraphs_datautils",
        "documentation": {}
    },
    {
        "label": "RGBDImages",
        "kind": 6,
        "importPath": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "description": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "peekOfCode": "class RGBDImages(object):\n    r\"\"\"Initializes an RGBDImage object consisting of a batch of a sequence of rgb images, depth maps,\n    camera intrinsics, and (optionally) poses.\n    Args:\n        rgb_image (torch.Tensor): 3-channel rgb image\n        depth_image (torch.Tensor): 1-channel depth map\n        intrinsics (torch.Tensor): camera intrinsics\n        poses (torch.Tensor or None): camera extrinsics. Default: None\n        channels_first(bool): indicates whether `rgb_image` and `depth_image` have channels first or channels last\n            representation (i.e. rgb_image.shape is :math:`(B, L, H, W, 3)` or :math:`(B, L, 3, H, W)`.",
        "detail": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "documentation": {}
    },
    {
        "label": "create_meshgrid",
        "kind": 2,
        "importPath": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "description": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "peekOfCode": "def create_meshgrid(\n    height: int, width: int, normalized_coords: Optional[bool] = True\n) -> torch.Tensor:\n    r\"\"\"Generates a coordinate grid for an image.\n    When `normalized_coords` is set to True, the grid is normalized to\n    be in the range [-1, 1] (to be consistent with the pytorch function\n    `grid_sample`.)\n    https://kornia.readthedocs.io/en/latest/utils.html#kornia.utils.create_meshgrid\n    Args:\n        height (int): Height of the image (number of rows).",
        "detail": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "documentation": {}
    },
    {
        "label": "inverse_intrinsics",
        "kind": 2,
        "importPath": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "description": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "peekOfCode": "def inverse_intrinsics(K: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n    r\"\"\"Efficient inversion of intrinsics matrix\n    Args:\n        K (torch.Tensor): Intrinsics matrix\n        eps (float): Epsilon for numerical stability\n    Returns:\n        torch.Tensor: Inverse of intrinsics matrices\n    Shape:\n        - K: :math:`(*, 4, 4)` or :math:`(*, 3, 3)`\n        - Kinv: Matches shape of `K` (:math:`(*, 4, 4)` or :math:`(*, 3, 3)`)",
        "detail": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "documentation": {}
    },
    {
        "label": "img_to_b64str",
        "kind": 2,
        "importPath": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "description": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "peekOfCode": "def img_to_b64str(img, quality=95):\n    r\"\"\"Converts a numpy array of uint8 into a base64 jpeg string.\n    Args\n        img (np.ndarray): RGB or greyscale image array\n        quality (int): Image quality from 0 to 100 (the higher is the better). Default: 95\n    Returns:\n        str: base64 jpeg string\n    \"\"\"\n    # Can also use px._imshow._array_to_b64str:\n    # https://github.com/plotly/plotly.py/blob/63f20ee08d2b83075d3749ec5d85f7909401a0ef/packages/python/plotly/plotly/express/_imshow.py#L27",
        "detail": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "documentation": {}
    },
    {
        "label": "numpy_to_plotly_image",
        "kind": 2,
        "importPath": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "description": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "peekOfCode": "def numpy_to_plotly_image(img, name=None, is_depth=False, scale=None, quality=95):\n    r\"\"\"Converts a numpy array img to a `plotly.graph_objects.Image` object.\n    Args\n        img (np.ndarray): RGB image array\n        name (str): Name for the returned `plotly.graph_objects.Image` object\n        is_depth (bool): Bool indicating whether input `img` is depth image. Default: False\n        scale (int or None): Scale factor to display on hover. If None, will not display `scale: ...`. Default: None\n        quality (int): Image quality from 0 to 100 (the higher is the better). Default: 95\n    Returns:\n        `plotly.graph_objects.Image`",
        "detail": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "description": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "peekOfCode": "__all__ = [\"RGBDImages\"]\ndef create_meshgrid(\n    height: int, width: int, normalized_coords: Optional[bool] = True\n) -> torch.Tensor:\n    r\"\"\"Generates a coordinate grid for an image.\n    When `normalized_coords` is set to True, the grid is normalized to\n    be in the range [-1, 1] (to be consistent with the pytorch function\n    `grid_sample`.)\n    https://kornia.readthedocs.io/en/latest/utils.html#kornia.utils.create_meshgrid\n    Args:",
        "detail": "conceptgraph.dataset.conceptgraphs_rgbd_images",
        "documentation": {}
    },
    {
        "label": "GradSLAMDataset",
        "kind": 6,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "class GradSLAMDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        config_dict,\n        stride: Optional[int] = 1,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: int = 480,\n        desired_width: int = 640,\n        channels_first: bool = False,",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "ICLDataset",
        "kind": 6,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "class ICLDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict: Dict,\n        basedir: Union[Path, str],\n        sequence: Union[Path, str],\n        stride: Optional[int] = 1,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 480,",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "ReplicaDataset",
        "kind": 6,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "class ReplicaDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 480,",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "ScannetDataset",
        "kind": 6,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "class ScannetDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 968,",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "Ai2thorDataset",
        "kind": 6,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "class Ai2thorDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 968,",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "AzureKinectDataset",
        "kind": 6,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "class AzureKinectDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 480,",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "RealsenseDataset",
        "kind": 6,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "class RealsenseDataset(GradSLAMDataset):\n    \"\"\"\n    Dataset class to process depth images captured by realsense camera on the tabletop manipulator \n    \"\"\"\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "Record3DDataset",
        "kind": 6,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "class Record3DDataset(GradSLAMDataset):\n    \"\"\"\n    Dataset class to read in saved files from the structure created by our\n    `save_record3d_stream.py` script\n    \"\"\"\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "MultiscanDataset",
        "kind": 6,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "class MultiscanDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 480,",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "Hm3dDataset",
        "kind": 6,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "class Hm3dDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 480,",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "as_intrinsics_matrix",
        "kind": 2,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "def as_intrinsics_matrix(intrinsics):\n    \"\"\"\n    Get matrix representation of intrinsics.\n    \"\"\"\n    K = np.eye(3)\n    K[0, 0] = intrinsics[0]\n    K[1, 1] = intrinsics[1]\n    K[0, 2] = intrinsics[2]\n    K[1, 2] = intrinsics[3]\n    return K",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "readEXR_onlydepth",
        "kind": 2,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "def readEXR_onlydepth(filename):\n    \"\"\"\n    Read depth data from EXR image file.\n    Args:\n        filename (str): File path.\n    Returns:\n        Y (numpy.array): Depth buffer in float32 format.\n    \"\"\"\n    # move the import here since only CoFusion needs these package\n    # sometimes installation of openexr is hard, you can run all other datasets",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "load_dataset_config",
        "kind": 2,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "def load_dataset_config(path, default_path=None):\n    \"\"\"\n    Loads config file.\n    Args:\n        path (str): path to config file.\n        default_path (str, optional): whether to use default path. Defaults to None.\n    Returns:\n        cfg (dict): config dict.\n    \"\"\"\n    # load configuration from file itself",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "update_recursive",
        "kind": 2,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "def update_recursive(dict1, dict2):\n    \"\"\"\n    Update two config dictionaries recursively.\n    Args:\n        dict1 (dict): first dictionary to be updated.\n        dict2 (dict): second dictionary which entries should be used.\n    \"\"\"\n    for k, v in dict2.items():\n        if k not in dict1:\n            dict1[k] = dict()",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "common_dataset_to_batch",
        "kind": 2,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "def common_dataset_to_batch(dataset):\n    colors, depths, poses = [], [], []\n    intrinsics, embeddings = None, None\n    for idx in range(len(dataset)):\n        _color, _depth, intrinsics, _pose, _embedding = dataset[idx]\n        colors.append(_color)\n        depths.append(_depth)\n        poses.append(_pose)\n        if _embedding is not None:\n            if embeddings is None:",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "kind": 2,
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "peekOfCode": "def get_dataset(dataconfig, basedir, sequence, **kwargs):\n    config_dict = load_dataset_config(dataconfig)\n    if config_dict[\"dataset_name\"].lower() in [\"icl\"]:\n        return ICLDataset(config_dict, basedir, sequence, **kwargs)\n    elif config_dict[\"dataset_name\"].lower() in [\"replica\"]:\n        return ReplicaDataset(config_dict, basedir, sequence, **kwargs)\n    elif config_dict[\"dataset_name\"].lower() in [\"azure\", \"azurekinect\"]:\n        return AzureKinectDataset(config_dict, basedir, sequence, **kwargs)\n    elif config_dict[\"dataset_name\"].lower() in [\"scannet\"]:\n        return ScannetDataset(config_dict, basedir, sequence, **kwargs)",
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "ProgramArgs",
        "kind": 6,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "class ProgramArgs:\n    datapath = \"/home/kuwajerw/new_local_data/new_record3d/ali_apartment/co_store\"\n    output_dir = None  # Optional, set dynamically if not provided\ndesired_width = 1440\ndesired_height = 1920\ndef load_depth(filepath):\n    with open(filepath, 'rb') as depth_fh:\n        raw_bytes = depth_fh.read()\n        decompressed_bytes = liblzfse.decompress(raw_bytes)\n        depth_img = np.frombuffer(decompressed_bytes, dtype=np.float32)",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "load_depth",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def load_depth(filepath):\n    with open(filepath, 'rb') as depth_fh:\n        raw_bytes = depth_fh.read()\n        decompressed_bytes = liblzfse.decompress(raw_bytes)\n        depth_img = np.frombuffer(decompressed_bytes, dtype=np.float32)\n        depth_img = depth_img.reshape((256, 192))  # Original resolution\n        depth_img = resize_depth(depth_img, desired_width, desired_height)\n    return depth_img\ndef load_conf(filepath):\n    with open(filepath, 'rb') as conf_fh:",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "load_conf",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def load_conf(filepath):\n    with open(filepath, 'rb') as conf_fh:\n        raw_bytes = conf_fh.read()\n        decompressed_bytes = liblzfse.decompress(raw_bytes)\n        conf_img = np.frombuffer(decompressed_bytes, dtype=np.uint8)\n        conf_img = conf_img.reshape((256, 192))  # Original resolution\n        conf_img = resize_depth(conf_img, desired_width, desired_height)  # Using the same resizing function as depth\n    return conf_img\ndef load_color(filepath):\n    img = cv2.imread(filepath)",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "load_color",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def load_color(filepath):\n    img = cv2.imread(filepath)\n    resized_img = cv2.resize(img, (desired_width, desired_height), interpolation=cv2.INTER_LINEAR)\n    return resized_img\ndef resize_depth(depth_img, desired_width, desired_height):\n    return cv2.resize(depth_img, (desired_width, desired_height), interpolation=cv2.INTER_NEAREST)\ndef write_color(outpath, img):\n    cv2.imwrite(outpath, img)\ndef write_depth(outpath, depth):\n    depth = depth * 1000",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "resize_depth",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def resize_depth(depth_img, desired_width, desired_height):\n    return cv2.resize(depth_img, (desired_width, desired_height), interpolation=cv2.INTER_NEAREST)\ndef write_color(outpath, img):\n    cv2.imwrite(outpath, img)\ndef write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)\ndef write_conf(outpath, conf):",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "write_color",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def write_color(outpath, img):\n    cv2.imwrite(outpath, img)\ndef write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)\ndef write_conf(outpath, conf):\n    np.save(outpath, conf)\ndef write_conf_img(outpath, conf):",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "write_depth",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)\ndef write_conf(outpath, conf):\n    np.save(outpath, conf)\ndef write_conf_img(outpath, conf):\n    conf_img = Image.fromarray(conf)\n    conf_img.save(outpath)",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "write_conf",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def write_conf(outpath, conf):\n    np.save(outpath, conf)\ndef write_conf_img(outpath, conf):\n    conf_img = Image.fromarray(conf)\n    conf_img.save(outpath)\ndef write_pose(outpath, pose):\n    np.save(outpath, pose.astype(np.float32))\ndef adjust_intrinsics(intrinsics_dict, original_width, original_height):\n    width_scale = desired_width / original_width\n    height_scale = desired_height / original_height",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "write_conf_img",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def write_conf_img(outpath, conf):\n    conf_img = Image.fromarray(conf)\n    conf_img.save(outpath)\ndef write_pose(outpath, pose):\n    np.save(outpath, pose.astype(np.float32))\ndef adjust_intrinsics(intrinsics_dict, original_width, original_height):\n    width_scale = desired_width / original_width\n    height_scale = desired_height / original_height\n    intrinsics_dict[\"fx\"] *= width_scale\n    intrinsics_dict[\"fy\"] *= height_scale",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "write_pose",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def write_pose(outpath, pose):\n    np.save(outpath, pose.astype(np.float32))\ndef adjust_intrinsics(intrinsics_dict, original_width, original_height):\n    width_scale = desired_width / original_width\n    height_scale = desired_height / original_height\n    intrinsics_dict[\"fx\"] *= width_scale\n    intrinsics_dict[\"fy\"] *= height_scale\n    intrinsics_dict[\"cx\"] *= width_scale\n    intrinsics_dict[\"cy\"] *= height_scale\n    intrinsics_dict[\"w\"] = desired_width",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "adjust_intrinsics",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def adjust_intrinsics(intrinsics_dict, original_width, original_height):\n    width_scale = desired_width / original_width\n    height_scale = desired_height / original_height\n    intrinsics_dict[\"fx\"] *= width_scale\n    intrinsics_dict[\"fy\"] *= height_scale\n    intrinsics_dict[\"cx\"] *= width_scale\n    intrinsics_dict[\"cy\"] *= height_scale\n    intrinsics_dict[\"w\"] = desired_width\n    intrinsics_dict[\"h\"] = desired_height\n    return intrinsics_dict",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "get_poses",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def get_poses(metadata_dict: dict) -> int:\n    \"\"\"Converts Record3D's metadata dict into pose matrices needed by nerfstudio\n    Args:\n        metadata_dict: Dict containing Record3D metadata\n    Returns:\n        np.array of pose matrices for each image of shape: (num_images, 4, 4)\n    \"\"\"\n    poses_data = np.array(metadata_dict[\"poses\"])  # (N, 3, 4)\n    # NB: Record3D / scipy use \"scalar-last\" format quaternions (x y z w)\n    # https://fzheng.me/2017/11/12/quaternion_conventions_en/",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "get_intrinsics",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def get_intrinsics(metadata_dict: dict, downscale_factor: float = 7.5) -> int:\n    \"\"\"Converts Record3D metadata dict into intrinsic info needed by nerfstudio\n    Args:\n        metadata_dict: Dict containing Record3D metadata\n        downscale_factor: factor to scale RGB image by (usually scale factor is\n            set to 7.5 for record3d 1.8 or higher -- this is the factor that downscales\n            RGB images to lidar)\n    Returns:\n        dict with camera intrinsics keys needed by nerfstudio\n    \"\"\"",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "def main():\n    args = tyro.cli(ProgramArgs)\n    metadata = None\n    with open(os.path.join(args.datapath, \"metadata\"), \"r\") as f:\n        metadata = json.load(f)\n    # If output_dir is not specified, set it to a \"preprocessed\" folder inside datapath parent folder\n    if args.output_dir is None:\n        datapath = Path(args.datapath)\n        args.output_dir = str(datapath.parent / (datapath.name + \"_preprocessed\"))\n    print(f\"Preprocessing Record3D data from \\n{args.datapath} to \\n{args.output_dir}\")",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "desired_width",
        "kind": 5,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "desired_width = 1440\ndesired_height = 1920\ndef load_depth(filepath):\n    with open(filepath, 'rb') as depth_fh:\n        raw_bytes = depth_fh.read()\n        decompressed_bytes = liblzfse.decompress(raw_bytes)\n        depth_img = np.frombuffer(decompressed_bytes, dtype=np.float32)\n        depth_img = depth_img.reshape((256, 192))  # Original resolution\n        depth_img = resize_depth(depth_img, desired_width, desired_height)\n    return depth_img",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "desired_height",
        "kind": 5,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file",
        "description": "conceptgraph.dataset.preprocess_r3d_file",
        "peekOfCode": "desired_height = 1920\ndef load_depth(filepath):\n    with open(filepath, 'rb') as depth_fh:\n        raw_bytes = depth_fh.read()\n        decompressed_bytes = liblzfse.decompress(raw_bytes)\n        depth_img = np.frombuffer(decompressed_bytes, dtype=np.float32)\n        depth_img = depth_img.reshape((256, 192))  # Original resolution\n        depth_img = resize_depth(depth_img, desired_width, desired_height)\n    return depth_img\ndef load_conf(filepath):",
        "detail": "conceptgraph.dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "ProgramArgs",
        "kind": 6,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "description": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "peekOfCode": "class ProgramArgs:\n    datapath: str = \"/home/krishna/data/record3d/krishna-bcs-room\"\ndef load_depth(filepath):\n    with open(filepath, 'rb') as depth_fh:\n        raw_bytes = depth_fh.read()\n        decompressed_bytes = liblzfse.decompress(raw_bytes)\n        depth_img = np.frombuffer(decompressed_bytes, dtype=np.float32)\n    # depth_img = depth_img.reshape((640, 480))  # For a FaceID camera 3D Video\n    depth_img = depth_img.reshape((256, 192))  # For a LiDAR 3D Video\n    return depth_img",
        "detail": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "documentation": {}
    },
    {
        "label": "load_depth",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "description": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "peekOfCode": "def load_depth(filepath):\n    with open(filepath, 'rb') as depth_fh:\n        raw_bytes = depth_fh.read()\n        decompressed_bytes = liblzfse.decompress(raw_bytes)\n        depth_img = np.frombuffer(decompressed_bytes, dtype=np.float32)\n    # depth_img = depth_img.reshape((640, 480))  # For a FaceID camera 3D Video\n    depth_img = depth_img.reshape((256, 192))  # For a LiDAR 3D Video\n    return depth_img\ndef load_conf(filepath):\n    with open(filepath, 'rb') as conf_fh:",
        "detail": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "documentation": {}
    },
    {
        "label": "load_conf",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "description": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "peekOfCode": "def load_conf(filepath):\n    with open(filepath, 'rb') as conf_fh:\n        raw_bytes = conf_fh.read()\n        decompressed_bytes = liblzfse.decompress(raw_bytes)\n        conf_img = np.frombuffer(decompressed_bytes, dtype=np.uint8)\n    # depth_img = depth_img.reshape((640, 480))  # For a FaceID camera 3D Video\n    conf_img = conf_img.reshape((256, 192))  # For a LiDAR 3D Video\n    return conf_img\ndef load_color(filepath):\n    img = cv2.imread(filepath)",
        "detail": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "documentation": {}
    },
    {
        "label": "load_color",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "description": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "peekOfCode": "def load_color(filepath):\n    img = cv2.imread(filepath)\n    return cv2.resize(img, (192, 256))\ndef write_color(outpath, img):\n    cv2.imwrite(outpath, img)\ndef write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)",
        "detail": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "documentation": {}
    },
    {
        "label": "write_color",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "description": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "peekOfCode": "def write_color(outpath, img):\n    cv2.imwrite(outpath, img)\ndef write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)\ndef write_conf(outpath, conf):\n    np.save(outpath, conf)\ndef write_pose(outpath, pose):",
        "detail": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "documentation": {}
    },
    {
        "label": "write_depth",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "description": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "peekOfCode": "def write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)\ndef write_conf(outpath, conf):\n    np.save(outpath, conf)\ndef write_pose(outpath, pose):\n    np.save(outpath, pose.astype(np.float32))\ndef get_poses(metadata_dict: dict) -> int:",
        "detail": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "documentation": {}
    },
    {
        "label": "write_conf",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "description": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "peekOfCode": "def write_conf(outpath, conf):\n    np.save(outpath, conf)\ndef write_pose(outpath, pose):\n    np.save(outpath, pose.astype(np.float32))\ndef get_poses(metadata_dict: dict) -> int:\n    \"\"\"Converts Record3D's metadata dict into pose matrices needed by nerfstudio\n    Args:\n        metadata_dict: Dict containing Record3D metadata\n    Returns:\n        np.array of pose matrices for each image of shape: (num_images, 4, 4)",
        "detail": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "documentation": {}
    },
    {
        "label": "write_pose",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "description": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "peekOfCode": "def write_pose(outpath, pose):\n    np.save(outpath, pose.astype(np.float32))\ndef get_poses(metadata_dict: dict) -> int:\n    \"\"\"Converts Record3D's metadata dict into pose matrices needed by nerfstudio\n    Args:\n        metadata_dict: Dict containing Record3D metadata\n    Returns:\n        np.array of pose matrices for each image of shape: (num_images, 4, 4)\n    \"\"\"\n    poses_data = np.array(metadata_dict[\"poses\"])  # (N, 3, 4)",
        "detail": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "documentation": {}
    },
    {
        "label": "get_poses",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "description": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "peekOfCode": "def get_poses(metadata_dict: dict) -> int:\n    \"\"\"Converts Record3D's metadata dict into pose matrices needed by nerfstudio\n    Args:\n        metadata_dict: Dict containing Record3D metadata\n    Returns:\n        np.array of pose matrices for each image of shape: (num_images, 4, 4)\n    \"\"\"\n    poses_data = np.array(metadata_dict[\"poses\"])  # (N, 3, 4)\n    # NB: Record3D / scipy use \"scalar-last\" format quaternions (x y z w)\n    # https://fzheng.me/2017/11/12/quaternion_conventions_en/",
        "detail": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "documentation": {}
    },
    {
        "label": "get_intrinsics",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "description": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "peekOfCode": "def get_intrinsics(metadata_dict: dict, downscale_factor: float = 7.5) -> int:\n    \"\"\"Converts Record3D metadata dict into intrinsic info needed by nerfstudio\n    Args:\n        metadata_dict: Dict containing Record3D metadata\n        downscale_factor: factor to scale RGB image by (usually scale factor is\n            set to 7.5 for record3d 1.8 or higher -- this is the factor that downscales\n            RGB images to lidar)\n    Returns:\n        dict with camera intrinsics keys needed by nerfstudio\n    \"\"\"",
        "detail": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "description": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "peekOfCode": "def main():\n    args = tyro.cli(ProgramArgs)\n    metadata = None\n    with open(os.path.join(args.datapath, \"metadata\"), \"r\") as f:\n        metadata = json.load(f)\n    # Keys in metadata dict\n    # h, w, K, fps, dw, dh, initPose, poses, cameraType, frameTimestamps\n    # print(metadata.keys())\n    poses = get_poses(metadata)\n    intrinsics_dict = get_intrinsics(metadata)",
        "detail": "conceptgraph.dataset.preprocess_r3d_file_backup",
        "documentation": {}
    },
    {
        "label": "REPLICA_CLASSES",
        "kind": 5,
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "peekOfCode": "REPLICA_CLASSES = [\n    \"other\",\n    \"backpack\",\n    \"base-cabinet\",\n    \"basket\",\n    \"bathtub\",\n    \"beam\",\n    \"beanbag\",\n    \"bed\",\n    \"bench\",",
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_SCENE_IDS",
        "kind": 5,
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "peekOfCode": "REPLICA_SCENE_IDS = [\n    \"room0\",\n    \"room1\",\n    \"room2\",\n    \"office0\",\n    \"office1\",\n    \"office2\",\n    \"office3\",\n    \"office4\",\n]",
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_SCENE_IDS_",
        "kind": 5,
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "peekOfCode": "REPLICA_SCENE_IDS_ = [\n    \"room_0\",\n    \"room_1\",\n    \"room_2\",\n    \"office_0\",\n    \"office_1\",\n    \"office_2\",\n    \"office_3\",\n    \"office_4\",\n]",
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_EXISTING_CLASSES",
        "kind": 5,
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "peekOfCode": "REPLICA_EXISTING_CLASSES = [\n    0,\n    3,\n    7,\n    8,\n    10,\n    11,\n    12,\n    13,\n    14,",
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "DemoApp",
        "kind": 6,
        "importPath": "conceptgraph.dataset.save_record3d",
        "description": "conceptgraph.dataset.save_record3d",
        "peekOfCode": "class DemoApp:\n    def __init__(self, savedir_name=\"saved-record3d\", seq_name=\"debug\"):\n        self.event = Event()\n        self.session = None\n        self.DEVICE_TYPE__TRUEDEPTH = 0\n        self.DEVICE_TYPE__LIDAR = 1\n        self.imgs = []\n        self.depths = []\n        self.intrinsics = None\n        self.poses = []",
        "detail": "conceptgraph.dataset.save_record3d",
        "documentation": {}
    },
    {
        "label": "write_color",
        "kind": 2,
        "importPath": "conceptgraph.dataset.save_record3d",
        "description": "conceptgraph.dataset.save_record3d",
        "peekOfCode": "def write_color(outpath, img):\n    cv2.imwrite(outpath, img)\ndef write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)\ndef write_pose(outpath, pose):\n    # quat_trans: Quaternion + world position (accessible via camera_pose.[qx|qy|qz|qw|tx|ty|tz])\n    # NB: Record3D / scipy use \"scalar-last\" format quaternions (x y z w)",
        "detail": "conceptgraph.dataset.save_record3d",
        "documentation": {}
    },
    {
        "label": "write_depth",
        "kind": 2,
        "importPath": "conceptgraph.dataset.save_record3d",
        "description": "conceptgraph.dataset.save_record3d",
        "peekOfCode": "def write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)\ndef write_pose(outpath, pose):\n    # quat_trans: Quaternion + world position (accessible via camera_pose.[qx|qy|qz|qw|tx|ty|tz])\n    # NB: Record3D / scipy use \"scalar-last\" format quaternions (x y z w)\n    # https://fzheng.me/2017/11/12/quaternion_conventions_en/\n    # pose = np.asarray([pose.qx, pose.qy, pose.qz, pose.qw, pose.tx, pose.ty, pose.tz])",
        "detail": "conceptgraph.dataset.save_record3d",
        "documentation": {}
    },
    {
        "label": "write_pose",
        "kind": 2,
        "importPath": "conceptgraph.dataset.save_record3d",
        "description": "conceptgraph.dataset.save_record3d",
        "peekOfCode": "def write_pose(outpath, pose):\n    # quat_trans: Quaternion + world position (accessible via camera_pose.[qx|qy|qz|qw|tx|ty|tz])\n    # NB: Record3D / scipy use \"scalar-last\" format quaternions (x y z w)\n    # https://fzheng.me/2017/11/12/quaternion_conventions_en/\n    # pose = np.asarray([pose.qx, pose.qy, pose.qz, pose.qw, pose.tx, pose.ty, pose.tz])\n    c2w = np.zeros((4, 4))\n    c2w[3, 3] = 1.0\n    c2w[:3, :3] = Rotation.from_quat(pose[:4]).as_matrix()\n    c2w[:3, 3] = pose[4:]\n    np.save(outpath, c2w.astype(np.float32))",
        "detail": "conceptgraph.dataset.save_record3d",
        "documentation": {}
    },
    {
        "label": "LlavaConfig",
        "kind": 6,
        "importPath": "conceptgraph.llava.llava_model",
        "description": "conceptgraph.llava.llava_model",
        "peekOfCode": "class LlavaConfig(LlamaConfig):\n    model_type = \"llava\"\nclass LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        # Instantiate the multimodal vision tower\n        if hasattr(config, \"mm_vision_tower\"):\n            # HACK: for FSDP (fully-sharded data parallel); cast to a list\n            self.vision_tower = [",
        "detail": "conceptgraph.llava.llava_model",
        "documentation": {}
    },
    {
        "label": "LlavaLlamaModelTweaked",
        "kind": 6,
        "importPath": "conceptgraph.llava.llava_model",
        "description": "conceptgraph.llava.llava_model",
        "peekOfCode": "class LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        # Instantiate the multimodal vision tower\n        if hasattr(config, \"mm_vision_tower\"):\n            # HACK: for FSDP (fully-sharded data parallel); cast to a list\n            self.vision_tower = [\n                CLIPVisionModel.from_pretrained(config.mm_vision_tower)\n            ]",
        "detail": "conceptgraph.llava.llava_model",
        "documentation": {}
    },
    {
        "label": "LlavaLlamaForCausalLMTweaked",
        "kind": 6,
        "importPath": "conceptgraph.llava.llava_model",
        "description": "conceptgraph.llava.llava_model",
        "peekOfCode": "class LlavaLlamaForCausalLMTweaked(LlamaForCausalLM):\n    config_class = LlavaConfig\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlavaLlamaModelTweaked(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n    def get_model(self):\n        return self.model",
        "detail": "conceptgraph.llava.llava_model",
        "documentation": {}
    },
    {
        "label": "LLaVaChat",
        "kind": 6,
        "importPath": "conceptgraph.llava.llava_model",
        "description": "conceptgraph.llava.llava_model",
        "peekOfCode": "class LLaVaChat(object):\n    def __init__(self, model_path, conv_mode=\"multimodal\", num_gpus=1):\n        self.model_path = model_path\n        self.conv_mode = conv_mode\n        self.num_gpus = num_gpus\n        # Handle multi-gpu config\n        if self.num_gpus == 1:\n            kwargs = {}\n        else:\n            kwargs = {",
        "detail": "conceptgraph.llava.llava_model",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_TOKEN",
        "kind": 5,
        "importPath": "conceptgraph.llava.llava_model",
        "description": "conceptgraph.llava.llava_model",
        "peekOfCode": "DEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava\"\nclass LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)",
        "detail": "conceptgraph.llava.llava_model",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_PATCH_TOKEN",
        "kind": 5,
        "importPath": "conceptgraph.llava.llava_model",
        "description": "conceptgraph.llava.llava_model",
        "peekOfCode": "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava\"\nclass LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        # Instantiate the multimodal vision tower",
        "detail": "conceptgraph.llava.llava_model",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IM_START_TOKEN",
        "kind": 5,
        "importPath": "conceptgraph.llava.llava_model",
        "description": "conceptgraph.llava.llava_model",
        "peekOfCode": "DEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava\"\nclass LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        # Instantiate the multimodal vision tower\n        if hasattr(config, \"mm_vision_tower\"):",
        "detail": "conceptgraph.llava.llava_model",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IM_END_TOKEN",
        "kind": 5,
        "importPath": "conceptgraph.llava.llava_model",
        "description": "conceptgraph.llava.llava_model",
        "peekOfCode": "DEFAULT_IM_END_TOKEN = \"<im_end>\"\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava\"\nclass LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        # Instantiate the multimodal vision tower\n        if hasattr(config, \"mm_vision_tower\"):\n            # HACK: for FSDP (fully-sharded data parallel); cast to a list",
        "detail": "conceptgraph.llava.llava_model",
        "documentation": {}
    },
    {
        "label": "GPTPrompt",
        "kind": 6,
        "importPath": "conceptgraph.scenegraph.GPTPrompt",
        "description": "conceptgraph.scenegraph.GPTPrompt",
        "peekOfCode": "class GPTPrompt:\n    def __init__(self):\n        self.old_system_prompt = \"\"\"\n        The input is a list of JSONs describing multiple predictions of a single object. Each JSON has four fields: \n        1. id: a unique identifier for the object\n        2. bbox_extent: the 3D bounding box extents of the object \n        3. bbox_center: the 3D bounding box center of the object \n        4. caption: a caption predicted by an image captioning model referencing that object. This caption should be brief and in sparse prose. For example, the caption \"the object described appears to be described as a electric bicycle, it is sitting alongside a red suitcase which is nearby\" should be shortened to \"electric bike, near red suitcase\".\n        There may be upto 10 such bounding boxes and captions in each input.\n        The captions may not always be accurate or consistent (often, predictions may just be wrong). ",
        "detail": "conceptgraph.scenegraph.GPTPrompt",
        "documentation": {}
    },
    {
        "label": "ProgramArgs",
        "kind": 6,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "class ProgramArgs:\n    mode: Literal[\n        \"extract-node-captions\",\n        \"refine-node-captions\",\n        \"build-scenegraph\",\n        \"generate-scenegraph-json\",\n        \"annotate-scenegraph\",\n    ]\n    # Path to cache directory\n    cachedir: str = \"saved/room0\"",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "load_scene_map",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def load_scene_map(args, scene_map):\n    \"\"\"\n    Loads a scene map from a gzip-compressed pickle file. This is a function because depending whether the mapfile was made using cfslam_pipeline_batch.py or merge_duplicate_objects.py, the file format is different (see below). So this function handles that case.\n    The function checks the structure of the deserialized object to determine\n    the correct way to load it into the `scene_map` object. There are two\n    expected formats:\n    1. A dictionary containing an \"objects\" key.\n    2. A list or a dictionary (replace with your expected type).\n    \"\"\"\n    with gzip.open(Path(args.mapfile), \"rb\") as f:",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "crop_image_pil",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def crop_image_pil(image: Image, x1: int, y1: int, x2: int, y2: int, padding: int = 0) -> Image:\n    \"\"\"\n    Crop the image with some padding\n    Args:\n        image: PIL image\n        x1, y1, x2, y2: bounding box coordinates\n        padding: padding around the bounding box\n    Returns:\n        image_crop: PIL image\n    Implementation from the CFSLAM repo",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "draw_red_outline",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def draw_red_outline(image, mask):\n    \"\"\" Draw a red outline around the object i nan image\"\"\"\n    # Convert PIL Image to numpy array\n    image_np = np.array(image)\n    red_outline = [255, 0, 0]\n    # Find contours in the binary mask\n    contours, _ = cv2.findContours(mask.astype(np.uint8) * 255, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    # Draw red outlines around the object. The last argument \"3\" indicates the thickness of the outline.\n    cv2.drawContours(image_np, contours, -1, red_outline, 3)\n    # Optionally, add padding around the object by dilating the drawn contours",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "crop_image_and_mask",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def crop_image_and_mask(image: Image, mask: np.ndarray, x1: int, y1: int, x2: int, y2: int, padding: int = 0):\n    \"\"\" Crop the image and mask with some padding. I made a single function that crops both the image and the mask at the same time because I was getting shape mismatches when I cropped them separately.This way I can check that they are the same shape.\"\"\"\n    image = np.array(image)\n    # Verify initial dimensions\n    if image.shape[:2] != mask.shape:\n        print(\"Initial shape mismatch: Image shape {} != Mask shape {}\".format(image.shape, mask.shape))\n        return None, None\n    # Define the cropping coordinates\n    x1 = max(0, x1 - padding)\n    y1 = max(0, y1 - padding)",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "blackout_nonmasked_area",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def blackout_nonmasked_area(image_pil, mask):\n    \"\"\" Blackout the non-masked area of an image\"\"\"\n    # convert image to numpy array\n    image_np = np.array(image_pil)\n    # Create an all-black image of the same shape as the input image\n    black_image = np.zeros_like(image_np)\n    # Wherever the mask is True, replace the black image pixel with the original image pixel\n    black_image[mask] = image_np[mask]\n    # convert back to pil image\n    black_image = Image.fromarray(black_image)",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "plot_images_with_captions",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def plot_images_with_captions(images, captions, confidences, low_confidences, masks, savedir, idx_obj):\n    \"\"\" This is debug helper function that plots the images with the captions and masks overlaid and saves them to a directory. This way you can inspect exactly what the LLaVA model is captioning which image with the mask, and the mask confidence scores overlaid.\"\"\"\n    n = min(9, len(images))  # Only plot up to 9 images\n    nrows = int(np.ceil(n / 3))\n    ncols = 3 if n > 1 else 1\n    fig, axarr = plt.subplots(nrows, ncols, figsize=(10, 5 * nrows), squeeze=False)  # Adjusted figsize\n    for i in range(n):\n        row, col = divmod(i, 3)\n        ax = axarr[row][col]\n        ax.imshow(images[i])",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "extract_node_captions",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def extract_node_captions(args):\n    from conceptgraph.llava.llava_model import LLaVaChat\n    # NOTE: args.mapfile is in cfslam format\n    from conceptgraph.slam.slam_classes import MapObjectList\n    # Load the scene map\n    scene_map = MapObjectList()\n    load_scene_map(args, scene_map)\n    # Scene map is in CFSLAM format\n    # keys: 'image_idx', 'mask_idx', 'color_path', 'class_id', 'num_detections',\n    # 'mask', 'xyxy', 'conf', 'n_points', 'pixel_area', 'contain_number', 'clip_ft',",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "save_json_to_file",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def save_json_to_file(json_str, filename):\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        json.dump(json_str, f, indent=4, sort_keys=False)\ndef refine_node_captions(args):\n    # NOTE: args.mapfile is in cfslam format\n    from conceptgraph.slam.slam_classes import MapObjectList\n    from conceptgraph.scenegraph.GPTPrompt import GPTPrompt\n    # Load the captions for each segment\n    caption_file = Path(args.cachedir) / \"cfslam_llava_captions.json\"\n    captions = None",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "refine_node_captions",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def refine_node_captions(args):\n    # NOTE: args.mapfile is in cfslam format\n    from conceptgraph.slam.slam_classes import MapObjectList\n    from conceptgraph.scenegraph.GPTPrompt import GPTPrompt\n    # Load the captions for each segment\n    caption_file = Path(args.cachedir) / \"cfslam_llava_captions.json\"\n    captions = None\n    with open(caption_file, \"r\") as f:\n        captions = json.load(f)\n    # loaddir_captions = Path(args.cachedir) / \"cfslam_captions_llava\"",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "extract_object_tag_from_json_str",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def extract_object_tag_from_json_str(json_str):\n    start_str_found = False\n    is_object_tag = False\n    object_tag_complete = False\n    object_tag = \"\"\n    r = json_str.strip().split()\n    for _idx, _r in enumerate(r):\n        if not start_str_found:\n            # Searching for open parenthesis of JSON\n            if _r == \"{\":",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "build_scenegraph",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def build_scenegraph(args):\n    from conceptgraph.slam.slam_classes import MapObjectList\n    # from conceptgraph.slam.utils import compute_overlap_matrix\n    from conceptgraph.slam.utils import compute_overlap_matrix_general\n    # Load the scene map\n    scene_map = MapObjectList()\n    load_scene_map(args, scene_map)\n    response_dir = Path(args.cachedir) / \"cfslam_gpt-4_responses\"\n    responses = []\n    object_tags = []",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "generate_scenegraph_json",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def generate_scenegraph_json(args):\n    from conceptgraph.slam.slam_classes import MapObjectList\n    # Generate the JSON file summarizing the scene, if it doesn't exist already\n    # or if the --recopmute_scenegraph_json flag is set\n    scene_desc = []\n    print(\"Generating scene graph JSON file...\")\n    # Load the pruned scene map\n    scene_map = MapObjectList()\n    with gzip.open(Path(args.cachedir) / \"map\" / \"scene_map_cfslam_pruned.pkl.gz\", \"rb\") as f:\n        scene_map.load_serializable(pkl.load(f))",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "display_images",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def display_images(image_list):\n    num_images = len(image_list)\n    cols = 2  # Number of columns for the subplots (you can change this as needed)\n    rows = (num_images + cols - 1) // cols\n    _, axes = plt.subplots(rows, cols, figsize=(10, 5))\n    for i, ax in enumerate(axes.flat):\n        if i < num_images:\n            img = image_list[i]\n            ax.imshow(img)\n            ax.axis(\"off\")",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "annotate_scenegraph",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def annotate_scenegraph(args):\n    from conceptgraph.slam.slam_classes import MapObjectList\n    # Load the pruned scene map\n    scene_map = MapObjectList()\n    with gzip.open(Path(args.cachedir) / \"map\" / \"scene_map_cfslam_pruned.pkl.gz\", \"rb\") as f:\n        scene_map.load_serializable(pkl.load(f))\n    annot_inds = None\n    if args.annot_inds is not None:\n        annot_inds = args.annot_inds\n    # If annot_inds is not None, we also need to load the annotation json file and only",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def main():\n    # Process command-line args (if any)\n    args = tyro.cli(ProgramArgs)\n    # print using masking option\n    print(f\"args.masking_option: {args.masking_option}\")\n    if args.mode == \"extract-node-captions\":\n        extract_node_captions(args)\n    elif args.mode == \"refine-node-captions\":\n        refine_node_captions(args)\n    elif args.mode == \"build-scenegraph\":",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "openai.api_key",
        "kind": 5,
        "importPath": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "description": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n@dataclass\nclass ProgramArgs:\n    mode: Literal[\n        \"extract-node-captions\",\n        \"refine-node-captions\",\n        \"build-scenegraph\",\n        \"generate-scenegraph-json\",\n        \"annotate-scenegraph\",\n    ]",
        "detail": "conceptgraph.scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "conceptgraph.scenegraph.merge_duplicate_objects",
        "description": "conceptgraph.scenegraph.merge_duplicate_objects",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--result_path\", type=str, required=True)\n    parser.add_argument(\"--savefile\", type=str, required=True)\n    # To inspect the results of merge_overlap_objects\n    # This is mainly to quickly try out different thresholds\n    parser.add_argument(\"--merge_overlap_thresh\", type=float, default=-1)\n    parser.add_argument(\"--merge_visual_sim_thresh\", type=float, default=-1)\n    parser.add_argument(\"--merge_text_sim_thresh\", type=float, default=-1)\n    parser.add_argument(\"--obj_min_points\", type=int, default=0)",
        "detail": "conceptgraph.scenegraph.merge_duplicate_objects",
        "documentation": {}
    },
    {
        "label": "RGBDFrame",
        "kind": 6,
        "importPath": "conceptgraph.scripts.scannet_process.SensorData",
        "description": "conceptgraph.scripts.scannet_process.SensorData",
        "peekOfCode": "class RGBDFrame():\n  def load(self, file_handle):\n    self.camera_to_world = np.asarray(struct.unpack('f'*16, file_handle.read(16*4)), dtype=np.float32).reshape(4, 4)\n    self.timestamp_color = struct.unpack('Q', file_handle.read(8))[0]\n    self.timestamp_depth = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.depth_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_data = b''.join(struct.unpack('c'*self.color_size_bytes, file_handle.read(self.color_size_bytes)))\n    self.depth_data = b''.join(struct.unpack('c'*self.depth_size_bytes, file_handle.read(self.depth_size_bytes)))\n  def decompress_depth(self, compression_type):",
        "detail": "conceptgraph.scripts.scannet_process.SensorData",
        "documentation": {}
    },
    {
        "label": "SensorData",
        "kind": 6,
        "importPath": "conceptgraph.scripts.scannet_process.SensorData",
        "description": "conceptgraph.scripts.scannet_process.SensorData",
        "peekOfCode": "class SensorData:\n  def __init__(self, filename):\n    self.version = 4\n    self.load(filename)\n  def load(self, filename):\n    with open(filename, 'rb') as f:\n      version = struct.unpack('I', f.read(4))[0]\n      assert self.version == version\n      strlen = struct.unpack('Q', f.read(8))[0]\n      self.sensor_name = b''.join(struct.unpack('c'*strlen, f.read(strlen)))",
        "detail": "conceptgraph.scripts.scannet_process.SensorData",
        "documentation": {}
    },
    {
        "label": "COMPRESSION_TYPE_COLOR",
        "kind": 5,
        "importPath": "conceptgraph.scripts.scannet_process.SensorData",
        "description": "conceptgraph.scripts.scannet_process.SensorData",
        "peekOfCode": "COMPRESSION_TYPE_COLOR = {-1:'unknown', 0:'raw', 1:'png', 2:'jpeg'}\nCOMPRESSION_TYPE_DEPTH = {-1:'unknown', 0:'raw_ushort', 1:'zlib_ushort', 2:'occi_ushort'}\nclass RGBDFrame():\n  def load(self, file_handle):\n    self.camera_to_world = np.asarray(struct.unpack('f'*16, file_handle.read(16*4)), dtype=np.float32).reshape(4, 4)\n    self.timestamp_color = struct.unpack('Q', file_handle.read(8))[0]\n    self.timestamp_depth = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.depth_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_data = b''.join(struct.unpack('c'*self.color_size_bytes, file_handle.read(self.color_size_bytes)))",
        "detail": "conceptgraph.scripts.scannet_process.SensorData",
        "documentation": {}
    },
    {
        "label": "COMPRESSION_TYPE_DEPTH",
        "kind": 5,
        "importPath": "conceptgraph.scripts.scannet_process.SensorData",
        "description": "conceptgraph.scripts.scannet_process.SensorData",
        "peekOfCode": "COMPRESSION_TYPE_DEPTH = {-1:'unknown', 0:'raw_ushort', 1:'zlib_ushort', 2:'occi_ushort'}\nclass RGBDFrame():\n  def load(self, file_handle):\n    self.camera_to_world = np.asarray(struct.unpack('f'*16, file_handle.read(16*4)), dtype=np.float32).reshape(4, 4)\n    self.timestamp_color = struct.unpack('Q', file_handle.read(8))[0]\n    self.timestamp_depth = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.depth_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_data = b''.join(struct.unpack('c'*self.color_size_bytes, file_handle.read(self.color_size_bytes)))\n    self.depth_data = b''.join(struct.unpack('c'*self.depth_size_bytes, file_handle.read(self.depth_size_bytes)))",
        "detail": "conceptgraph.scripts.scannet_process.SensorData",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.scripts.scannet_process.reader",
        "description": "conceptgraph.scripts.scannet_process.reader",
        "peekOfCode": "def main():\n  if not os.path.exists(opt.output_path):\n    os.makedirs(opt.output_path)\n  # load the data\n  sys.stdout.write('loading %s...' % opt.filename)\n  sd = SensorData(opt.filename)\n  sys.stdout.write('loaded!\\n')\n  if opt.export_depth_images:\n    sd.export_depth_images(os.path.join(opt.output_path, 'depth'))\n  if opt.export_color_images:",
        "detail": "conceptgraph.scripts.scannet_process.reader",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "conceptgraph.scripts.scannet_process.reader",
        "description": "conceptgraph.scripts.scannet_process.reader",
        "peekOfCode": "parser = argparse.ArgumentParser()\n# data paths\nparser.add_argument('--filename', required=True, help='path to sens file to read')\nparser.add_argument('--output_path', required=True, help='path to output folder')\nparser.add_argument('--export_depth_images', dest='export_depth_images', action='store_true')\nparser.add_argument('--export_color_images', dest='export_color_images', action='store_true')\nparser.add_argument('--export_poses', dest='export_poses', action='store_true')\nparser.add_argument('--export_intrinsics', dest='export_intrinsics', action='store_true')\nparser.set_defaults(export_depth_images=False, export_color_images=False, export_poses=False, export_intrinsics=False)\nopt = parser.parse_args()",
        "detail": "conceptgraph.scripts.scannet_process.reader",
        "documentation": {}
    },
    {
        "label": "opt",
        "kind": 5,
        "importPath": "conceptgraph.scripts.scannet_process.reader",
        "description": "conceptgraph.scripts.scannet_process.reader",
        "peekOfCode": "opt = parser.parse_args()\nprint(opt)\ndef main():\n  if not os.path.exists(opt.output_path):\n    os.makedirs(opt.output_path)\n  # load the data\n  sys.stdout.write('loading %s...' % opt.filename)\n  sd = SensorData(opt.filename)\n  sys.stdout.write('loaded!\\n')\n  if opt.export_depth_images:",
        "detail": "conceptgraph.scripts.scannet_process.reader",
        "documentation": {}
    },
    {
        "label": "COLOR_MODE",
        "kind": 6,
        "importPath": "conceptgraph.scripts.animate_mapping_interactive",
        "description": "conceptgraph.scripts.animate_mapping_interactive",
        "peekOfCode": "class COLOR_MODE(Enum):\n    RGB = 0\n    CLASS = 1\n    INSTANCE = 2\ndef create_ball_mesh(center, radius, color=(0, 1, 0)):\n    \"\"\"\n    Create a colored mesh sphere.\n    Args:\n    - center (tuple): (x, y, z) coordinates for the center of the sphere.\n    - radius (float): Radius of the sphere.",
        "detail": "conceptgraph.scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "create_ball_mesh",
        "kind": 2,
        "importPath": "conceptgraph.scripts.animate_mapping_interactive",
        "description": "conceptgraph.scripts.animate_mapping_interactive",
        "peekOfCode": "def create_ball_mesh(center, radius, color=(0, 1, 0)):\n    \"\"\"\n    Create a colored mesh sphere.\n    Args:\n    - center (tuple): (x, y, z) coordinates for the center of the sphere.\n    - radius (float): Radius of the sphere.\n    - color (tuple): RGB values in the range [0, 1] for the color of the sphere.\n    Returns:\n    - o3d.geometry.TriangleMesh: Colored mesh sphere.\n    \"\"\"",
        "detail": "conceptgraph.scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "conceptgraph.scripts.animate_mapping_interactive",
        "description": "conceptgraph.scripts.animate_mapping_interactive",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser(description=\"Visualize a series of point clouds as an animation.\")\n    parser.add_argument(\"--input_folder\", type=str, required=True, help=\"Folder where the objects of the mapping process are stored.\")\n    parser.add_argument(\"--meta_folder\", type=str, default=None, help=\"Folder where the meta information is stored. Default the same as input_folder.\")\n    parser.add_argument(\"--edge_file\", type=str, default=None, help=\"Path to the scene graph relationship json file. If not provided, the edges will not be shown.\")\n    parser.add_argument(\"--sleep_time\", type=float, default=0.1, help=\"Time to sleep between each frame.\")\n    parser.add_argument(\"--follow_cam\", action=\"store_true\", help=\"If set, follow the camera pose.\")\n    parser.add_argument(\"--use_original_color\", action=\"store_true\", help=\"If set, will use color scheme from the CFSLAM pipeline.\")\n    parser.add_argument(\"--height_cutoff\", type=float, default=np.inf, help=\"Object nodes above this height will not be shown.\")\n    return parser",
        "detail": "conceptgraph.scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "load_frame",
        "kind": 2,
        "importPath": "conceptgraph.scripts.animate_mapping_interactive",
        "description": "conceptgraph.scripts.animate_mapping_interactive",
        "peekOfCode": "def load_frame(path):\n    global cached_frame\n    if cached_frame is None:\n        with gzip.open(path, \"rb\") as f:\n            frame = pickle.load(f)\n    else:\n        frame = cached_frame\n    if isinstance(frame, dict):\n        camera_pose = frame.get(\"camera_pose\")\n        objects = MapObjectList()",
        "detail": "conceptgraph.scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.scripts.animate_mapping_interactive",
        "description": "conceptgraph.scripts.animate_mapping_interactive",
        "peekOfCode": "def main(args):\n    # Load metadata\n    if args.meta_folder is None:\n        args.meta_folder = args.input_folder\n    meta_path = os.path.join(args.meta_folder, \"meta.pkl.gz\")\n    with gzip.open(meta_path, \"rb\") as f:\n        meta_info = pickle.load(f)\n    cfg = meta_info[\"cfg\"]\n    class_names = meta_info[\"class_names\"]\n    main.class_colors = meta_info[\"class_colors\"]",
        "detail": "conceptgraph.scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "pyqt_plugin_path",
        "kind": 5,
        "importPath": "conceptgraph.scripts.animate_mapping_interactive",
        "description": "conceptgraph.scripts.animate_mapping_interactive",
        "peekOfCode": "pyqt_plugin_path = os.path.join(os.path.dirname(PyQt5.__file__), \"Qt\", \"plugins\", \"platforms\")\nos.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = pyqt_plugin_path\nimport argparse\nimport json\nimport glob\nimport imageio\nimport natsort\nimport gzip, pickle\nfrom enum import Enum\nimport open3d as o3d",
        "detail": "conceptgraph.scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "os.environ['QT_QPA_PLATFORM_PLUGIN_PATH']",
        "kind": 5,
        "importPath": "conceptgraph.scripts.animate_mapping_interactive",
        "description": "conceptgraph.scripts.animate_mapping_interactive",
        "peekOfCode": "os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = pyqt_plugin_path\nimport argparse\nimport json\nimport glob\nimport imageio\nimport natsort\nimport gzip, pickle\nfrom enum import Enum\nimport open3d as o3d\nimport numpy as np",
        "detail": "conceptgraph.scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "cached_frame",
        "kind": 5,
        "importPath": "conceptgraph.scripts.animate_mapping_interactive",
        "description": "conceptgraph.scripts.animate_mapping_interactive",
        "peekOfCode": "cached_frame = None\ndef load_frame(path):\n    global cached_frame\n    if cached_frame is None:\n        with gzip.open(path, \"rb\") as f:\n            frame = pickle.load(f)\n    else:\n        frame = cached_frame\n    if isinstance(frame, dict):\n        camera_pose = frame.get(\"camera_pose\")",
        "detail": "conceptgraph.scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "conceptgraph.scripts.animate_mapping_save",
        "description": "conceptgraph.scripts.animate_mapping_save",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser(description=\"Visualize a series of point clouds as an animation.\")\n    parser.add_argument(\"--input_folder\", type=str, help=\"Folder where the objects of the mapping process are stored.\")\n    return parser\ndef main(args):\n    meta_path = os.path.join(args.input_folder, \"meta.pkl.gz\")\n    frame_paths = glob.glob(os.path.join(args.input_folder, \"*.pkl.gz\"))\n    frame_paths = [path for path in frame_paths if path != meta_path]\n    frame_paths = natsort.natsorted(frame_paths)\n    with gzip.open(meta_path, \"rb\") as f:",
        "detail": "conceptgraph.scripts.animate_mapping_save",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.scripts.animate_mapping_save",
        "description": "conceptgraph.scripts.animate_mapping_save",
        "peekOfCode": "def main(args):\n    meta_path = os.path.join(args.input_folder, \"meta.pkl.gz\")\n    frame_paths = glob.glob(os.path.join(args.input_folder, \"*.pkl.gz\"))\n    frame_paths = [path for path in frame_paths if path != meta_path]\n    frame_paths = natsort.natsorted(frame_paths)\n    with gzip.open(meta_path, \"rb\") as f:\n        meta_info = pickle.load(f)\n    cfg = meta_info[\"cfg\"]\n    class_names = meta_info[\"class_names\"]\n    class_colors = meta_info[\"class_colors\"]",
        "detail": "conceptgraph.scripts.animate_mapping_save",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "conceptgraph.scripts.eval_replica_semseg",
        "description": "conceptgraph.scripts.eval_replica_semseg",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--replica_root\", type=Path, default=Path(\"~/rdata/Replica/\").expanduser()\n    )\n    parser.add_argument(\n        \"--replica_semantic_root\",\n        type=Path,\n        default=Path(\"~/rdata/Replica-semantic/\").expanduser()\n    )",
        "detail": "conceptgraph.scripts.eval_replica_semseg",
        "documentation": {}
    },
    {
        "label": "eval_replica",
        "kind": 2,
        "importPath": "conceptgraph.scripts.eval_replica_semseg",
        "description": "conceptgraph.scripts.eval_replica_semseg",
        "peekOfCode": "def eval_replica(\n    scene_id: str,\n    scene_id_: str,\n    class_names: list[str],\n    class_feats: torch.Tensor,\n    args: argparse.Namespace,\n    class_all2existing: torch.Tensor,\n    ignore_index=[],\n    gt_class_only: bool = True, # only compute the conf matrix for the GT classes\n):",
        "detail": "conceptgraph.scripts.eval_replica_semseg",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.scripts.eval_replica_semseg",
        "description": "conceptgraph.scripts.eval_replica_semseg",
        "peekOfCode": "def main(args: argparse.Namespace):\n    # map REPLICA_CLASSES to REPLICA_EXISTING_CLASSES\n    class_all2existing = torch.ones(len(REPLICA_CLASSES)).long() * -1\n    for i, c in enumerate(REPLICA_EXISTING_CLASSES):\n        class_all2existing[c] = i\n    class_names = [REPLICA_CLASSES[i] for i in REPLICA_EXISTING_CLASSES]\n    if args.n_exclude == 1:\n        exclude_class = [class_names.index(c) for c in [\n            \"other\"\n        ]]",
        "detail": "conceptgraph.scripts.eval_replica_semseg",
        "documentation": {}
    },
    {
        "label": "generate_obs_from_poses",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_ai2thor_dataset",
        "description": "conceptgraph.scripts.generate_ai2thor_dataset",
        "peekOfCode": "def generate_obs_from_poses(\n    controller,\n    K,\n    sampled_poses,\n    save_root,\n    depth_scale=1000.0,\n):\n    color_path_temp = save_root + \"/color/{:06d}.png\"\n    depth_path_temp = save_root + \"/depth/{:06d}.png\"\n    instance_path_temp = save_root + \"/instance/{:06d}.png\"",
        "detail": "conceptgraph.scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "sample_pose_from_file",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_ai2thor_dataset",
        "description": "conceptgraph.scripts.generate_ai2thor_dataset",
        "peekOfCode": "def sample_pose_from_file(traj_file):\n    # Load the trajectory file (json)\n    with open(traj_file, \"r\") as f:\n        traj = json.load(f)\n    sampled_poses = []\n    for log in traj[\"agent_logs\"]:\n        sampled_poses.append(\n            {\n                \"position\": log[\"position\"],\n                \"rotation\": log[\"rotation\"],",
        "detail": "conceptgraph.scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "is_removeable",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_ai2thor_dataset",
        "description": "conceptgraph.scripts.generate_ai2thor_dataset",
        "peekOfCode": "def is_removeable(obj, level: int):\n    if level == 1: # all objects except those in NOT_TO_REMOVE\n        return obj['objectType'] not in NOT_TO_REMOVE\n    elif level == 2: # objects that are pickupable or moveable\n        return obj['pickupable'] or obj['moveable']\n    elif level == 3: # objects that are pickupable\n        return obj['pickupable']\ndef randomize_scene(args, controller) -> list[str]|None:\n    '''\n    Since we want to keep track of which objects are removed, but it is not done in ai2thor",
        "detail": "conceptgraph.scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "randomize_scene",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_ai2thor_dataset",
        "description": "conceptgraph.scripts.generate_ai2thor_dataset",
        "peekOfCode": "def randomize_scene(args, controller) -> list[str]|None:\n    '''\n    Since we want to keep track of which objects are removed, but it is not done in ai2thor\n    So we will keep of a list of object ids that are kept in the scene. \n    if no object is removed from the scene, then return None.\n    '''\n    if args.randomize_lighting:\n        controller.step(\n            action=\"RandomizeLighting\",\n            brightness=(0.5, 1.5),",
        "detail": "conceptgraph.scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "randomize_scene_from_log",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_ai2thor_dataset",
        "description": "conceptgraph.scripts.generate_ai2thor_dataset",
        "peekOfCode": "def randomize_scene_from_log(controller, randomization_log):\n    if randomization_log['randomize_lighting']:\n        warnings.warn(\"randomize_lighting from log file is not implemented yet\")\n    if randomization_log['randomize_material']:\n        warnings.warn(\"randomize_material from log file is not implemented yet\")\n    # Remove some objects\n    removed_object_ids = randomization_log['removed_object_ids']\n    if len(removed_object_ids) > 0:\n        for obj_id in removed_object_ids:\n            event = controller.step(",
        "detail": "conceptgraph.scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "load_or_randomize_scene",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_ai2thor_dataset",
        "description": "conceptgraph.scripts.generate_ai2thor_dataset",
        "peekOfCode": "def load_or_randomize_scene(args, controller):\n    randomization_file_path = args.save_root + \"/randomization.json\"\n    if os.path.exists(randomization_file_path):\n        with open(randomization_file_path, \"r\") as f:\n            randomization_log = json.load(f)\n        randomize_scene_from_log(controller, randomization_log)\n        print(\"Loaded Randomization from {}\".format(randomization_file_path))\n    else:\n        randomization_log = randomize_scene(args, controller)\n        with open(randomization_file_path, \"w\") as f:",
        "detail": "conceptgraph.scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_ai2thor_dataset",
        "description": "conceptgraph.scripts.generate_ai2thor_dataset",
        "peekOfCode": "def main(args: argparse.Namespace):\n    save_folder_name = (\n        args.scene_name\n        if args.save_suffix is None\n        else args.scene_name + \"_\" + args.save_suffix\n    )\n    save_root = args.dataset_root + \"/\" + save_folder_name + \"/\"\n    os.makedirs(save_root, exist_ok=True)\n    args.save_folder_name = save_folder_name\n    args.save_root = save_root",
        "detail": "conceptgraph.scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "main_interact",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_ai2thor_dataset",
        "description": "conceptgraph.scripts.generate_ai2thor_dataset",
        "peekOfCode": "def main_interact(args: argparse.Namespace):\n    '''\n    Interact with the AI2Thor simulator, navigating the robot. \n    The agent trajectory will be saved to a file as a file. \n    Note that this saves the agent pose but not the camera pose. \n    '''\n    save_folder_name = (\n        args.scene_name + \"_interact\"\n        if args.save_suffix is None\n        else args.scene_name + \"_\" + args.save_suffix",
        "detail": "conceptgraph.scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_ai2thor_dataset",
        "description": "conceptgraph.scripts.generate_ai2thor_dataset",
        "peekOfCode": "def get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser(description=\"Program Arguments\")\n    parser.add_argument(\n        \"--dataset_root\",\n        default=str(Path(\"~/ldata/ai2thor/\").expanduser()),\n        help=\"The root path to the dataset.\",\n    )\n    parser.add_argument(\n        \"--grid_size\",\n        default=0.5,",
        "detail": "conceptgraph.scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "NOT_TO_REMOVE",
        "kind": 5,
        "importPath": "conceptgraph.scripts.generate_ai2thor_dataset",
        "description": "conceptgraph.scripts.generate_ai2thor_dataset",
        "peekOfCode": "NOT_TO_REMOVE = [\n    \"Wall\",\n    \"Floor\",\n    \"Window\",\n    \"Doorway\",\n    \"Room\",\n]\ndef generate_obs_from_poses(\n    controller,\n    K,",
        "detail": "conceptgraph.scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "def get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dataset_root\", type=Path, required=True,\n    )\n    parser.add_argument(\n        \"--dataset_config\", type=str, required=True,\n        help=\"This path may need to be changed depending on where you run this script. \"\n    )\n    parser.add_argument(\"--scene_id\", type=str, default=\"train_3\")",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "get_sam_segmentation_from_xyxy",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "def get_sam_segmentation_from_xyxy(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n    sam_predictor.set_image(image)\n    result_masks = []\n    for box in xyxy:\n        masks, scores, logits = sam_predictor.predict(\n            box=box,\n            multimask_output=True\n        )\n        index = np.argmax(scores)\n        result_masks.append(masks[index])",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "get_sam_predictor",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "def get_sam_predictor(variant: str, device: str | int) -> SamPredictor:\n    if variant == \"sam\":\n        sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH)\n        sam.to(device)\n        sam_predictor = SamPredictor(sam)\n        return sam_predictor\n    if variant == \"mobilesam\":\n        from MobileSAM.setup_mobile_sam import setup_model\n        MOBILE_SAM_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./EfficientSAM/mobile_sam.pt\")\n        checkpoint = torch.load(MOBILE_SAM_CHECKPOINT_PATH)",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "get_sam_segmentation_dense",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "def get_sam_segmentation_dense(\n    variant:str, model: Any, image: np.ndarray\n) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n    '''\n    The SAM based on automatic mask generation, without bbox prompting\n    Args:\n        model: The mask generator or the YOLO model\n        image: )H, W, 3), in RGB color space, in range [0, 255]\n    Returns:\n        mask: (N, H, W)",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "get_sam_mask_generator",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "def get_sam_mask_generator(variant:str, device: str | int) -> SamAutomaticMaskGenerator:\n    if variant == \"sam\":\n        sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH)\n        sam.to(device)\n        mask_generator = SamAutomaticMaskGenerator(\n            model=sam,\n            points_per_side=12,\n            points_per_batch=144,\n            pred_iou_thresh=0.88,\n            stability_score_thresh=0.95,",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "process_tag_classes",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "def process_tag_classes(text_prompt:str, add_classes:List[str]=[], remove_classes:List[str]=[]) -> list[str]:\n    '''\n    Convert a text prompt from Tag2Text to a list of classes. \n    '''\n    classes = text_prompt.split(',')\n    classes = [obj_class.strip() for obj_class in classes]\n    classes = [obj_class for obj_class in classes if obj_class != '']\n    for c in add_classes:\n        if c not in classes:\n            classes.append(c)",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "process_ai2thor_classes",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "def process_ai2thor_classes(classes: List[str], add_classes:List[str]=[], remove_classes:List[str]=[]) -> List[str]:\n    '''\n    Some pre-processing on AI2Thor objectTypes in a scene\n    '''\n    classes = list(set(classes))\n    for c in add_classes:\n        classes.append(c)\n    for c in remove_classes:\n        classes = [obj_class for obj_class in classes if c not in obj_class.lower()]\n    # Split the element in classes by captical letters",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "def main(args: argparse.Namespace):\n    ### Initialize the Grounding DINO model ###\n    grounding_dino_model = Model(\n        model_config_path=GROUNDING_DINO_CONFIG_PATH, \n        model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH, \n        device=args.device\n    )\n    ### Initialize the SAM model ###\n    if args.class_set == \"none\":\n        mask_generator = get_sam_mask_generator(args.sam_variant, args.device)",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "TAG2TEXT_PATH",
        "kind": 5,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "TAG2TEXT_PATH = os.path.join(GSA_PATH, \"\")\nEFFICIENTSAM_PATH = os.path.join(GSA_PATH, \"EfficientSAM\")\nsys.path.append(GSA_PATH) # This is needed for the following imports in this file\nsys.path.append(TAG2TEXT_PATH) # This is needed for some imports in the Tag2Text files\nsys.path.append(EFFICIENTSAM_PATH)\nimport torchvision.transforms as TS\ntry:\n    sys.path.append('/home/kev/packages/Grounded-Segment-Anything/recognize-anything')\n    from ram.models import ram\n    from ram.models import tag2text",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "EFFICIENTSAM_PATH",
        "kind": 5,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "EFFICIENTSAM_PATH = os.path.join(GSA_PATH, \"EfficientSAM\")\nsys.path.append(GSA_PATH) # This is needed for the following imports in this file\nsys.path.append(TAG2TEXT_PATH) # This is needed for some imports in the Tag2Text files\nsys.path.append(EFFICIENTSAM_PATH)\nimport torchvision.transforms as TS\ntry:\n    sys.path.append('/home/kev/packages/Grounded-Segment-Anything/recognize-anything')\n    from ram.models import ram\n    from ram.models import tag2text\n    from ram import inference_tag2text, inference_ram",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "GROUNDING_DINO_CONFIG_PATH",
        "kind": 5,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "GROUNDING_DINO_CONFIG_PATH = os.path.join(GSA_PATH, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\nGROUNDING_DINO_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./groundingdino_swint_ogc.pth\")\n# Segment-Anything checkpoint\nSAM_ENCODER_VERSION = \"vit_h\"\nSAM_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./sam_vit_h_4b8939.pth\")\n# Tag2Text checkpoint\nTAG2TEXT_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./tag2text_swin_14m.pth\")\nRAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "GROUNDING_DINO_CHECKPOINT_PATH",
        "kind": 5,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./groundingdino_swint_ogc.pth\")\n# Segment-Anything checkpoint\nSAM_ENCODER_VERSION = \"vit_h\"\nSAM_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./sam_vit_h_4b8939.pth\")\n# Tag2Text checkpoint\nTAG2TEXT_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./tag2text_swin_14m.pth\")\nRAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "SAM_ENCODER_VERSION",
        "kind": 5,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "SAM_ENCODER_VERSION = \"vit_h\"\nSAM_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./sam_vit_h_4b8939.pth\")\n# Tag2Text checkpoint\nTAG2TEXT_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./tag2text_swin_14m.pth\")\nRAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]\nFOREGROUND_MINIMAL_CLASSES = [\n    \"item\"",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "SAM_CHECKPOINT_PATH",
        "kind": 5,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "SAM_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./sam_vit_h_4b8939.pth\")\n# Tag2Text checkpoint\nTAG2TEXT_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./tag2text_swin_14m.pth\")\nRAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]\nFOREGROUND_MINIMAL_CLASSES = [\n    \"item\"\n]",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "TAG2TEXT_CHECKPOINT_PATH",
        "kind": 5,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "TAG2TEXT_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./tag2text_swin_14m.pth\")\nRAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]\nFOREGROUND_MINIMAL_CLASSES = [\n    \"item\"\n]\ndef get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "RAM_CHECKPOINT_PATH",
        "kind": 5,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "RAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]\nFOREGROUND_MINIMAL_CLASSES = [\n    \"item\"\n]\ndef get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "FOREGROUND_GENERIC_CLASSES",
        "kind": 5,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "FOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]\nFOREGROUND_MINIMAL_CLASSES = [\n    \"item\"\n]\ndef get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dataset_root\", type=Path, required=True,",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "FOREGROUND_MINIMAL_CLASSES",
        "kind": 5,
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "peekOfCode": "FOREGROUND_MINIMAL_CLASSES = [\n    \"item\"\n]\ndef get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dataset_root\", type=Path, required=True,\n    )\n    parser.add_argument(\n        \"--dataset_config\", type=str, required=True,",
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "encode_image_for_openai",
        "kind": 2,
        "importPath": "conceptgraph.scripts.gpt4v_test",
        "description": "conceptgraph.scripts.gpt4v_test",
        "peekOfCode": "def encode_image_for_openai(image_path: str):\n    # check if the image exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n# Getting the base64 string\nimg_path = '/home/kuwajerw/new_local_data/new_record3d/ali_apartment/apt_scan_no_smooth_processed/exps/s_detections_stride1_69/vis/160_for_vlm.jpg'\nbase64_image = encode_image_for_openai(str(img_path))\n# Initialize OpenAI client with the new format",
        "detail": "conceptgraph.scripts.gpt4v_test",
        "documentation": {}
    },
    {
        "label": "analyze_image",
        "kind": 2,
        "importPath": "conceptgraph.scripts.gpt4v_test",
        "description": "conceptgraph.scripts.gpt4v_test",
        "peekOfCode": "def analyze_image():\n    response = client.chat.completions.create(\n    model=\"gpt-4-vision-preview\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt\n        },\n        {\n            \"role\": \"user\",",
        "detail": "conceptgraph.scripts.gpt4v_test",
        "documentation": {}
    },
    {
        "label": "img_path",
        "kind": 5,
        "importPath": "conceptgraph.scripts.gpt4v_test",
        "description": "conceptgraph.scripts.gpt4v_test",
        "peekOfCode": "img_path = '/home/kuwajerw/new_local_data/new_record3d/ali_apartment/apt_scan_no_smooth_processed/exps/s_detections_stride1_69/vis/160_for_vlm.jpg'\nbase64_image = encode_image_for_openai(str(img_path))\n# Initialize OpenAI client with the new format\nclient = OpenAI(\n  api_key=os.getenv('OPENAI_API_KEY')\n)\nsystem_prompt = '''\n    You are an agent specialized in describing the spatial relationships between objects in an annotated image.\n    You will be provided with an annotated image and a list of labels for the annotations. Your task is to determine the spatial relationships between the annotated objects in the image, and return a list of these relationships in the correct list of tuples format as follows:\n    [(\"object1\", \"spatial relationship\", \"object2\"), (\"object3\", \"spatial relationship\", \"object4\"), ...]",
        "detail": "conceptgraph.scripts.gpt4v_test",
        "documentation": {}
    },
    {
        "label": "base64_image",
        "kind": 5,
        "importPath": "conceptgraph.scripts.gpt4v_test",
        "description": "conceptgraph.scripts.gpt4v_test",
        "peekOfCode": "base64_image = encode_image_for_openai(str(img_path))\n# Initialize OpenAI client with the new format\nclient = OpenAI(\n  api_key=os.getenv('OPENAI_API_KEY')\n)\nsystem_prompt = '''\n    You are an agent specialized in describing the spatial relationships between objects in an annotated image.\n    You will be provided with an annotated image and a list of labels for the annotations. Your task is to determine the spatial relationships between the annotated objects in the image, and return a list of these relationships in the correct list of tuples format as follows:\n    [(\"object1\", \"spatial relationship\", \"object2\"), (\"object3\", \"spatial relationship\", \"object4\"), ...]\n    Your options for the spatial relationship are \"on top of\" and \"next to\".",
        "detail": "conceptgraph.scripts.gpt4v_test",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "conceptgraph.scripts.gpt4v_test",
        "description": "conceptgraph.scripts.gpt4v_test",
        "peekOfCode": "client = OpenAI(\n  api_key=os.getenv('OPENAI_API_KEY')\n)\nsystem_prompt = '''\n    You are an agent specialized in describing the spatial relationships between objects in an annotated image.\n    You will be provided with an annotated image and a list of labels for the annotations. Your task is to determine the spatial relationships between the annotated objects in the image, and return a list of these relationships in the correct list of tuples format as follows:\n    [(\"object1\", \"spatial relationship\", \"object2\"), (\"object3\", \"spatial relationship\", \"object4\"), ...]\n    Your options for the spatial relationship are \"on top of\" and \"next to\".\n    For example, you may get an annotated image and a list such as \n    [\"cup 3\", \"book 4\", \"clock 5\", \"table 2\", \"candle 7\", \"music stand 6\", \"lamp 8\"]",
        "detail": "conceptgraph.scripts.gpt4v_test",
        "documentation": {}
    },
    {
        "label": "system_prompt",
        "kind": 5,
        "importPath": "conceptgraph.scripts.gpt4v_test",
        "description": "conceptgraph.scripts.gpt4v_test",
        "peekOfCode": "system_prompt = '''\n    You are an agent specialized in describing the spatial relationships between objects in an annotated image.\n    You will be provided with an annotated image and a list of labels for the annotations. Your task is to determine the spatial relationships between the annotated objects in the image, and return a list of these relationships in the correct list of tuples format as follows:\n    [(\"object1\", \"spatial relationship\", \"object2\"), (\"object3\", \"spatial relationship\", \"object4\"), ...]\n    Your options for the spatial relationship are \"on top of\" and \"next to\".\n    For example, you may get an annotated image and a list such as \n    [\"cup 3\", \"book 4\", \"clock 5\", \"table 2\", \"candle 7\", \"music stand 6\", \"lamp 8\"]\n    Your response should be a description of the spatial relationships between the objects in the image. \n    An example to illustrate the response format:\n    [(\"book 4\", \"on top of\", \"table 2\"), (\"cup 3\", \"next to\", \"book 4\"), (\"lamp 8\", \"on top of\", \"music stand 6\")]",
        "detail": "conceptgraph.scripts.gpt4v_test",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "conceptgraph.scripts.gpt4v_test",
        "description": "conceptgraph.scripts.gpt4v_test",
        "peekOfCode": "result = analyze_image()\nprint(result)",
        "detail": "conceptgraph.scripts.gpt4v_test",
        "documentation": {}
    },
    {
        "label": "model_id",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "model_id = \"llava-hf/llava-1.5-13b-hf\"\n# image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=True, \n    load_in_8bit=True\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n# raw_image = Image.open(requests.get(image_file, stream=True).raw)",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "model = LlavaForConditionalGeneration.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=True, \n    load_in_8bit=True\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n# raw_image = Image.open(requests.get(image_file, stream=True).raw)\nimg_path = '/home/kuwajerw/new_local_data/new_record3d/ali_apartment/apt_scan_no_smooth_processed/exps/s_detections_stride1_69/vis/160_for_vlm.jpg'\nraw_image = Image.open(img_path).convert('RGB')",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "processor",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "processor = AutoProcessor.from_pretrained(model_id)\n# raw_image = Image.open(requests.get(image_file, stream=True).raw)\nimg_path = '/home/kuwajerw/new_local_data/new_record3d/ali_apartment/apt_scan_no_smooth_processed/exps/s_detections_stride1_69/vis/160_for_vlm.jpg'\nraw_image = Image.open(img_path).convert('RGB')\nlabels = ['power outlet 1', 'backpack 2', 'computer tower 3', 'poster 4', 'desk 5', 'picture 6', 'bowl 7', 'folded chair 8', 'trash bin 9', 'tissue box 10']\nexample_labels = []\n# inputs = processor(prompt, raw_image, return_tensors='pt').to(\"cuda\", torch.float16)\nsystem_prompt = \"What follows is a chat between a human and an artificial intelligence assistant. The assistant always answers the question in the required format\"\nexample_prompt = \"\"\nuser_prompt = f\"In this picture, there are these annotated objects, labels: {labels}. Are any of these annotated objects (not the labels, the objects) on top of one another in this picture?\"",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "img_path",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "img_path = '/home/kuwajerw/new_local_data/new_record3d/ali_apartment/apt_scan_no_smooth_processed/exps/s_detections_stride1_69/vis/160_for_vlm.jpg'\nraw_image = Image.open(img_path).convert('RGB')\nlabels = ['power outlet 1', 'backpack 2', 'computer tower 3', 'poster 4', 'desk 5', 'picture 6', 'bowl 7', 'folded chair 8', 'trash bin 9', 'tissue box 10']\nexample_labels = []\n# inputs = processor(prompt, raw_image, return_tensors='pt').to(\"cuda\", torch.float16)\nsystem_prompt = \"What follows is a chat between a human and an artificial intelligence assistant. The assistant always answers the question in the required format\"\nexample_prompt = \"\"\nuser_prompt = f\"In this picture, there are these annotated objects, labels: {labels}. Are any of these annotated objects (not the labels, the objects) on top of one another in this picture?\"\nprompt = f\"{system_prompt} USER: <image>\\n{user_prompt} ASSISTANT:\"\nprint(f\"Line 263, prompt: {prompt}\")",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "raw_image",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "raw_image = Image.open(img_path).convert('RGB')\nlabels = ['power outlet 1', 'backpack 2', 'computer tower 3', 'poster 4', 'desk 5', 'picture 6', 'bowl 7', 'folded chair 8', 'trash bin 9', 'tissue box 10']\nexample_labels = []\n# inputs = processor(prompt, raw_image, return_tensors='pt').to(\"cuda\", torch.float16)\nsystem_prompt = \"What follows is a chat between a human and an artificial intelligence assistant. The assistant always answers the question in the required format\"\nexample_prompt = \"\"\nuser_prompt = f\"In this picture, there are these annotated objects, labels: {labels}. Are any of these annotated objects (not the labels, the objects) on top of one another in this picture?\"\nprompt = f\"{system_prompt} USER: <image>\\n{user_prompt} ASSISTANT:\"\nprint(f\"Line 263, prompt: {prompt}\")\ninputs = processor(prompt, raw_image, return_tensors='pt')",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "labels",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "labels = ['power outlet 1', 'backpack 2', 'computer tower 3', 'poster 4', 'desk 5', 'picture 6', 'bowl 7', 'folded chair 8', 'trash bin 9', 'tissue box 10']\nexample_labels = []\n# inputs = processor(prompt, raw_image, return_tensors='pt').to(\"cuda\", torch.float16)\nsystem_prompt = \"What follows is a chat between a human and an artificial intelligence assistant. The assistant always answers the question in the required format\"\nexample_prompt = \"\"\nuser_prompt = f\"In this picture, there are these annotated objects, labels: {labels}. Are any of these annotated objects (not the labels, the objects) on top of one another in this picture?\"\nprompt = f\"{system_prompt} USER: <image>\\n{user_prompt} ASSISTANT:\"\nprint(f\"Line 263, prompt: {prompt}\")\ninputs = processor(prompt, raw_image, return_tensors='pt')\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "example_labels",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "example_labels = []\n# inputs = processor(prompt, raw_image, return_tensors='pt').to(\"cuda\", torch.float16)\nsystem_prompt = \"What follows is a chat between a human and an artificial intelligence assistant. The assistant always answers the question in the required format\"\nexample_prompt = \"\"\nuser_prompt = f\"In this picture, there are these annotated objects, labels: {labels}. Are any of these annotated objects (not the labels, the objects) on top of one another in this picture?\"\nprompt = f\"{system_prompt} USER: <image>\\n{user_prompt} ASSISTANT:\"\nprint(f\"Line 263, prompt: {prompt}\")\ninputs = processor(prompt, raw_image, return_tensors='pt')\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "system_prompt",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "system_prompt = \"What follows is a chat between a human and an artificial intelligence assistant. The assistant always answers the question in the required format\"\nexample_prompt = \"\"\nuser_prompt = f\"In this picture, there are these annotated objects, labels: {labels}. Are any of these annotated objects (not the labels, the objects) on top of one another in this picture?\"\nprompt = f\"{system_prompt} USER: <image>\\n{user_prompt} ASSISTANT:\"\nprint(f\"Line 263, prompt: {prompt}\")\ninputs = processor(prompt, raw_image, return_tensors='pt')\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\nk=1",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "example_prompt",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "example_prompt = \"\"\nuser_prompt = f\"In this picture, there are these annotated objects, labels: {labels}. Are any of these annotated objects (not the labels, the objects) on top of one another in this picture?\"\nprompt = f\"{system_prompt} USER: <image>\\n{user_prompt} ASSISTANT:\"\nprint(f\"Line 263, prompt: {prompt}\")\ninputs = processor(prompt, raw_image, return_tensors='pt')\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\nk=1",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "user_prompt",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "user_prompt = f\"In this picture, there are these annotated objects, labels: {labels}. Are any of these annotated objects (not the labels, the objects) on top of one another in this picture?\"\nprompt = f\"{system_prompt} USER: <image>\\n{user_prompt} ASSISTANT:\"\nprint(f\"Line 263, prompt: {prompt}\")\ninputs = processor(prompt, raw_image, return_tensors='pt')\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\nk=1",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "prompt = f\"{system_prompt} USER: <image>\\n{user_prompt} ASSISTANT:\"\nprint(f\"Line 263, prompt: {prompt}\")\ninputs = processor(prompt, raw_image, return_tensors='pt')\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\nk=1",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "inputs = processor(prompt, raw_image, return_tensors='pt')\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\nk=1",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "conceptgraph.scripts.lava_15_test",
        "description": "conceptgraph.scripts.lava_15_test",
        "peekOfCode": "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\nk=1",
        "detail": "conceptgraph.scripts.lava_15_test",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "conceptgraph.scripts.run_post_filter_merge",
        "description": "conceptgraph.scripts.run_post_filter_merge",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--result_path\", type=str, required=True)\n    parser.add_argument(\"--rgb_pcd_path\", type=str, default=None)\n    # To inspect the results of merge_overlap_objects\n    # This is mainly to quickly try out different thresholds\n    parser.add_argument(\"--merge_overlap_thresh\", type=float, default=-1)\n    parser.add_argument(\"--merge_visual_sim_thresh\", type=float, default=-1)\n    parser.add_argument(\"--merge_text_sim_thresh\", type=float, default=-1)\n    parser.add_argument(\"--obj_min_points\", type=int, default=0)",
        "detail": "conceptgraph.scripts.run_post_filter_merge",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "conceptgraph.scripts.run_slam_rgb",
        "description": "conceptgraph.scripts.run_slam_rgb",
        "peekOfCode": "def get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dataset_root\", type=Path, required=True,\n    )\n    parser.add_argument(\n        \"--dataset_config\", type=str, required=True,\n        help=\"This path may need to be changed depending on where you run this script. \"\n    )\n    parser.add_argument(\"--scene_id\", type=str, default=\"train_3\")",
        "detail": "conceptgraph.scripts.run_slam_rgb",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.scripts.run_slam_rgb",
        "description": "conceptgraph.scripts.run_slam_rgb",
        "peekOfCode": "def main(args: argparse.Namespace):\n    if args.load_semseg:\n        load_embeddings = True\n        embedding_dir = \"embed_semseg\"\n        semseg_classes = json.load(open(\n            args.dataset_root / args.scene_id / \"embed_semseg_classes.json\", \"r\"\n        ))\n        embedding_dim = len(semseg_classes)\n    else:\n        load_embeddings = False",
        "detail": "conceptgraph.scripts.run_slam_rgb",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.scripts.streamlined_detections",
        "description": "conceptgraph.scripts.streamlined_detections",
        "peekOfCode": "def main(cfg : DictConfig):\n    # Initialize the dataset\n    dataset = get_dataset(\n        dataconfig=cfg.dataset_config,\n        start=cfg.start,\n        end=cfg.end,\n        stride=cfg.stride,\n        basedir=cfg.dataset_root,\n        sequence=cfg.scene_id,\n        desired_height=cfg.desired_height,",
        "detail": "conceptgraph.scripts.streamlined_detections",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "conceptgraph.scripts.visualize_cfslam_interact_llava",
        "description": "conceptgraph.scripts.visualize_cfslam_interact_llava",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--result_path\", type=str, required=True)\n    parser.add_argument(\"--rgb_pcd_path\", type=str, default=None)\n    return parser\nif __name__ == \"__main__\":\n    parser = get_parser()\n    args = parser.parse_args()\n    # rich console for pretty printing\n    # console = rich.console.Console()",
        "detail": "conceptgraph.scripts.visualize_cfslam_interact_llava",
        "documentation": {}
    },
    {
        "label": "create_ball_mesh",
        "kind": 2,
        "importPath": "conceptgraph.scripts.visualize_cfslam_results",
        "description": "conceptgraph.scripts.visualize_cfslam_results",
        "peekOfCode": "def create_ball_mesh(center, radius, color=(0, 1, 0)):\n    \"\"\"\n    Create a colored mesh sphere.\n    Args:\n    - center (tuple): (x, y, z) coordinates for the center of the sphere.\n    - radius (float): Radius of the sphere.\n    - color (tuple): RGB values in the range [0, 1] for the color of the sphere.\n    Returns:\n    - o3d.geometry.TriangleMesh: Colored mesh sphere.\n    \"\"\"",
        "detail": "conceptgraph.scripts.visualize_cfslam_results",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "conceptgraph.scripts.visualize_cfslam_results",
        "description": "conceptgraph.scripts.visualize_cfslam_results",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--result_path\", type=str, default=None)\n    parser.add_argument(\"--rgb_pcd_path\", type=str, default=None)\n    parser.add_argument(\"--edge_file\", type=str, default=None)\n    parser.add_argument(\"--no_clip\", action=\"store_true\", \n                        help=\"If set, the CLIP model will not init for fast debugging.\")\n    # To inspect the results of merge_overlap_objects\n    # This is mainly to quickly try out different thresholds\n    parser.add_argument(\"--merge_overlap_thresh\", type=float, default=-1)",
        "detail": "conceptgraph.scripts.visualize_cfslam_results",
        "documentation": {}
    },
    {
        "label": "load_result",
        "kind": 2,
        "importPath": "conceptgraph.scripts.visualize_cfslam_results",
        "description": "conceptgraph.scripts.visualize_cfslam_results",
        "peekOfCode": "def load_result(result_path):\n    # check if theres a potential symlink for result_path and resolve it\n    potential_path = os.path.realpath(result_path)\n    if potential_path != result_path:\n        print(f\"Resolved symlink for result_path: {result_path} -> \\n{potential_path}\")\n        result_path = potential_path\n    with gzip.open(result_path, \"rb\") as f:\n        results = pickle.load(f)\n    if not isinstance(results, dict):\n        raise ValueError(\"Results should be a dictionary! other types are not supported!\")",
        "detail": "conceptgraph.scripts.visualize_cfslam_results",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.scripts.visualize_cfslam_results",
        "description": "conceptgraph.scripts.visualize_cfslam_results",
        "peekOfCode": "def main(args):\n    result_path = args.result_path\n    rgb_pcd_path = args.rgb_pcd_path\n    assert not (result_path is None and rgb_pcd_path is None), \\\n        \"Either result_path or rgb_pcd_path must be provided.\"\n    if rgb_pcd_path is not None:        \n        pointclouds = Pointclouds.load_pointcloud_from_h5(rgb_pcd_path)\n        global_pcd = pointclouds.open3d(0, include_colors=True)\n        if result_path is None:\n            print(\"Only visualizing the pointcloud...\")",
        "detail": "conceptgraph.scripts.visualize_cfslam_results",
        "documentation": {}
    },
    {
        "label": "compute_match_batch",
        "kind": 2,
        "importPath": "conceptgraph.slam.cfslam_pipeline_batch",
        "description": "conceptgraph.slam.cfslam_pipeline_batch",
        "peekOfCode": "def compute_match_batch(cfg, spatial_sim: torch.Tensor, visual_sim: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute object association based on spatial and visual similarities\n    Args:\n        spatial_sim: a MxN tensor of spatial similarities\n        visual_sim: a MxN tensor of visual similarities\n    Returns:\n        A MxN tensor of binary values, indicating whether a detection is associate with an object. \n        Each row has at most one 1, indicating one detection can be associated with at most one existing object.\n        One existing object can receive multiple new detections",
        "detail": "conceptgraph.slam.cfslam_pipeline_batch",
        "documentation": {}
    },
    {
        "label": "prepare_objects_save_vis",
        "kind": 2,
        "importPath": "conceptgraph.slam.cfslam_pipeline_batch",
        "description": "conceptgraph.slam.cfslam_pipeline_batch",
        "peekOfCode": "def prepare_objects_save_vis(objects: MapObjectList, downsample_size: float=0.025):\n    objects_to_save = copy.deepcopy(objects)\n    # Downsample the point cloud\n    for i in range(len(objects_to_save)):\n        objects_to_save[i]['pcd'] = objects_to_save[i]['pcd'].voxel_down_sample(downsample_size)\n    # Remove unnecessary keys\n    for i in range(len(objects_to_save)):\n        for k in list(objects_to_save[i].keys()):\n            if k not in [\n                'pcd', 'bbox', 'clip_ft', 'text_ft', 'class_id', 'num_detections', 'inst_color'",
        "detail": "conceptgraph.slam.cfslam_pipeline_batch",
        "documentation": {}
    },
    {
        "label": "process_cfg",
        "kind": 2,
        "importPath": "conceptgraph.slam.cfslam_pipeline_batch",
        "description": "conceptgraph.slam.cfslam_pipeline_batch",
        "peekOfCode": "def process_cfg(cfg: DictConfig):\n    cfg.dataset_root = Path(cfg.dataset_root)\n    cfg.dataset_config = Path(cfg.dataset_config)\n    if cfg.dataset_config.name != \"multiscan.yaml\":\n        # For datasets whose depth and RGB have the same resolution\n        # Set the desired image heights and width from the dataset config\n        dataset_cfg = omegaconf.OmegaConf.load(cfg.dataset_config)\n        if cfg.image_height is None:\n            cfg.image_height = dataset_cfg.camera_params.image_height\n        if cfg.image_width is None:",
        "detail": "conceptgraph.slam.cfslam_pipeline_batch",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.slam.cfslam_pipeline_batch",
        "description": "conceptgraph.slam.cfslam_pipeline_batch",
        "peekOfCode": "def main(cfg : DictConfig):\n    cfg = process_cfg(cfg)\n    # Initialize the dataset\n    dataset = get_dataset(\n        dataconfig=cfg.dataset_config,\n        start=cfg.start,\n        end=cfg.end,\n        stride=cfg.stride,\n        basedir=cfg.dataset_root,\n        sequence=cfg.scene_id,",
        "detail": "conceptgraph.slam.cfslam_pipeline_batch",
        "documentation": {}
    },
    {
        "label": "BG_CLASSES",
        "kind": 5,
        "importPath": "conceptgraph.slam.cfslam_pipeline_batch",
        "description": "conceptgraph.slam.cfslam_pipeline_batch",
        "peekOfCode": "BG_CLASSES = [\"wall\", \"floor\", \"ceiling\"]\n# Disable torch gradient computation\ntorch.set_grad_enabled(False)\ndef compute_match_batch(cfg, spatial_sim: torch.Tensor, visual_sim: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute object association based on spatial and visual similarities\n    Args:\n        spatial_sim: a MxN tensor of spatial similarities\n        visual_sim: a MxN tensor of visual similarities\n    Returns:",
        "detail": "conceptgraph.slam.cfslam_pipeline_batch",
        "documentation": {}
    },
    {
        "label": "MultiWinApp",
        "kind": 6,
        "importPath": "conceptgraph.slam.gui_realtime_mapping",
        "description": "conceptgraph.slam.gui_realtime_mapping",
        "peekOfCode": "class MultiWinApp:\n    def __init__(\n        self,\n        cfg,\n        owandb,\n        dataset,\n        objects,\n        exp_out_path,\n        det_exp_path,\n        detections_exp_cfg,",
        "detail": "conceptgraph.slam.gui_realtime_mapping",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.slam.gui_realtime_mapping",
        "description": "conceptgraph.slam.gui_realtime_mapping",
        "peekOfCode": "def main(cfg : DictConfig):\n    tracker = MappingTracker()\n    owandb = OptionalWandB()\n    owandb.set_use_wandb(cfg.use_wandb)\n    owandb.init(project=\"concept-graphs\", \n            #    entity=\"concept-graphs\",\n                config=cfg_to_dict(cfg),\n               )\n    cfg = process_cfg(cfg)\n    # Initialize the dataset",
        "detail": "conceptgraph.slam.gui_realtime_mapping",
        "documentation": {}
    },
    {
        "label": "CLOUD_NAME",
        "kind": 5,
        "importPath": "conceptgraph.slam.gui_realtime_mapping",
        "description": "conceptgraph.slam.gui_realtime_mapping",
        "peekOfCode": "CLOUD_NAME = \"points\"\n@hydra.main(version_base=None, config_path=\"../hydra_configs/\", config_name=\"gui_realtime_mapping\")\ndef main(cfg : DictConfig):\n    tracker = MappingTracker()\n    owandb = OptionalWandB()\n    owandb.set_use_wandb(cfg.use_wandb)\n    owandb.init(project=\"concept-graphs\", \n            #    entity=\"concept-graphs\",\n                config=cfg_to_dict(cfg),\n               )",
        "detail": "conceptgraph.slam.gui_realtime_mapping",
        "documentation": {}
    },
    {
        "label": "compute_spatial_similarities",
        "kind": 2,
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "peekOfCode": "def compute_spatial_similarities(spatial_sim_type: str, detection_list: DetectionList, objects: MapObjectList, downsample_voxel_size) -> torch.Tensor:\n    det_bboxes = detection_list.get_stacked_values_torch('bbox')\n    obj_bboxes = objects.get_stacked_values_torch('bbox')\n    if spatial_sim_type == \"iou\":\n        spatial_sim = compute_iou_batch(det_bboxes, obj_bboxes)\n    elif spatial_sim_type == \"giou\":\n        spatial_sim = compute_giou_batch(det_bboxes, obj_bboxes)\n    elif spatial_sim_type == \"iou_accurate\":\n        spatial_sim = compute_3d_iou_accurate_batch(det_bboxes, obj_bboxes)\n    elif spatial_sim_type == \"giou_accurate\":",
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_visual_similarities",
        "kind": 2,
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "peekOfCode": "def compute_visual_similarities(detection_list: DetectionList, objects: MapObjectList) -> torch.Tensor:\n    '''\n    Compute the visual similarities between the detections and the objects\n    Args:\n        detection_list: a list of M detections\n        objects: a list of N objects in the map\n    Returns:\n        A MxN tensor of visual similarities\n    '''\n    det_fts = detection_list.get_stacked_values_torch('clip_ft') # (M, D)",
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "aggregate_similarities",
        "kind": 2,
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "peekOfCode": "def aggregate_similarities(match_method: str, phys_bias: float, spatial_sim: torch.Tensor, visual_sim: torch.Tensor) -> torch.Tensor:\n    '''\n    Aggregate spatial and visual similarities into a single similarity score\n    Args:\n        spatial_sim: a MxN tensor of spatial similarities\n        visual_sim: a MxN tensor of visual similarities\n    Returns:\n        A MxN tensor of aggregated similarities\n    '''\n    if match_method == \"sim_sum\":",
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "match_detections_to_objects",
        "kind": 2,
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "peekOfCode": "def match_detections_to_objects(\n    agg_sim: torch.Tensor, detection_threshold: float = float('-inf')\n) -> List[Optional[int]]:\n    \"\"\"\n    Matches detections to objects based on similarity, returning match indices or None for unmatched.\n    Args:\n        agg_sim: Similarity matrix (detections vs. objects).\n        detection_threshold: Threshold for a valid match (default: -inf).\n    Returns:\n        List of matching object indices (or None if unmatched) for each detection.",
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "merge_obj_matches",
        "kind": 2,
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "peekOfCode": "def merge_obj_matches(\n    detection_list: DetectionList,\n    objects: MapObjectList,\n    match_indices: List[Optional[int]],\n    downsample_voxel_size: float,\n    dbscan_remove_noise: bool,\n    dbscan_eps: float,\n    dbscan_min_points: int,\n    spatial_sim_type: str,\n    device: str,",
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "merge_detections_to_objects",
        "kind": 2,
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "peekOfCode": "def merge_detections_to_objects(\n    downsample_voxel_size: float, dbscan_remove_noise: bool, dbscan_eps: float, dbscan_min_points: int,\n    spatial_sim_type: str, device: str, match_method: str, phys_bias: float,\n    detection_list: DetectionList, objects: MapObjectList, agg_sim: torch.Tensor\n) -> MapObjectList:\n    for detected_obj_idx in range(agg_sim.shape[0]):\n        if agg_sim[detected_obj_idx].max() == float('-inf'):\n            objects.append(detection_list[detected_obj_idx])\n        else:\n            existing_obj_match_idx = agg_sim[detected_obj_idx].argmax()",
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "owandb",
        "kind": 5,
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "peekOfCode": "owandb = OptionalWandB()\ntracker = MappingTracker()\ndef compute_spatial_similarities(spatial_sim_type: str, detection_list: DetectionList, objects: MapObjectList, downsample_voxel_size) -> torch.Tensor:\n    det_bboxes = detection_list.get_stacked_values_torch('bbox')\n    obj_bboxes = objects.get_stacked_values_torch('bbox')\n    if spatial_sim_type == \"iou\":\n        spatial_sim = compute_iou_batch(det_bboxes, obj_bboxes)\n    elif spatial_sim_type == \"giou\":\n        spatial_sim = compute_giou_batch(det_bboxes, obj_bboxes)\n    elif spatial_sim_type == \"iou_accurate\":",
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "tracker",
        "kind": 5,
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "peekOfCode": "tracker = MappingTracker()\ndef compute_spatial_similarities(spatial_sim_type: str, detection_list: DetectionList, objects: MapObjectList, downsample_voxel_size) -> torch.Tensor:\n    det_bboxes = detection_list.get_stacked_values_torch('bbox')\n    obj_bboxes = objects.get_stacked_values_torch('bbox')\n    if spatial_sim_type == \"iou\":\n        spatial_sim = compute_iou_batch(det_bboxes, obj_bboxes)\n    elif spatial_sim_type == \"giou\":\n        spatial_sim = compute_giou_batch(det_bboxes, obj_bboxes)\n    elif spatial_sim_type == \"iou_accurate\":\n        spatial_sim = compute_3d_iou_accurate_batch(det_bboxes, obj_bboxes)",
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.slam.r3d_stream_rerun_realtime_mapping",
        "description": "conceptgraph.slam.r3d_stream_rerun_realtime_mapping",
        "peekOfCode": "def main(cfg : DictConfig):\n    app = DemoApp()\n    app.connect_to_device(dev_idx=0)\n    tracker = MappingTracker()\n    orr = OptionalReRun()\n    orr.set_use_rerun(cfg.use_rerun)\n    orr.init(\"realtime_mapping\")\n    orr.spawn()\n    owandb = OptionalWandB()\n    owandb.set_use_wandb(cfg.use_wandb)",
        "detail": "conceptgraph.slam.r3d_stream_rerun_realtime_mapping",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.slam.realtime_mapping",
        "description": "conceptgraph.slam.realtime_mapping",
        "peekOfCode": "def main(cfg : DictConfig):\n    tracker = MappingTracker()\n    owandb = OptionalWandB()\n    owandb.set_use_wandb(cfg.use_wandb)\n    owandb.init(project=\"concept-graphs\", \n            #    entity=\"concept-graphs\",\n                config=cfg_to_dict(cfg),\n               )\n    cfg = process_cfg(cfg)\n    # Initialize the dataset",
        "detail": "conceptgraph.slam.realtime_mapping",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.slam.rerun_realtime_mapping",
        "description": "conceptgraph.slam.rerun_realtime_mapping",
        "peekOfCode": "def main(cfg : DictConfig):\n    tracker = MappingTracker()\n    orr = OptionalReRun()\n    orr.set_use_rerun(cfg.use_rerun)\n    orr.init(\"realtime_mapping\")\n    orr.spawn()\n    owandb = OptionalWandB()\n    owandb.set_use_wandb(cfg.use_wandb)\n    owandb.init(project=\"concept-graphs\", \n            #    entity=\"concept-graphs\",",
        "detail": "conceptgraph.slam.rerun_realtime_mapping",
        "documentation": {}
    },
    {
        "label": "DetectionList",
        "kind": 6,
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "peekOfCode": "class DetectionList(list):\n    def get_values(self, key, idx:int=None):\n        if idx is None:\n            return [detection[key] for detection in self]\n        else:\n            return [detection[key][idx] for detection in self]\n    def get_stacked_values_torch(self, key, idx:int=None):\n        values = []\n        for detection in self:\n            v = detection[key]",
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "kind": 6,
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "peekOfCode": "class MapObjectList(DetectionList):\n    def compute_similarities(self, new_clip_ft):\n        '''\n        The input feature should be of shape (D, ), a one-row vector\n        This is mostly for backward compatibility\n        '''\n        # if it is a numpy array, make it a tensor \n        new_clip_ft = to_tensor(new_clip_ft)\n        # assuming cosine similarity for features\n        clip_fts = self.get_stacked_values_torch('clip_ft')",
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapEdge",
        "kind": 6,
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "peekOfCode": "class MapEdge():\n    def __init__(self, obj1_idx, obj2_idx, rel_type, num_detections=1, first_detected=None):\n        self.obj1_idx = obj1_idx\n        self.obj2_idx = obj2_idx\n        self.rel_type = rel_type\n        self.num_detections = num_detections\n        self.first_detected = first_detected # frame index that the object was first detected \n    def to_serializable(self):\n        return {\n            'obj1_idx': self.obj1_idx,",
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapEdgeMapping",
        "kind": 6,
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "peekOfCode": "class MapEdgeMapping:\n    def __init__(self, objects):\n        self.objects = objects  # Reference to the list of existing objects\n        self.edges_by_index = {}  # {(obj1_index, obj2_index): MapEdge}\n        self.edges_by_uuid = {}  # {(obj1_uuid, obj2_uuid): MapEdge}\n    def add_or_update_edge(self, obj1_index, obj2_index, rel_type, first_detected=None):\n        obj1_uuid, obj2_uuid = self.objects[obj1_index]['id'], self.objects[obj2_index]['id']\n        uuid_key = (obj1_uuid, obj2_uuid)\n        if obj1_index == obj2_index:\n            print(f\"LOOOPY EDGE DETECTED: {obj1_index} == {obj2_index}\")",
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "kind": 2,
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "peekOfCode": "def to_numpy(tensor):\n    if isinstance(tensor, np.ndarray):\n        return tensor\n    return tensor.detach().cpu().numpy()\ndef to_tensor(numpy_array, device=None):\n    if isinstance(numpy_array, torch.Tensor):\n        return numpy_array\n    if device is None:\n        return torch.from_numpy(numpy_array)\n    else:",
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "to_tensor",
        "kind": 2,
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "peekOfCode": "def to_tensor(numpy_array, device=None):\n    if isinstance(numpy_array, torch.Tensor):\n        return numpy_array\n    if device is None:\n        return torch.from_numpy(numpy_array)\n    else:\n        return torch.from_numpy(numpy_array).to(device)\nclass DetectionList(list):\n    def get_values(self, key, idx:int=None):\n        if idx is None:",
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "conceptgraph.slam.streamlined_mapping",
        "description": "conceptgraph.slam.streamlined_mapping",
        "peekOfCode": "def main(cfg : DictConfig):\n    cfg = process_cfg(cfg)\n    # Initialize the dataset\n    dataset = get_dataset(\n        dataconfig=cfg.dataset_config,\n        start=cfg.start,\n        end=cfg.end,\n        stride=cfg.stride,\n        basedir=cfg.dataset_root,\n        sequence=cfg.scene_id,",
        "detail": "conceptgraph.slam.streamlined_mapping",
        "documentation": {}
    },
    {
        "label": "gobs_to_detection_list",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def gobs_to_detection_list(\n    cfg, \n    image, \n    depth_array,\n    cam_K, \n    idx, \n    gobs, \n    trans_pose = None,\n    class_names = None,\n    BG_CLASSES = [\"wall\", \"floor\", \"ceiling\"],",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "to_scalar",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def to_scalar(d: np.ndarray | torch.Tensor | float) -> int | float:\n    '''\n    Convert the d to a scalar\n    '''\n    if isinstance(d, float):\n        return d\n    elif \"numpy\" in str(type(d)):\n        assert d.size == 1\n        return d.item()\n    elif isinstance(d, torch.Tensor):",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "from_intrinsics_matrix",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def from_intrinsics_matrix(K: torch.Tensor) -> tuple[float, float, float, float]:\n    '''\n    Get fx, fy, cx, cy from the intrinsics matrix\n    return 4 scalars\n    '''\n    fx = to_scalar(K[0, 0])\n    fy = to_scalar(K[1, 1])\n    cx = to_scalar(K[0, 2])\n    cy = to_scalar(K[1, 2])\n    return fx, fy, cx, cy",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "get_classes_colors",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def get_classes_colors(classes):\n    class_colors = {}\n    # Generate a random color for each class\n    for class_idx, class_name in enumerate(classes):\n        # Generate random RGB values between 0 and 255\n        r = np.random.randint(0, 256)/255.0\n        g = np.random.randint(0, 256)/255.0\n        b = np.random.randint(0, 256)/255.0\n        # Assign the RGB values as a tuple to the class in the dictionary\n        class_colors[class_idx] = (r, g, b)",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "create_or_load_colors",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def create_or_load_colors(cfg, filename=\"gsa_classes_tag2text\"):\n    # get the classes, should be saved when making the dataset\n    classes_fp = cfg['dataset_root'] / cfg['scene_id'] / f\"{filename}.json\"\n    classes  = None\n    with open(classes_fp, \"r\") as f:\n        classes = json.load(f)\n    # create the class colors, or load them if they exist\n    class_colors  = None\n    class_colors_fp = cfg['dataset_root'] / cfg['scene_id'] / f\"{filename}_colors.json\"\n    if class_colors_fp.exists():",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "create_object_pcd",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def create_object_pcd(depth_array, mask, cam_K, image, obj_color=None) -> o3d.geometry.PointCloud:\n    fx, fy, cx, cy = from_intrinsics_matrix(cam_K)\n    # Also remove points with invalid depth values\n    mask = np.logical_and(mask, depth_array > 0)\n    # if no valid points, return an empty point cloud\n    if not np.any(mask):\n        pcd = o3d.geometry.PointCloud()\n        return pcd\n    height, width = depth_array.shape\n    x = np.arange(0, width, 1.0)",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "pcd_denoise_dbscan",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def pcd_denoise_dbscan(pcd: o3d.geometry.PointCloud, eps=0.02, min_points=10) -> o3d.geometry.PointCloud:\n    # Remove noise via clustering\n    pcd_clusters = pcd.cluster_dbscan(\n        eps=eps,\n        min_points=min_points,\n    )\n    # Convert to numpy arrays\n    obj_points = np.asarray(pcd.points)\n    obj_colors = np.asarray(pcd.colors)\n    pcd_clusters = np.array(pcd_clusters)",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "init_pcd_denoise_dbscan",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def init_pcd_denoise_dbscan(pcd: o3d.geometry.PointCloud, eps=0.02, min_points=10) -> o3d.geometry.PointCloud:\n    ## Remove noise via clustering\n    pcd_clusters = pcd.cluster_dbscan( # inint\n        eps=eps,\n        min_points=min_points,\n    )\n    # Convert to numpy arrays\n    obj_points = np.asarray(pcd.points)\n    obj_colors = np.asarray(pcd.colors)\n    pcd_clusters = np.array(pcd_clusters)",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "init_process_pcd",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def init_process_pcd(pcd, downsample_voxel_size, dbscan_remove_noise, dbscan_eps, dbscan_min_points, run_dbscan=True):\n    pcd = pcd.voxel_down_sample(voxel_size=downsample_voxel_size)\n    if dbscan_remove_noise and run_dbscan:\n        pcd = init_pcd_denoise_dbscan(\n            pcd, \n            eps=dbscan_eps, \n            min_points=dbscan_min_points\n        )\n    return pcd\n# @profile",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_pcd",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def process_pcd(pcd, downsample_voxel_size, dbscan_remove_noise, dbscan_eps, dbscan_min_points, run_dbscan=True):\n    pcd = pcd.voxel_down_sample(voxel_size=downsample_voxel_size)\n    if dbscan_remove_noise and run_dbscan:\n        pass\n        pcd = pcd_denoise_dbscan(\n            pcd, \n            eps=dbscan_eps, \n            min_points=dbscan_min_points\n        )\n    return pcd",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "get_bounding_box",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def get_bounding_box(spatial_sim_type, pcd):\n    if (\"accurate\" in spatial_sim_type or \"overlap\" in spatial_sim_type) and len(pcd.points) >= 4:\n        try:\n            return pcd.get_oriented_bounding_box(robust=True)\n        except RuntimeError as e:\n            print(f\"Met {e}, use axis aligned bounding box instead\")\n            return pcd.get_axis_aligned_bounding_box()\n    else:\n        return pcd.get_axis_aligned_bounding_box()\n# @profile",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_obj2_into_obj1",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def merge_obj2_into_obj1(obj1, obj2, downsample_voxel_size, dbscan_remove_noise, dbscan_eps, dbscan_min_points, spatial_sim_type, device, run_dbscan=True):\n    '''\n    Merges obj2 into obj1 with structured attribute handling, including explicit checks for unhandled keys.\n    Parameters:\n    - obj1, obj2: Objects to merge.\n    - downsample_voxel_size, dbscan_remove_noise, dbscan_eps, dbscan_min_points, spatial_sim_type: Parameters for point cloud processing.\n    - device: Computation device.\n    - run_dbscan: Whether to run DBSCAN for noise removal.\n    Returns:\n    - obj1: Updated object after merging.",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "compute_overlap_matrix",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def compute_overlap_matrix(objects: MapObjectList, downsample_voxel_size):\n    '''\n    compute pairwise overlapping between objects in terms of point nearest neighbor. \n    Suppose we have a list of n point cloud, each of which is a o3d.geometry.PointCloud object. \n    Now we want to construct a matrix of size n x n, where the (i, j) entry is the ratio of points in point cloud i \n    that are within a distance threshold of any point in point cloud j. \n    '''\n    n = len(objects)\n    overlap_matrix = np.zeros((n, n))\n    # Convert the point clouds into numpy arrays and then into FAISS indices for efficient search",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "compute_overlap_matrix_2set",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def compute_overlap_matrix_2set(objects_map: MapObjectList, objects_new: DetectionList, downsample_voxel_size) -> np.ndarray:\n    \"\"\"\n    Computes pairwise overlap between two sets of objects based on point proximity. \n    This function evaluates how much each new object overlaps with each existing object in the map by calculating the ratio of points in one object's point cloud that are within a specified distance threshold of points in the other object's point cloud.\n    Args:\n        objects_map (MapObjectList): The existing objects in the map, where each object includes a point cloud.\n        objects_new (DetectionList): The new objects to be added to the map, each with its own point cloud.\n        downsample_voxel_size (float): The distance threshold for considering points as overlapping. Points within this distance are counted as overlapping.\n    Returns:\n        np.ndarray: An overlap matrix of size m x n, where m is the number of existing objects and n is the number of new objects. Each entry (i, j) in the matrix represents the ratio of points in the i-th existing object's point cloud that are within the distance threshold of any point in the j-th new object's point cloud.",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "compute_overlap_matrix_general",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def compute_overlap_matrix_general(objects_a: MapObjectList, objects_b = None, downsample_voxel_size = None) -> np.ndarray:\n    \"\"\"\n    Compute the overlap matrix between two sets of objects represented by their point clouds. This function can also perform self-comparison when `objects_b` is not provided. The overlap is quantified based on the proximity of points from one object to the nearest points of another, within a threshold specified by `downsample_voxel_size`.\n    Parameters\n    ----------\n    objects_a : MapObjectList\n        A list of object representations where each object contains a point cloud ('pcd') and bounding box ('bbox').\n        This is the primary set of objects for comparison.\n    objects_b : Optional[MapObjectList]\n        A second list of object representations similar to `objects_a`. If None, `objects_a` will be compared with itself to calculate self-overlap. Defaults to None.",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_overlap_objects",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def merge_overlap_objects(\n    merge_overlap_thresh: float,\n    merge_visual_sim_thresh: float,\n    merge_text_sim_thresh: float,\n    objects: MapObjectList,\n    overlap_matrix: np.ndarray,\n    downsample_voxel_size: float,\n    dbscan_remove_noise: bool,\n    dbscan_eps: float,\n    dbscan_min_points: int,",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "denoise_objects",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def denoise_objects(\n    downsample_voxel_size: float,\n    dbscan_remove_noise: bool,\n    dbscan_eps: float,\n    dbscan_min_points: int,\n    spatial_sim_type: str,\n    device: str,\n    objects: MapObjectList,\n):\n    tracker = DenoisingTracker()  # Get the singleton instance of DenoisingTracker",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def filter_objects(\n    obj_min_points: int, \n    obj_min_detections: int, \n    objects: MapObjectList, \n    map_edges: MapEdgeMapping = None\n):\n    print(\"Before filtering:\", len(objects))\n    objects_to_keep = []\n    new_index_map = {}  # Maps old indices to new indices if edges are provided\n    # Identify which objects to keep",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def merge_objects(\n    merge_overlap_thresh: float,\n    merge_visual_sim_thresh: float,\n    merge_text_sim_thresh: float,\n    objects: MapObjectList,\n    downsample_voxel_size: float,\n    dbscan_remove_noise: bool,\n    dbscan_eps: float,\n    dbscan_min_points: int,\n    spatial_sim_type: str,",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_captions",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def filter_captions(captions, detection_class_labels):\n    # Create a dictionary to map id to the index in the captions list\n    captions_index = {item['id']: index for index, item in enumerate(captions)}\n    # Initialize a new list to store the cleaned and matched captions\n    new_captions = []\n    # Process each detection class label\n    for label in detection_class_labels:\n        # Split the label by spaces\n        parts = label.split()\n        # The last part is the id",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_gobs",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def filter_gobs(\n    cfg: DictConfig,\n    gobs: dict,\n    image: np.ndarray,\n    BG_CLASSES = [\"wall\", \"floor\", \"ceiling\"],\n):\n    # If no detection at all\n    if len(gobs['xyxy']) == 0:\n        return gobs\n    # Filter out the objects based on various criteria",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "resize_gobs",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def resize_gobs(gobs, image):\n    # If the shapes are the same, no resizing is necessary\n    if gobs['mask'].shape[1:] == image.shape[:2]:\n        return gobs\n    new_masks = []\n    for mask_idx in range(len(gobs['xyxy'])):\n        # TODO: rewrite using interpolation/resize in numpy or torch rather than cv2\n        mask = gobs['mask'][mask_idx]\n        # Rescale the xyxy coordinates to the image shape\n        x1, y1, x2, y2 = gobs['xyxy'][mask_idx]",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "transform_detection_list",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def transform_detection_list(\n    detection_list: DetectionList,\n    transform: torch.Tensor,\n    deepcopy = False,\n):\n    '''\n    Transform the detection list by the given transform\n    Args:\n        detection_list: DetectionList\n        transform: 4x4 torch.Tensor",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "make_detection_list_from_pcd_and_gobs",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def make_detection_list_from_pcd_and_gobs(\n    obj_pcds_and_bboxes, gobs, color_path, obj_classes, image_idx\n):\n    '''\n    This function makes a detection list for the objects\n    Ideally I don't want it to be needed, the detection list has too much info and is inefficient\n    '''\n    global tracker\n    detection_list = DetectionList()\n    # bg_detection_list = DetectionList()",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "dynamic_downsample",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def dynamic_downsample(points, colors=None, target=5000):\n    \"\"\"\n    Simplified and configurable downsampling function that dynamically adjusts the \n    downsampling rate based on the number of input points. If a target of -1 is provided, \n    downsampling is bypassed, returning the original points and colors.\n    Args:\n        points (torch.Tensor): Tensor of shape (N, 3) for N points.\n        target (int): Target number of points to aim for in the downsampled output, \n                      or -1 to bypass downsampling.\n        colors (torch.Tensor, optional): Corresponding colors tensor of shape (N, 3). ",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "batch_mask_depth_to_points_colors",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def batch_mask_depth_to_points_colors(\n    depth_tensor: torch.Tensor,\n    masks_tensor: torch.Tensor,\n    cam_K: torch.Tensor,\n    image_rgb_tensor: torch.Tensor = None,  # Parameter for RGB image tensor\n    device: str = 'cuda'\n) -> tuple:\n    \"\"\"\n    Converts a batch of masked depth images to 3D points and corresponding colors.\n    Args:",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "detections_to_obj_pcd_and_bbox",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def detections_to_obj_pcd_and_bbox(\n    depth_array, \n    masks, \n    cam_K, \n    image_rgb=None, \n    trans_pose=None, \n    min_points_threshold=5, \n    spatial_sim_type='axis_aligned', \n    obj_pcd_max_points = None,\n    downsample_voxel_size = None,",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "processing_needed",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def processing_needed(\n    process_interval, run_on_final_frame, frame_idx, is_final_frame=False\n):\n    if process_interval > 0 and (frame_idx+1) % process_interval == 0:\n        return True\n    if run_on_final_frame and is_final_frame:\n        return True\n    return False\ndef prepare_objects_save_vis(objects: MapObjectList, downsample_size: float=0.025):\n    objects_to_save = copy.deepcopy(objects)",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "prepare_objects_save_vis",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def prepare_objects_save_vis(objects: MapObjectList, downsample_size: float=0.025):\n    objects_to_save = copy.deepcopy(objects)\n    # Downsample the point cloud\n    for i in range(len(objects_to_save)):\n        objects_to_save[i]['pcd'] = objects_to_save[i]['pcd'].voxel_down_sample(downsample_size)\n    # Remove unnecessary keys\n    for i in range(len(objects_to_save)):\n        for k in list(objects_to_save[i].keys()):\n            if k not in [\n                'pcd', 'bbox', 'clip_ft', 'text_ft', 'class_id', 'num_detections', 'inst_color'",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_cfg",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def process_cfg(cfg: DictConfig):\n    cfg.dataset_root = Path(cfg.dataset_root)\n    cfg.dataset_config = Path(cfg.dataset_config)\n    if cfg.dataset_config.name != \"multiscan.yaml\":\n        # For datasets whose depth and RGB have the same resolution\n        # Set the desired image heights and width from the dataset config\n        dataset_cfg = omegaconf.OmegaConf.load(cfg.dataset_config)\n        if cfg.image_height is None:\n            cfg.image_height = dataset_cfg.camera_params.image_height\n        if cfg.image_width is None:",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "prepare_objects_save_vis",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def prepare_objects_save_vis(objects: MapObjectList, downsample_size: float=0.025):\n    objects_to_save = copy.deepcopy(objects)\n    # Downsample the point cloud\n    for i in range(len(objects_to_save)):\n        objects_to_save[i]['pcd'] = objects_to_save[i]['pcd'].voxel_down_sample(downsample_size)\n    # Remove unnecessary keys\n    for i in range(len(objects_to_save)):\n        for k in list(objects_to_save[i].keys()):\n            if k not in [\n                'pcd', 'bbox', 'clip_ft', 'text_ft', 'class_id', 'num_detections', 'inst_color'",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "process_edges",
        "kind": 2,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "def process_edges(match_indices, gobs, initial_objects_count, objects, map_edges, frame_idx):\n    # Step 1: Generate match_indices_w_new_obj with indices for new objects\n    # Initial count of objects before processing new detections\n    new_object_count = 0  # Counter for new objects\n    # Create a list of match indices with new objects index instead of None\n    match_indices_w_new_obj = []\n    for match_index in match_indices:\n        if match_index is None:\n            # Assign the future index for new objects and increment the counter\n            new_obj_index = initial_objects_count + new_object_count",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "tracker",
        "kind": 5,
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "peekOfCode": "tracker = MappingTracker()\ndef gobs_to_detection_list(\n    cfg, \n    image, \n    depth_array,\n    cam_K, \n    idx, \n    gobs, \n    trans_pose = None,\n    class_names = None,",
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "get_top_down_frame",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def get_top_down_frame(controller):\n    # Setup the top-down camera\n    event = controller.step(action=\"GetMapViewCameraProperties\", raise_for_failure=True)\n    pose = copy.deepcopy(event.metadata[\"actionReturn\"])\n    bounds = event.metadata[\"sceneBounds\"][\"size\"]\n    max_bound = max(bounds[\"x\"], bounds[\"z\"])\n    pose[\"position\"][\"y\"] += 1.1 * max_bound\n    pose[\"farClippingPlane\"] = 50\n    # pose[\"fieldOfView\"] = 50\n    # pose[\"orthographic\"] = False",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "adjust_ai2thor_pose",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def adjust_ai2thor_pose(pose):\n    '''\n    Adjust the camera pose from the one used in Unity to that in Open3D.\n    '''\n    # Transformation matrix to flip Y-axis\n    flip_y = np.array([\n        [1, 0, 0, 0],\n        [0, -1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "adjust_ai2thor_pose_batch",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def adjust_ai2thor_pose_batch(poses):\n    '''\n    Adjust the camera poses from the one used in Unity to that in Open3D.\n    '''\n    N = poses.shape[0]\n    # Transformation matrix to flip Y-axis\n    flip_y = np.array([\n        [1, 0, 0, 0],\n        [0, -1, 0, 0],\n        [0, 0, 1, 0],",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "adjust_ai2thor_batch_torch",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def adjust_ai2thor_batch_torch(poses):\n    '''\n    Adjust the camera poses from the one used in Unity to that in Open3D.\n    Args:\n        poses: torch.Tensor, shape (N, 4, 4)\n    Returns:\n        adjusted_pose: torch.Tensor, shape (N, 4, 4)\n    '''\n    N = poses.shape[0]\n    # Transformation matrix to flip Y-axis",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "depth2xyz",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def depth2xyz(depth: np.ndarray, K: np.ndarray) -> np.ndarray:\n    '''\n    Convert depth image to 3D XYZ image in the camera coordinate frame\n    Args:\n        depth: depth image, shape (H, W), in meters\n        K: camera intrinsics matrix, shape (3, 3)\n    Returns:\n        xyz_camera: 3D XYZ image in the camera coordinate frame, shape (H, W, 3)\n    '''\n    frame_size = depth.shape[:2]",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "transform_xyz",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def transform_xyz(xyz: np.ndarray, pose: np.ndarray) -> np.ndarray:\n    '''\n    Transform the 3D XYZ image using the input pose matrix\n    Args:\n        xyz: 3D XYZ image, shape (H, W, 3)\n        pose: 4x4 pose matrix, shape (4, 4)\n    Returns:\n        xyz_transformed: transformed 3D XYZ image, shape (H, W, 3)\n    '''\n    xyz_flatten = xyz.reshape(-1, 3)",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_scene",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def get_scene(scene_name):\n    # By default, use scene from AI2THOR\n    # If the scene name starts with train, val, or test, use the scene from ProcTHOR\n    scene = scene_name\n    if (\n        scene_name.startswith(\"train\")\n        or scene_name.startswith(\"val\")\n        or scene_name.startswith(\"test\")\n    ):\n        dataset = prior.load_dataset(\"procthor-10k\")",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_intrinsics",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def compute_intrinsics(vfov, height, width):\n    \"\"\"\n    Compute the camera intrinsics matrix K from the\n    vertical field of view (in degree), height, and width.\n    \"\"\"\n    # For Unity, the field view is the vertical field of view.\n    f = height / (2 * np.tan(np.deg2rad(vfov) / 2))\n    return np.array([[f, 0, width / 2], [0, f, height / 2], [0, 0, 1]])\ndef compute_pose(position: dict, rotation: dict) -> np.ndarray:\n    \"\"\"",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_pose",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def compute_pose(position: dict, rotation: dict) -> np.ndarray:\n    \"\"\"\n    Compute the camera extrinsics matrix from the position and rotation.\n    Note that in Unity, XYZ follows the left-hand rule, with Y pointing up.\n    See: https://docs.unity3d.com/560/Documentation/Manual/Transforms.html\n    In the camera coordinate, Z is the viewing direction, X is right, and Y is up. \n    See: https://library.vuforia.com/device-tracking/spatial-frame-reference\n    Euler angles are in degrees and in Rotation is done in the ZXY order.\n    See: https://docs.unity3d.com/ScriptReference/Transform-eulerAngles.html\n    \"\"\"",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_posrot",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def compute_posrot(T: np.ndarray) -> Tuple[dict, dict]:\n    \"\"\"\n    Decompose the camera extrinsics matrix into position and rotation.\n    This function reverses the operation performed in `compute_pose`, considering\n    Unity's conventions, left-hand coordinate system, and ZXY Euler angle rotation order.\n    \"\"\"\n    # Extract the rotation matrix from the first 3x3 elements\n    R = T[:3, :3]\n    # Extract the translation vector from the last column\n    t = T[:3, 3]",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_agent_pose_from_event",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def get_agent_pose_from_event(event) -> np.ndarray:\n    '''\n    Compute the 4x4 agent pose matrix from the event\n    '''\n    position = event.metadata[\"agent\"][\"position\"]\n    rotation = event.metadata[\"agent\"][\"rotation\"]\n    # Compute the agent pose (position and rotation of agent's body in global 3D space)\n    agent_pose = compute_pose(position, rotation)\n    return agent_pose\ndef get_camera_pose_from_event(event) -> np.ndarray:",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_camera_pose_from_event",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def get_camera_pose_from_event(event) -> np.ndarray:\n    '''\n    Compute the 4x4 camera pose matrix from the event\n    This is different from the agent pose!\n    '''\n    camera_position = event.metadata['cameraPosition']\n    camera_rotation = copy.deepcopy(event.metadata[\"agent\"][\"rotation\"])\n    camera_rotation['x'] = event.metadata['agent']['cameraHorizon']\n    camera_pose = compute_pose(camera_position, camera_rotation)\n    return camera_pose",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "sample_pose_random",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def sample_pose_random(controller: Controller, n_poses: int):\n    reachable_positions = controller.step(action=\"GetReachablePositions\").metadata[\n        \"actionReturn\"\n    ]\n    # Convert the positions to numpy array\n    reachable_np = np.array([[p[\"x\"], p[\"y\"], p[\"z\"]] for p in reachable_positions])\n    print(reachable_np)\n    # Generate a list of poses for taking pictures\n    sampled_poses = []\n    for i in trange(n_poses):",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "sample_pose_uniform",
        "kind": 2,
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "peekOfCode": "def sample_pose_uniform(controller: Controller, n_positions: int):\n    \"\"\"\n    Uniformly sample n_positions from the reachable positions\n    for each position, uniformly sample 8 rotations (0, 45, 90, 135, 180, 225, 270, 315)\n    \"\"\"\n    reachable_positions = controller.step(action=\"GetReachablePositions\").metadata[\n        \"actionReturn\"\n    ]\n    # Convert the positions to numpy array\n    reachable_np = np.array([[p[\"x\"], p[\"y\"], p[\"z\"]] for p in reachable_positions])",
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_pred_gt_associations",
        "kind": 2,
        "importPath": "conceptgraph.utils.eval",
        "description": "conceptgraph.utils.eval",
        "peekOfCode": "def compute_pred_gt_associations(pred, gt):\n    # pred: predicted pointcloud\n    # gt: GT pointcloud\n    from chamferdist.chamfer import knn_points\n    # pred = pointclouds.points_padded.cuda().contiguous()\n    # gt = pts_gt.unsqueeze(0).cuda().contiguous()\n    b, l, d = pred.shape\n    lengths_src = torch.ones(b, dtype=torch.long, device=pred.device) * l\n    b, l, d = gt.shape\n    lengths_tgt = torch.ones(b, dtype=torch.long, device=pred.device) * l",
        "detail": "conceptgraph.utils.eval",
        "documentation": {}
    },
    {
        "label": "compute_confmatrix",
        "kind": 2,
        "importPath": "conceptgraph.utils.eval",
        "description": "conceptgraph.utils.eval",
        "peekOfCode": "def compute_confmatrix(\n    labels_pred, labels_gt, idx_pred_to_gt, idx_gt_to_pred, class_names\n):\n    labels_gt = labels_gt[idx_pred_to_gt]\n    # num_classes = labels_gt.max().item() + 1\n    # print(num_classes)\n    num_classes = len(class_names)\n    print(num_classes)\n    confmatrix = torch.zeros(num_classes, num_classes, device=labels_pred.device)\n    for class_gt_int in range(num_classes):",
        "detail": "conceptgraph.utils.eval",
        "documentation": {}
    },
    {
        "label": "compute_metrics",
        "kind": 2,
        "importPath": "conceptgraph.utils.eval",
        "description": "conceptgraph.utils.eval",
        "peekOfCode": "def compute_metrics(confmatrix, class_names):\n    if isinstance(confmatrix, torch.Tensor):\n        confmatrix = confmatrix.cpu().numpy()\n    num_classes = len(class_names)\n    ious = np.zeros((num_classes))\n    precision = np.zeros((num_classes))\n    recall = np.zeros((num_classes))\n    f1score = np.zeros((num_classes))\n    for _idx in range(num_classes):\n        ious[_idx] = confmatrix[_idx, _idx] / (",
        "detail": "conceptgraph.utils.eval",
        "documentation": {}
    },
    {
        "label": "Timer",
        "kind": 6,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "class Timer:\n    def __init__(self, heading = \"\", verbose = True):\n        self.verbose = verbose\n        if not self.verbose:\n            return\n        self.heading = heading\n    def __enter__(self):\n        if not self.verbose:\n            return self\n        self.start = time.time()",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "ObjectClasses",
        "kind": 6,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "class ObjectClasses:\n    \"\"\"\n    Manages object classes and their associated colors, allowing for exclusion of background classes.\n    This class facilitates the creation or loading of a color map from a specified file containing\n    class names. It also manages background classes based on configuration, allowing for their\n    inclusion or exclusion. Background classes are [\"wall\", \"floor\", \"ceiling\"] by default.\n    Attributes:\n        classes_file_path (str): Path to the file containing class names, one per line.\n    Usage:\n        obj_classes = ObjectClasses(classes_file_path, skip_bg=True)",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "prjson",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def prjson(input_json, indent=0):\n    \"\"\" Pretty print a json object \"\"\"\n    if not isinstance(input_json, list):\n        input_json = [input_json]\n    print(\"[\")\n    for i, entry in enumerate(input_json):\n        print(\"  {\")\n        for j, (key, value) in enumerate(entry.items()):\n            terminator = \",\" if j < len(entry) - 1 else \"\"\n            if isinstance(value, str):",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "cfg_to_dict",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def cfg_to_dict(input_cfg):\n    \"\"\" Convert a Hydra configuration object to a native Python dictionary,\n    ensuring all special types (e.g., ListConfig, DictConfig, PosixPath) are\n    converted to serializable types for JSON. Checks for non-serializable objects. \"\"\"\n    def convert_to_serializable(obj):\n        \"\"\" Recursively convert non-serializable objects to serializable types. \"\"\"\n        if isinstance(obj, dict):\n            return {k: convert_to_serializable(v) for k, v in obj.items()}\n        elif isinstance(obj, list):\n            return [convert_to_serializable(v) for v in obj]",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_stream_data_out_path",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def get_stream_data_out_path(dataset_root, scene_id, make_dir=True):\n    stream_data_out_path = Path(dataset_root) / scene_id\n    stream_rgb_path = stream_data_out_path / \"rgb\"\n    stream_depth_path = stream_data_out_path / \"depth\"\n    stream_poses_path = stream_data_out_path / \"poses\"\n    if make_dir:\n        stream_rgb_path.mkdir(parents=True, exist_ok=True)\n        stream_depth_path.mkdir(parents=True, exist_ok=True)\n        stream_poses_path.mkdir(parents=True, exist_ok=True)\n    return stream_rgb_path, stream_depth_path, stream_poses_path",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_exp_out_path",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def get_exp_out_path(dataset_root, scene_id, exp_suffix, make_dir=True):\n    exp_out_path = Path(dataset_root) / scene_id / \"exps\" / f\"{exp_suffix}\"\n    if make_dir:\n        exp_out_path.mkdir(exist_ok=True, parents=True)\n    return exp_out_path\ndef get_vis_out_path(exp_out_path):\n    vis_folder_path = exp_out_path / \"vis\"\n    vis_folder_path.mkdir(exist_ok=True, parents=True)\n    return vis_folder_path\ndef get_det_out_path(exp_out_path, make_dir=True):",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_vis_out_path",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def get_vis_out_path(exp_out_path):\n    vis_folder_path = exp_out_path / \"vis\"\n    vis_folder_path.mkdir(exist_ok=True, parents=True)\n    return vis_folder_path\ndef get_det_out_path(exp_out_path, make_dir=True):\n    detections_folder_path = exp_out_path / \"detections\"\n    if make_dir:\n        detections_folder_path.mkdir(exist_ok=True, parents=True)\n    return detections_folder_path\ndef check_run_detections(force_detection, det_exp_path):",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_det_out_path",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def get_det_out_path(exp_out_path, make_dir=True):\n    detections_folder_path = exp_out_path / \"detections\"\n    if make_dir:\n        detections_folder_path.mkdir(exist_ok=True, parents=True)\n    return detections_folder_path\ndef check_run_detections(force_detection, det_exp_path):\n    # first check if det_exp_path directory exists\n    if force_detection:\n        return True\n    if not det_exp_path.exists():",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "check_run_detections",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def check_run_detections(force_detection, det_exp_path):\n    # first check if det_exp_path directory exists\n    if force_detection:\n        return True\n    if not det_exp_path.exists():\n        return True\n    return False\ndef mask_iou(mask1, mask2):\n    intersection = np.logical_and(mask1, mask2).sum()\n    union = np.logical_or(mask1, mask2).sum()",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "mask_iou",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def mask_iou(mask1, mask2):\n    intersection = np.logical_and(mask1, mask2).sum()\n    union = np.logical_or(mask1, mask2).sum()\n    if union == 0:\n        return 0\n    return intersection / union\ndef annotate_for_vlm(\n    image: np.ndarray, \n    detections: sv.Detections,\n    obj_classes, ",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "annotate_for_vlm",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def annotate_for_vlm(\n    image: np.ndarray, \n    detections: sv.Detections,\n    obj_classes, \n    labels: list[str], \n    save_path=None, \n    color: tuple=(0, 255, 0), \n    thickness: int=2, \n    text_color: tuple=(255, 255, 255), \n    text_scale: float=0.6, ",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "plot_edges_from_vlm",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def plot_edges_from_vlm(image: np.ndarray, edges, detections: sv.Detections, obj_classes, labels: list[str], sorted_indices: list[int], save_path=None) -> np.ndarray:\n    annotated_image = image.copy()\n    # Create a map from label to mask centroid and color for quick lookup\n    label_to_centroid_color = {}\n    for idx in sorted_indices:\n        mask = detections.mask[idx]\n        label_num = labels[idx].split(' ')[-1]  # Assuming label format is 'object X'\n        obj_color = obj_classes.get_class_color(int(detections.class_id[idx]))\n        obj_color = tuple([int(c * 255) for c in obj_color])  # Convert to BGR\n        # Determine the centroid of the mask",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "filter_detections",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def filter_detections(\n    image,\n    detections: sv.Detections, \n    classes, \n    top_x_detections = None, \n    confidence_threshold: float = 0.0,\n    given_labels = None,\n    iou_threshold: float = 0.80,  # IoU similarity threshold\n    proximity_threshold: float = 20.0,  # Default proximity threshold\n    keep_larger: bool = True,  # Keep the larger bounding box by area if True, else keep the smaller",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_vlm_annotated_image_path",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def get_vlm_annotated_image_path(det_exp_vis_path, color_path, w_edges=False, suffix=\"annotated_for_vlm.jpg\", ):\n    # Define suffixes based on whether edges are included\n    if w_edges:\n        suffix = suffix.replace(\".jpg\", \"_w_edges.jpg\")\n    # Create the file path\n    vis_save_path = (det_exp_vis_path / color_path.name).with_suffix(\".jpg\").with_name(\n        (det_exp_vis_path / color_path.name).stem + suffix\n    )\n    return str(vis_save_path)\ndef make_vlm_edges_and_captions(image, curr_det, obj_classes, detection_class_labels, det_exp_vis_path, color_path, make_edges_flag, openai_client):",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "make_vlm_edges_and_captions",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def make_vlm_edges_and_captions(image, curr_det, obj_classes, detection_class_labels, det_exp_vis_path, color_path, make_edges_flag, openai_client):\n    \"\"\"\n    Process detections by filtering, annotating, and extracting object relationships.\n    Args:\n        image (numpy.ndarray): The image on which detections are performed.\n        curr_det (list): Current detections from the detection model.\n        obj_classes (list): Object classes used in detection.\n        detection_class_labels (list): Labels for each detection class.\n        det_exp_vis_path (str): Directory path for saving visualizations.\n        color_path (str): Additional path element for creating unique save paths.",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "handle_rerun_saving",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def handle_rerun_saving(use_rerun, save_rerun, exp_suffix, exp_out_path):\n    # Save the rerun output if needed\n    if use_rerun and save_rerun:\n        rerun_file_path = exp_out_path / f\"rerun_{exp_suffix}.rrd\"\n        print(\"Mapping done!\")\n        print(\"If you want to save the rerun file, you should do so from the rerun viewer now.\")\n        print(\"You can't yet both save and log a file in rerun.\")\n        print(\"If you do, make a pull request!\")\n        print(\"Also, close the viewer before continuing, it frees up a lot of RAM, which helps for saving the pointclouds.\")\n        print(f\"Feel free to copy and use this path below, or choose your own:\\n{rerun_file_path}\")",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def measure_time(func):\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        # print(f\"Starting {func.__name__}...\")\n        result = func(*args, **kwargs)  # Call the function with any arguments it was called with\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        print(f\"Done! Execution time of {func.__name__} function: {elapsed_time:.2f} seconds\")\n        return result  # Return the result of the function call\n    return wrapper",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "get_exp_config_save_path",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def get_exp_config_save_path(exp_out_path, is_detection_config=False):\n    params_file_name = \"config_params\"\n    if is_detection_config:\n        params_file_name += \"_detections\"\n    return exp_out_path / f\"{params_file_name}.json\"\ndef save_hydra_config(hydra_cfg, exp_out_path, is_detection_config=False):\n    exp_out_path.mkdir(exist_ok=True, parents=True)\n    with open(get_exp_config_save_path(exp_out_path, is_detection_config), \"w\") as f:\n        dict_to_dump = cfg_to_dict(hydra_cfg)\n        json.dump(dict_to_dump, f, indent=2)",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_hydra_config",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def save_hydra_config(hydra_cfg, exp_out_path, is_detection_config=False):\n    exp_out_path.mkdir(exist_ok=True, parents=True)\n    with open(get_exp_config_save_path(exp_out_path, is_detection_config), \"w\") as f:\n        dict_to_dump = cfg_to_dict(hydra_cfg)\n        json.dump(dict_to_dump, f, indent=2)\ndef load_saved_hydra_json_config(exp_out_path):\n    with open(get_exp_config_save_path(exp_out_path), \"r\") as f:\n        return json.load(f)\ndef prepare_detection_paths(dataset_root, scene_id, detections_exp_suffix, force_detection, output_base_path):\n    \"\"\"",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "load_saved_hydra_json_config",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def load_saved_hydra_json_config(exp_out_path):\n    with open(get_exp_config_save_path(exp_out_path), \"r\") as f:\n        return json.load(f)\ndef prepare_detection_paths(dataset_root, scene_id, detections_exp_suffix, force_detection, output_base_path):\n    \"\"\"\n    Prepare and return paths needed for detection output, creating directories as needed.\n    \"\"\"\n    det_exp_path = get_exp_out_path(dataset_root, scene_id, detections_exp_suffix)\n    if force_detection:\n        det_vis_folder_path = get_vis_out_path(det_exp_path)",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "prepare_detection_paths",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def prepare_detection_paths(dataset_root, scene_id, detections_exp_suffix, force_detection, output_base_path):\n    \"\"\"\n    Prepare and return paths needed for detection output, creating directories as needed.\n    \"\"\"\n    det_exp_path = get_exp_out_path(dataset_root, scene_id, detections_exp_suffix)\n    if force_detection:\n        det_vis_folder_path = get_vis_out_path(det_exp_path)\n        det_detections_folder_path = get_det_out_path(det_exp_path)\n        os.makedirs(det_vis_folder_path, exist_ok=True)\n        os.makedirs(det_detections_folder_path, exist_ok=True)",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "should_exit_early",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def should_exit_early(file_path):\n    try:\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n        # Check if we should exit early\n        if data.get(\"exit_early\", False):\n            # Reset the exit_early flag to False\n            data[\"exit_early\"] = False\n            # Write the updated data back to the file\n            with open(file_path, 'w') as file:",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_detection_results",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def save_detection_results(base_path, results):\n    base_path.mkdir(exist_ok=True, parents=True)\n    for key, value in results.items():\n        save_path = Path(base_path) / f\"{key}\"\n        if isinstance(value, np.ndarray):\n            # Save NumPy arrays using .npz for efficient storage\n            np.savez_compressed(f\"{save_path}.npz\", value)\n        else:\n            # For other types, fall back to pickle\n            with gzip.open(f\"{save_path}.pkl.gz\", \"wb\") as f:",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "load_saved_detections",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def load_saved_detections(base_path):\n    base_path = Path(base_path)\n    # Construct potential .pkl.gz file path based on the base_path\n    potential_pkl_gz_path = Path(str(base_path) + '.pkl.gz')\n    # Check if the constructed .pkl.gz file exists\n    # This is the old wat \n    if potential_pkl_gz_path.exists() and potential_pkl_gz_path.is_file():\n        # The path points directly to a .pkl.gz file\n        with gzip.open(potential_pkl_gz_path, \"rb\") as f:\n            return pickle.load(f)",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_obj_json",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def save_obj_json(exp_suffix, exp_out_path, objects):\n    \"\"\"\n    Saves the objects to a JSON file with the specified suffix.\n    Args:\n    - exp_suffix (str): Suffix for the experiment, used in naming the saved file.\n    - exp_out_path (Path or str): Output path for the experiment's saved files.\n    - objects: The objects to save, assumed to have necessary attributes.\n    \"\"\"\n    json_obj_list = {}\n    for curr_idx, curr_obj in enumerate(objects):",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_edge_json",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def save_edge_json(exp_suffix, exp_out_path, objects, edges):\n    \"\"\"\n    Saves the edges to a JSON file with the specified suffix.\n    Args:\n    - exp_suffix (str): Suffix for the experiment, used in naming the saved file.\n    - exp_out_path (Path or str): Output path for the experiment's saved files.\n    - objects: The objects involved in the edges.\n    - edges: The edges to save, assumed to have necessary attributes.\n    \"\"\"\n    json_edge_list = {}",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_pointcloud",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def save_pointcloud(exp_suffix, exp_out_path, cfg, objects, obj_classes, latest_pcd_filepath=None, create_symlink=True, edges = None):\n    \"\"\"\n    Saves the point cloud data to a .pkl.gz file. Optionally, creates or updates a symlink to the latest saved file.\n    Args:\n    - exp_suffix (str): Suffix for the experiment, used in naming the saved file.\n    - exp_out_path (Path or str): Output path for the experiment's saved files.\n    - objects: The objects to save, assumed to have a `to_serializable()` method.\n    - obj_classes: The object classes, assumed to have `get_classes_arr()` and `get_class_color_dict_by_index()` methods.\n    - latest_pcd_filepath (Path or str, optional): Path for the symlink to the latest point cloud save. Default is None.\n    - create_symlink (bool): Whether to create/update a symlink to the latest save. Default is True.",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "find_existing_image_path",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def find_existing_image_path(base_path, extensions):\n    \"\"\"\n    Checks for the existence of a file with the given base path and any of the provided extensions.\n    Returns the path of the first existing file found or None if no file is found.\n    Parameters:\n    - base_path: The base file path without the extension.\n    - extensions: A list of file extensions to check for.\n    Returns:\n    - Path of the existing file or None if no file exists.\n    \"\"\"",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_objects_for_frame",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def save_objects_for_frame(obj_all_frames_out_path, frame_idx, objects, obj_min_detections, adjusted_pose, color_path):\n    save_path = obj_all_frames_out_path / f\"{frame_idx:06d}.pkl.gz\"\n    filtered_objects = [obj for obj in objects if obj['num_detections'] >= obj_min_detections]\n    prepared_objects = prepare_objects_save_vis(MapObjectList(filtered_objects))\n    result = {\n        \"camera_pose\": adjusted_pose, \n        \"objects\": prepared_objects,\n        \"frame_idx\": frame_idx,\n        \"num_objects\": len(filtered_objects),\n        \"color_path\": str(color_path)",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "add_info_to_image",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def add_info_to_image(image, frame_idx, num_objects, color_path):\n    frame_info_text = f\"Frame: {frame_idx}, Objects: {num_objects}, Path: {str(color_path)}\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = 0.5\n    color = (255, 0, 0)\n    thickness = 1\n    line_type = cv2.LINE_AA\n    position = (10, image.shape[0] - 10)\n    cv2.putText(image, frame_info_text, position, font, font_scale, color, thickness, line_type)\ndef save_video_from_frames(frames, exp_out_path, exp_suffix):",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_video_from_frames",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def save_video_from_frames(frames, exp_out_path, exp_suffix):\n    video_save_path = exp_out_path / (f\"s_mapping_{exp_suffix}.mp4\")\n    save_video_from_frames(frames, video_save_path, fps=10)\n    print(f\"Save video to {video_save_path}\")\ndef vis_render_image(objects, obj_classes, obj_renderer, image_original_pil, adjusted_pose, frames, frame_idx, color_path, obj_min_detections, class_agnostic, debug_render, is_final_frame, exp_out_path, exp_suffix):\n    filtered_objects = [\n        deepcopy(obj) for obj in objects \n        if obj['num_detections'] >= obj_min_detections and not obj['is_background']\n    ]\n    objects_vis = MapObjectList(filtered_objects)",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "vis_render_image",
        "kind": 2,
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "peekOfCode": "def vis_render_image(objects, obj_classes, obj_renderer, image_original_pil, adjusted_pose, frames, frame_idx, color_path, obj_min_detections, class_agnostic, debug_render, is_final_frame, exp_out_path, exp_suffix):\n    filtered_objects = [\n        deepcopy(obj) for obj in objects \n        if obj['num_detections'] >= obj_min_detections and not obj['is_background']\n    ]\n    objects_vis = MapObjectList(filtered_objects)\n    if class_agnostic:\n        objects_vis.color_by_instance()\n    else:\n        objects_vis.color_by_most_common_classes(obj_classes)",
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "transform_points_batch",
        "kind": 2,
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "peekOfCode": "def transform_points_batch(poses: torch.Tensor, points: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    poses: (M, 4, 4)\n    points: (N, 3)\n    return: (M, N, 3)\n    \"\"\"\n    N = points.shape[0]\n    M = poses.shape[0]\n    # Convert points to homogeneous coordinates (N, 4)\n    points_homogeneous = torch.cat((points, torch.ones(N, 1).to(points.device)), dim=-1) # (N, 4)",
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "project_points_camera2plane",
        "kind": 2,
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "peekOfCode": "def project_points_camera2plane(points: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Project a set of points (in camera coordinates) to the image plane according to the camera intrinsics\n    points: (N, 3)\n    K: (3, 3)\n    return: \n        points_proj: (N, 2) the projected points in the image plane\n        points_depth: (N,) the depth of the points in the camera coordinates\n    \"\"\"\n    # multiply points by the camera intrinsics",
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "project_points_world2plane",
        "kind": 2,
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "peekOfCode": "def project_points_world2plane(points: torch.Tensor, poses: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n    \"\"\"Project a set of points (in the world coordinates) to the image plane according to the camera pose and intrinsics\n    Args:\n        points (torch.Tensor): (N, 3)\n        poses (torch.Tensor): (M, 4, 4) the camera poses in the world coordinates\n        K (torch.Tensor): (3, 3)\n    Returns:\n        points_coord (torch.Tensor): (M, N, 2) the projected points in the image plane\n        points_depth (torch.Tensor): (M, N) the depth of the points in the camera coordinates\n    \"\"\"",
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "check_proj_points",
        "kind": 2,
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "peekOfCode": "def check_proj_points(\n    points: torch.Tensor, \n    depth_tensor: torch.Tensor,\n    K: torch.Tensor, \n    pose: torch.Tensor,\n    depth_margin: float = 0.05,\n) -> torch.Tensor:\n    '''\n    Project points to the image plane and perform visibility checks\n    Args:",
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "rotation_matrix_to_quaternion",
        "kind": 2,
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "peekOfCode": "def rotation_matrix_to_quaternion(R):\n    \"\"\"\n    Convert a rotation matrix to a quaternion.\n    Parameters:\n    - R: A 3x3 rotation matrix.\n    Returns:\n    - A quaternion in the format [x, y, z, w].\n    \"\"\"\n    # Make sure the matrix is a numpy array\n    R = np.asarray(R)",
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "quaternion_to_rotation_matrix",
        "kind": 2,
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "peekOfCode": "def quaternion_to_rotation_matrix(q):\n    \"\"\"\n    Convert a quaternion into a rotation matrix.\n    Parameters:\n    - q: A quaternion in the format [x, y, z, w].\n    Returns:\n    - A 3x3 rotation matrix.\n    \"\"\"\n    w, x, y, z = q[3], q[0], q[1], q[2]\n    return np.array([",
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "relative_transformation",
        "kind": 2,
        "importPath": "conceptgraph.utils.geometry",
        "description": "conceptgraph.utils.geometry",
        "peekOfCode": "def relative_transformation(\n    trans_01: torch.Tensor, trans_02: torch.Tensor, orthogonal_rotations: bool = False\n) -> torch.Tensor:\n    r\"\"\"Function that computes the relative homogenous transformation from a\n    reference transformation :math:`T_1^{0} = \\begin{bmatrix} R_1 & t_1 \\\\\n    \\mathbf{0} & 1 \\end{bmatrix}` to destination :math:`T_2^{0} =\n    \\begin{bmatrix} R_2 & t_2 \\\\ \\mathbf{0} & 1 \\end{bmatrix}`.\n    .. note:: Works with imperfect (non-orthogonal) rotation matrices as well.\n    The relative transformation is computed as follows:\n    .. math::",
        "detail": "conceptgraph.utils.geometry",
        "documentation": {}
    },
    {
        "label": "crop_image_pil",
        "kind": 2,
        "importPath": "conceptgraph.utils.image",
        "description": "conceptgraph.utils.image",
        "peekOfCode": "def crop_image_pil(image: Image, x1:int, y1:int, x2:int, y2:int, padding:int=0) -> Image:\n    '''\n    Crop the image with some padding\n    Args:\n        image: PIL image\n        x1, y1, x2, y2: bounding box coordinates\n        padding: padding around the bounding box\n    Returns:\n        image_crop: PIL image\n    '''",
        "detail": "conceptgraph.utils.image",
        "documentation": {}
    },
    {
        "label": "compute_3d_iou",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_3d_iou(bbox1, bbox2, padding=0, use_iou=True):\n    # Get the coordinates of the first bounding box\n    bbox1_min = np.asarray(bbox1.get_min_bound()) - padding\n    bbox1_max = np.asarray(bbox1.get_max_bound()) + padding\n    # Get the coordinates of the second bounding box\n    bbox2_min = np.asarray(bbox2.get_min_bound()) - padding\n    bbox2_max = np.asarray(bbox2.get_max_bound()) + padding\n    # Compute the overlap between the two bounding boxes\n    overlap_min = np.maximum(bbox1_min, bbox2_min)\n    overlap_max = np.minimum(bbox1_max, bbox2_max)",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_iou_batch",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_iou_batch(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute IoU between two sets of axis-aligned 3D bounding boxes.\n    bbox1: (M, V, D), e.g. (M, 8, 3)\n    bbox2: (N, V, D), e.g. (N, 8, 3)\n    returns: (M, N)\n    '''\n    # Compute min and max for each box\n    bbox1_min, _ = bbox1.min(dim=1) # Shape: (M, 3)\n    bbox1_max, _ = bbox1.max(dim=1) # Shape: (M, 3)",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_giou",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_3d_giou(bbox1, bbox2):\n    # Get the coordinates of the first bounding box\n    bbox1_min = np.asarray(bbox1.get_min_bound())\n    bbox1_max = np.asarray(bbox1.get_max_bound())\n    # Get the coordinates of the second bounding box\n    bbox2_min = np.asarray(bbox2.get_min_bound())\n    bbox2_max = np.asarray(bbox2.get_max_bound())\n    # Intersection\n    intersec_min = np.maximum(bbox1_min, bbox2_min)\n    intersec_max = np.minimum(bbox1_max, bbox2_max)",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_giou_batch",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_giou_batch(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute the generalized IoU between two sets of axis-aligned 3D bounding boxes.\n    bbox1: (M, V, D), e.g. (M, 8, 3)\n    bbox2: (N, V, D), e.g. (N, 8, 3)\n    returns: (M, N)\n    '''\n    # Compute min and max for each box\n    bbox1_min, _ = bbox1.min(dim=1) # Shape: (M, D)\n    bbox1_max, _ = bbox1.max(dim=1) # Shape: (M, D)",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_iou_accurate_batch",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_3d_iou_accurate_batch(bbox1, bbox2):\n    '''\n    Compute IoU between two sets of oriented (or axis-aligned) 3D bounding boxes.\n    bbox1: (M, 8, D), e.g. (M, 8, 3)\n    bbox2: (N, 8, D), e.g. (N, 8, 3)\n    returns: (M, N)\n    '''\n    # Must expend the box beforehand, otherwise it may results overestimated results\n    bbox1 = expand_3d_box(bbox1, 0.02)\n    bbox2 = expand_3d_box(bbox2, 0.02)",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_giou_accurate",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_3d_giou_accurate(obj1, obj2):\n    '''\n    Compute the 3D GIoU in a more accurate way. \n    '''\n    import pytorch3d.ops as ops\n    # This is too slow\n    # bbox1 = pcd1.get_minimal_oriented_bounding_box()\n    # bbox2 = pcd2.get_minimal_oriented_bounding_box()\n    # This is still slow ... \n    # Moved it outside of this function so that it is computed less times",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_box_volume_batch",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_3d_box_volume_batch(bbox: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute the volume of a set of rectangular boxes.\n    This assumes bbox corner order follows the open3d convention, which is:\n    ---, +--, -+-, --+, +++, -++, +-+, ++-\n    See https://github.com/isl-org/Open3D/blob/47f4ee936841ae9f8a9c4ce5d9162bd5b3e0279f/cpp/open3d/geometry/BoundingVolume.cpp#L92\n    bbox: (N, 8, D)\n    returns: (N,)\n    '''\n    a = torch.linalg.vector_norm(bbox[:, 0, :] - bbox[:, 1, :], ord=2, dim=1)",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "expand_3d_box",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def expand_3d_box(bbox: torch.Tensor, eps=0.02) -> torch.Tensor:\n    '''\n    Expand the side of 3D boxes such that each side has at least eps length.\n    Assumes the bbox cornder order in open3d convention. \n    bbox: (N, 8, D)\n    returns: (N, 8, D)\n    '''\n    center = bbox.mean(dim=1)  # shape: (N, D)\n    va = bbox[:, 1, :] - bbox[:, 0, :]  # shape: (N, D)\n    vb = bbox[:, 2, :] - bbox[:, 0, :]  # shape: (N, D)",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_enclosing_vol",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_enclosing_vol(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute the enclosing volume between every pair of boxes in bbox1 and bbox2.\n    This is an accurate but slow version using convex hull\n    bbox1: (M, 8, D)\n    bbox2: (N, 8, D)\n    returns: (M, N)\n    '''\n    M = bbox1.shape[0]\n    N = bbox2.shape[0]",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_enclosing_vol_fast",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_enclosing_vol_fast(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute the enclosing volume between every pair of boxes in bbox1 and bbox2.\n    This is fast but approximate version using axis-aligned bounding box\n    bbox1: (M, 8, 3)\n    bbox2: (N, 8, 3)\n    returns: (M, N)\n    '''\n    M = bbox1.shape[0]\n    N = bbox2.shape[0]",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_giou_accurate_batch",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_3d_giou_accurate_batch(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute Generalized IoU between two sets of oriented (or axis-aligned) 3D bounding boxes.\n    bbox1: (M, 8, D), e.g. (M, 8, 3)\n    bbox2: (N, 8, D), e.g. (N, 8, 3)\n    returns: (M, N)\n    '''\n    # Must expend the box beforehand, otherwise it may results overestimated results\n    bbox1 = expand_3d_box(bbox1, 0.02)\n    bbox2 = expand_3d_box(bbox2, 0.02)",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_contain_ratio_accurate_batch",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_3d_contain_ratio_accurate_batch(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute for i-th box in bbox1, how much of it is contained in j-th box in bbox2.\n    bbox1: (M, 8, D), e.g. (M, 8, 3)\n    bbox2: (N, 8, D), e.g. (N, 8, 3)\n    returns: (M, N)\n    '''\n    # Must expend the box beforehand, otherwise it may results overestimated results\n    bbox1 = expand_3d_box(bbox1)\n    bbox2 = expand_3d_box(bbox2)",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_2d_box_contained_batch",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def compute_2d_box_contained_batch(bbox: torch.Tensor, thresh:float=0.95) -> torch.Tensor:\n    '''\n    For each bbox, compute how many other bboxes are containing it. \n    First compute the area of the intersection between each pair of bboxes. \n    Then for each bbox, count how many bboxes have the intersection area larger than thresh of its own area.\n    bbox: (N, 4), in (x1, y1, x2, y2) format\n    returns: (N,)\n    '''\n    N = bbox.shape[0]\n    # Get areas of each bbox",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "mask_subtract_contained",
        "kind": 2,
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "peekOfCode": "def mask_subtract_contained(xyxy: np.ndarray, mask: np.ndarray, th1=0.8, th2=0.7):\n    '''\n    Compute the containing relationship between all pair of bounding boxes.\n    For each mask, subtract the mask of bounding boxes that are contained by it.\n    Args:\n        xyxy: (N, 4), in (x1, y1, x2, y2) format\n        mask: (N, H, W), binary mask\n        th1: float, threshold for computing intersection over box1\n        th2: float, threshold for computing intersection over box2\n    Returns:",
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "MappingTracker",
        "kind": 6,
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "peekOfCode": "class MappingTracker:\n    _instance = None\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(MappingTracker, cls).__new__(cls)\n            # Initialize the instance \"once\"\n            cls._instance.__initialized = False\n        return cls._instance\n    def __init__(self):\n        if not self.__initialized:",
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "DenoisingTracker",
        "kind": 6,
        "importPath": "conceptgraph.utils.logging_metrics",
        "description": "conceptgraph.utils.logging_metrics",
        "peekOfCode": "class DenoisingTracker:\n    _instance = None\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(DenoisingTracker, cls).__new__(cls)\n            # Initialize the instance \"once\"\n            cls._instance.__initialized = False\n        return cls._instance\n    def __init__(self):\n        if not self.__initialized:",
        "detail": "conceptgraph.utils.logging_metrics",
        "documentation": {}
    },
    {
        "label": "get_sam_segmentation_from_xyxy_batched",
        "kind": 2,
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "peekOfCode": "def get_sam_segmentation_from_xyxy_batched(sam_predictor, image: np.ndarray, xyxy_tensor: torch.Tensor) -> torch.Tensor:\n    sam_predictor.set_image(image)\n    transformed_boxes = sam_predictor.transform.apply_boxes_torch(xyxy_tensor, image.shape[:2])\n    masks, _, _ = sam_predictor.predict_torch(\n        point_coords=None,\n        point_labels=None,\n        boxes=transformed_boxes,\n        multimask_output=False,\n    )\n    return masks.squeeze()",
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "get_sam_segmentation_from_xyxy",
        "kind": 2,
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "peekOfCode": "def get_sam_segmentation_from_xyxy(sam_predictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n    sam_predictor.set_image(image)\n    result_masks = []\n    for box in xyxy:\n        masks, scores, logits = sam_predictor.predict(\n            box=box,\n            multimask_output=True\n        )\n        index = np.argmax(scores)\n        result_masks.append(masks[index])",
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_clip_features",
        "kind": 2,
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "peekOfCode": "def compute_clip_features(image, detections, clip_model, clip_preprocess, clip_tokenizer, classes, device):\n    backup_image = image.copy()\n    image = Image.fromarray(image)\n    # padding = args.clip_padding  # Adjust the padding amount as needed\n    padding = 20  # Adjust the padding amount as needed\n    image_crops = []\n    image_feats = []\n    text_feats = []\n    for idx in range(len(detections.xyxy)):\n        # Get the crop of the mask with padding",
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_clip_features_batched",
        "kind": 2,
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "peekOfCode": "def compute_clip_features_batched(image, detections, clip_model, clip_preprocess, clip_tokenizer, classes, device):\n    image = Image.fromarray(image)\n    padding = 20  # Adjust the padding amount as needed\n    image_crops = []\n    preprocessed_images = []\n    text_tokens = []\n    # Prepare data for batch processing\n    for idx in range(len(detections.xyxy)):\n        x_min, y_min, x_max, y_max = detections.xyxy[idx]\n        image_width, image_height = image.size",
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_ft_vector_closeness_statistics",
        "kind": 2,
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "peekOfCode": "def compute_ft_vector_closeness_statistics(unbatched, batched):\n    # Initialize lists to store statistics\n    mad = []  # Mean Absolute Difference\n    max_diff = []  # Maximum Absolute Difference\n    mrd = []  # Mean Relative Difference\n    cosine_sim = []  # Cosine Similarity\n    for i in range(len(unbatched)):\n        diff = np.abs(unbatched[i] - batched[i])\n        mad.append(np.mean(diff))\n        max_diff.append(np.max(diff))",
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "OptionalReRun",
        "kind": 6,
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "peekOfCode": "class OptionalReRun:\n    _instance = None\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._config_use_rerun = None\n            cls._instance._rerun = None\n        return cls._instance\n    def set_use_rerun(self, config_use_rerun):\n        self._config_use_rerun = config_use_rerun",
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_camera",
        "kind": 2,
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "peekOfCode": "def orr_log_camera(intrinsics, adjusted_pose, prev_adjusted_pose, img_width, img_height, frame_idx):\n    # Extract intrinsic camera parameters\n    focal_length = [intrinsics[0, 0].item(), intrinsics[1, 1].item()]\n    principal_point = [intrinsics[0, 2].item(), intrinsics[1, 2].item()]\n    resolution = [img_width, img_height]  # Width x Height from the RGB image\n    # Log camera intrinsics and resolution\n    orr.log(\n        \"world/camera\",\n        orr.Pinhole(\n            resolution=resolution,",
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_rgb_image",
        "kind": 2,
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "peekOfCode": "def orr_log_rgb_image(color_path):\n    # Log RGB image from the specified path\n    color_path = color_path\n    orr.log(\n        \"world/camera/rgb_image_encoded\",\n        orr.ImageEncoded(path=str(color_path))\n    )\ndef orr_log_depth_image(depth_tensor):\n    depth_in_meters = depth_tensor.numpy() \n    # Ensure depth data is in the expected format for rerun (HxW)",
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_depth_image",
        "kind": 2,
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "peekOfCode": "def orr_log_depth_image(depth_tensor):\n    depth_in_meters = depth_tensor.numpy() \n    # Ensure depth data is in the expected format for rerun (HxW)\n    # depth_in_meters should be a 2D numpy array at this point\n    assert len(depth_in_meters.shape) == 2, \"Depth data must be a 2D array\"\n    # This should really use meter = 1.0, but setting it to that makes it too big\n    # I wanna confirm its not me before making an issue on their github\n    orr.log(\n        \"world/camera/depth\",\n        orr.DepthImage(depth_in_meters , meter=0.9999999)",
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_annotated_image",
        "kind": 2,
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "peekOfCode": "def orr_log_annotated_image(color_path, det_exp_vis_path):\n    # Check if the visualizations exist and log them\n    color_path = color_path\n    det_exp_vis_path = det_exp_vis_path\n    base_vis_save_path = det_exp_vis_path / color_path.stem\n    existing_vis_save_path = find_existing_image_path(base_vis_save_path, ['.jpg', '.png'])\n    if existing_vis_save_path:\n        orr.log(\n            \"world/camera/rgb_image_annotated\",\n            orr.ImageEncoded(path=existing_vis_save_path)",
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_vlm_image",
        "kind": 2,
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "peekOfCode": "def orr_log_vlm_image(vlm_image_path, label=\"\"):\n    if os.path.exists(vlm_image_path):\n        orr.log(\n            f\"world/camera/vlm_image_{label}\",\n            orr.ImageEncoded(path=vlm_image_path)\n        )\n    else:\n        logging.warning(f\"VLM image not found at path: {vlm_image_path}\")\ndef orr_log_objs_pcd_and_bbox(objects, obj_classes):\n    global prev_logged_entities",
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_objs_pcd_and_bbox",
        "kind": 2,
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "peekOfCode": "def orr_log_objs_pcd_and_bbox(objects, obj_classes):\n    global prev_logged_entities\n    global counter\n    new_logged_entities = set()\n    for obj_idx, obj in enumerate(objects):\n        if obj['num_detections'] < 1:\n            continue\n        if obj['is_background']:\n            continue\n        obj_label = f\"{obj['curr_obj_num']}_{obj['class_name']}\"",
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr_log_edges",
        "kind": 2,
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "peekOfCode": "def orr_log_edges(objects, map_edges, obj_classes):\n    # first clear all edges \n    orr.log(\n        \"world/edges\", \n        orr.Clear(recursive=True)\n    )\n    # do the same for edges\n    for map_edge_tuple in map_edges.edges_by_index.items():\n        obj1_idx, obj2_idx = map_edge_tuple[0]\n        map_edge = map_edge_tuple[1]",
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "orr",
        "kind": 5,
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "peekOfCode": "orr = OptionalReRun()\nprev_logged_entities = set()\ncounter = 0\ndef orr_log_camera(intrinsics, adjusted_pose, prev_adjusted_pose, img_width, img_height, frame_idx):\n    # Extract intrinsic camera parameters\n    focal_length = [intrinsics[0, 0].item(), intrinsics[1, 1].item()]\n    principal_point = [intrinsics[0, 2].item(), intrinsics[1, 2].item()]\n    resolution = [img_width, img_height]  # Width x Height from the RGB image\n    # Log camera intrinsics and resolution\n    orr.log(",
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "prev_logged_entities",
        "kind": 5,
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "peekOfCode": "prev_logged_entities = set()\ncounter = 0\ndef orr_log_camera(intrinsics, adjusted_pose, prev_adjusted_pose, img_width, img_height, frame_idx):\n    # Extract intrinsic camera parameters\n    focal_length = [intrinsics[0, 0].item(), intrinsics[1, 1].item()]\n    principal_point = [intrinsics[0, 2].item(), intrinsics[1, 2].item()]\n    resolution = [img_width, img_height]  # Width x Height from the RGB image\n    # Log camera intrinsics and resolution\n    orr.log(\n        \"world/camera\",",
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "counter",
        "kind": 5,
        "importPath": "conceptgraph.utils.optional_rerun_wrapper",
        "description": "conceptgraph.utils.optional_rerun_wrapper",
        "peekOfCode": "counter = 0\ndef orr_log_camera(intrinsics, adjusted_pose, prev_adjusted_pose, img_width, img_height, frame_idx):\n    # Extract intrinsic camera parameters\n    focal_length = [intrinsics[0, 0].item(), intrinsics[1, 1].item()]\n    principal_point = [intrinsics[0, 2].item(), intrinsics[1, 2].item()]\n    resolution = [img_width, img_height]  # Width x Height from the RGB image\n    # Log camera intrinsics and resolution\n    orr.log(\n        \"world/camera\",\n        orr.Pinhole(",
        "detail": "conceptgraph.utils.optional_rerun_wrapper",
        "documentation": {}
    },
    {
        "label": "OptionalWandB",
        "kind": 6,
        "importPath": "conceptgraph.utils.optional_wandb_wrapper",
        "description": "conceptgraph.utils.optional_wandb_wrapper",
        "peekOfCode": "class OptionalWandB:\n    '''\n    A class for optionally integrating Weights & Biases (wandb) into your projects. It's designed to \n    allow projects to function with or without wandb installed, making the logging functionality \n    flexible and optional. This is particularly useful in environments where wandb is not available \n    or in scenarios where you want to run your code without wandb logging. The class follows the \n    Singleton design pattern to ensure a single, consistent state is maintained throughout the \n    application's lifetime.\n    How It Works:\n    - On first use, it attempts to configure itself based on the provided settings, specifically ",
        "detail": "conceptgraph.utils.optional_wandb_wrapper",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "kind": 6,
        "importPath": "conceptgraph.utils.pointclouds",
        "description": "conceptgraph.utils.pointclouds",
        "peekOfCode": "class Pointclouds(object):\n    r\"\"\"Batch of pointclouds (with varying numbers of points), enabling conversion between 2 representations:\n    - List: Store points of each pointcloud of shape :math:`(N_b, 3)` in a list of length :math:`B`.\n    - Padded: Store all points in a :math:`(B, max(N_b), 3)` tensor with zero padding as required.\n    Args:\n        points (torch.Tensor or list of torch.Tensor or None): :math:`(X, Y, Z)` coordinates of each point.\n            Default: None\n        normals (torch.Tensor or list of torch.Tensor or None): Normals :math:`(N_x, N_y, N_z)` of each point.\n            Default: None\n        colors (torch.Tensor or list of torch.Tensor or None): :math:`(R, G, B)` color of each point.",
        "detail": "conceptgraph.utils.pointclouds",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "conceptgraph.utils.pointclouds",
        "description": "conceptgraph.utils.pointclouds",
        "peekOfCode": "__all__ = [\"Pointclouds\"]\nclass Pointclouds(object):\n    r\"\"\"Batch of pointclouds (with varying numbers of points), enabling conversion between 2 representations:\n    - List: Store points of each pointcloud of shape :math:`(N_b, 3)` in a list of length :math:`B`.\n    - Padded: Store all points in a :math:`(B, max(N_b), 3)` tensor with zero padding as required.\n    Args:\n        points (torch.Tensor or list of torch.Tensor or None): :math:`(X, Y, Z)` coordinates of each point.\n            Default: None\n        normals (torch.Tensor or list of torch.Tensor or None): Normals :math:`(N_x, N_y, N_z)` of each point.\n            Default: None",
        "detail": "conceptgraph.utils.pointclouds",
        "documentation": {}
    },
    {
        "label": "homogenize_points",
        "kind": 2,
        "importPath": "conceptgraph.utils.projutils",
        "description": "conceptgraph.utils.projutils",
        "peekOfCode": "def homogenize_points(pts: torch.Tensor):\n    r\"\"\"Convert a set of points to homogeneous coordinates.\n    Args:\n        pts (torch.Tensor): Tensor containing points to be homogenized.\n    Returns:\n        torch.Tensor: Homogeneous coordinates of pts.\n    Shape:\n        - pts: :math:`(N, *, K)` where :math:`N` indicates the number of points in a cloud if\n          the shape is :math:`(N, K)` and indicates batchsize if the number of dimensions is greater than 2.\n          Also, :math:`*` means any number of additional dimensions, and `K` is the dimensionality of each point.",
        "detail": "conceptgraph.utils.projutils",
        "documentation": {}
    },
    {
        "label": "unhomogenize_points",
        "kind": 2,
        "importPath": "conceptgraph.utils.projutils",
        "description": "conceptgraph.utils.projutils",
        "peekOfCode": "def unhomogenize_points(pts: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n    r\"\"\"Convert a set of points from homogeneous coordinates to Euclidean\n    coordinates. This is usually done by taking each point :math:`(X, Y, Z, W)` and dividing it by\n    the last coordinate :math:`(w)`.\n    Args:\n        pts (torch.Tensor): Tensor containing points to be unhomogenized.\n    Returns:\n        torch.Tensor: 'Unhomogenized' points\n    Shape:\n        - pts: :math:`(N, *, K)` where :math:`N` indicates the number of points in a cloud if",
        "detail": "conceptgraph.utils.projutils",
        "documentation": {}
    },
    {
        "label": "project_points",
        "kind": 2,
        "importPath": "conceptgraph.utils.projutils",
        "description": "conceptgraph.utils.projutils",
        "peekOfCode": "def project_points(\n    cam_coords: torch.Tensor, proj_mat: torch.Tensor, eps: Optional[float] = 1e-6\n) -> torch.Tensor:\n    r\"\"\"Projects points from the camera coordinate frame to the image (pixel) frame.\n    Args:\n        cam_coords (torch.Tensor): pixel coordinates (defined in the\n            frame of the first camera).\n        proj_mat (torch.Tensor): projection matrix between the reference\n            and the non-reference camera frame.\n    Returns:",
        "detail": "conceptgraph.utils.projutils",
        "documentation": {}
    },
    {
        "label": "unproject_points",
        "kind": 2,
        "importPath": "conceptgraph.utils.projutils",
        "description": "conceptgraph.utils.projutils",
        "peekOfCode": "def unproject_points(\n    pixel_coords: torch.Tensor, intrinsics_inv: torch.Tensor, depths: torch.Tensor\n) -> torch.Tensor:\n    r\"\"\"Unprojects points from the image (pixel) frame to the camera coordinate frame.\n    Args:\n        pixel_coords (torch.Tensor): pixel coordinates.\n        intrinsics_inv (torch.Tensor): inverse of the camera intrinsics matrix.\n        depths (torch.Tensor): per-pixel depth estimates.\n    Returns:\n        torch.Tensor: camera coordinates",
        "detail": "conceptgraph.utils.projutils",
        "documentation": {}
    },
    {
        "label": "inverse_intrinsics",
        "kind": 2,
        "importPath": "conceptgraph.utils.projutils",
        "description": "conceptgraph.utils.projutils",
        "peekOfCode": "def inverse_intrinsics(K: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n    r\"\"\"Efficient inversion of intrinsics matrix\n    Args:\n        K (torch.Tensor): Intrinsics matrix\n        eps (float): Epsilon for numerical stability\n    Returns:\n        torch.Tensor: Inverse of intrinsics matrices\n    Shape:\n        - K: :math:`(*, 4, 4)` or :math:`(*, 3, 3)`\n        - Kinv: Matches shape of `K` (:math:`(*, 4, 4)` or :math:`(*, 3, 3)`)",
        "detail": "conceptgraph.utils.projutils",
        "documentation": {}
    },
    {
        "label": "DemoApp",
        "kind": 6,
        "importPath": "conceptgraph.utils.record3d_utils",
        "description": "conceptgraph.utils.record3d_utils",
        "peekOfCode": "class DemoApp:\n    def __init__(self):\n        self.event = Event()\n        self.session = None\n        self.DEVICE_TYPE__TRUEDEPTH = 0\n        self.DEVICE_TYPE__LIDAR = 1\n    def on_new_frame(self):\n        \"\"\"\n        This method is called from non-main thread, therefore cannot be used for presenting UI.\n        \"\"\"",
        "detail": "conceptgraph.utils.record3d_utils",
        "documentation": {}
    },
    {
        "label": "list_to_padded",
        "kind": 2,
        "importPath": "conceptgraph.utils.structutils",
        "description": "conceptgraph.utils.structutils",
        "peekOfCode": "def list_to_padded(\n    x: List[torch.Tensor],\n    pad_size: Union[list, tuple, None] = None,\n    pad_value: float = 0.0,\n    equisized: bool = False,\n) -> torch.Tensor:\n    r\"\"\"Transforms a list of B tensors each of shape :math:`(N_b, C_b)` into a single tensor of shape\n    :math:`(N, pad_size(0), pad_size(1))`, or :math:`(N, max(N_b), max(C_b))` if pad_size is None.\n    Args:\n        x: list of Tensors",
        "detail": "conceptgraph.utils.structutils",
        "documentation": {}
    },
    {
        "label": "padded_to_list",
        "kind": 2,
        "importPath": "conceptgraph.utils.structutils",
        "description": "conceptgraph.utils.structutils",
        "peekOfCode": "def padded_to_list(x: torch.Tensor, split_size: Union[list, tuple, None] = None):\n    r\"\"\"Transforms a padded tensor of shape :math:`(B, N, C)` into a list of :math:`B` tensors of shape\n    :math:`(N_b, C_b)` where :math:`(N_b, C_b)` is specified in split_size(b), or of shape :math:`(N, C)` if\n    split_size is None. split_size support only for 3-dimensional input tensor.\n    Args:\n        x: tensor consisting of padded input tensors\n        split_size: the shape of the final tensor to be returned (of length N).\n    Returns:\n        x_list: list of Tensors\n    Shape:",
        "detail": "conceptgraph.utils.structutils",
        "documentation": {}
    },
    {
        "label": "numpy_to_plotly_image",
        "kind": 2,
        "importPath": "conceptgraph.utils.structutils",
        "description": "conceptgraph.utils.structutils",
        "peekOfCode": "def numpy_to_plotly_image(img, name=None, is_depth=False, scale=None, quality=95):\n    r\"\"\"Converts a numpy array img to a `plotly.graph_objects.Image` object.\n    Args\n        img (np.ndarray): RGB image array\n        name (str): Name for the returned `plotly.graph_objects.Image` object\n        is_depth (bool): Bool indicating whether input `img` is depth image. Default: False\n        scale (int or None): Scale factor to display on hover. If None, will not display `scale: ...`. Default: None\n        quality (int): Image quality from 0 to 100 (the higher is the better). Default: 95\n    Returns:\n        `plotly.graph_objects.Image`",
        "detail": "conceptgraph.utils.structutils",
        "documentation": {}
    },
    {
        "label": "img_to_b64str",
        "kind": 2,
        "importPath": "conceptgraph.utils.structutils",
        "description": "conceptgraph.utils.structutils",
        "peekOfCode": "def img_to_b64str(img, quality=95):\n    r\"\"\"Converts a numpy array of uint8 into a base64 jpeg string.\n    Args\n        img (np.ndarray): RGB or greyscale image array\n        quality (int): Image quality from 0 to 100 (the higher is the better). Default: 95\n    Returns:\n        str: base64 jpeg string\n    \"\"\"\n    # Can also use px._imshow._array_to_b64str:\n    # https://github.com/plotly/plotly.py/blob/63f20ee08d2b83075d3749ec5d85f7909401a0ef/packages/python/plotly/plotly/express/_imshow.py#L27",
        "detail": "conceptgraph.utils.structutils",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "conceptgraph.utils.structutils",
        "description": "conceptgraph.utils.structutils",
        "peekOfCode": "__all__ = []\ndef list_to_padded(\n    x: List[torch.Tensor],\n    pad_size: Union[list, tuple, None] = None,\n    pad_value: float = 0.0,\n    equisized: bool = False,\n) -> torch.Tensor:\n    r\"\"\"Transforms a list of B tensors each of shape :math:`(N_b, C_b)` into a single tensor of shape\n    :math:`(N, pad_size(0), pad_size(1))`, or :math:`(N, max(N_b), max(C_b))` if pad_size is None.\n    Args:",
        "detail": "conceptgraph.utils.structutils",
        "documentation": {}
    },
    {
        "label": "OnlineObjectRenderer",
        "kind": 6,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "class OnlineObjectRenderer():\n    '''\n    Refactor of the open3d visualization code to make it more modular\n    '''\n    def __init__(\n        self, \n        view_param: str | dict,\n        base_objects: MapObjectList | None = None,\n        gray_map: bool = False\n    ) -> None:",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "CustomBoxAnnotator",
        "kind": 6,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "class CustomBoxAnnotator(sv.BoxAnnotator):\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        thickness: int = 2,\n        text_color: Color = Color.BLACK,\n        text_scale: float = 0.5,\n        text_thickness: int = 1,\n        text_padding: int = 10,\n        detections: sv.Detections = None,",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "LineMesh",
        "kind": 6,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "class LineMesh(object):\n    def __init__(self, points, lines=None, colors=[0, 1, 0], radius=0.15):\n        \"\"\"Creates a line represented as sequence of cylinder triangular meshes\n        Arguments:\n            points {ndarray} -- Numpy array of ponts Nx3.\n        Keyword Arguments:\n            lines {list[list] or None} -- List of point index pairs denoting line segments. If None, implicit lines from ordered pairwise points. (default: {None})\n            colors {list} -- list of colors, or single color of the line (default: {[0, 1, 0]})\n            radius {float} -- radius of cylinder (default: {0.15})\n        \"\"\"",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "get_random_colors",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def get_random_colors(num_colors):\n    '''\n    Generate random colors for visualization\n    Args:\n        num_colors (int): number of colors to generate\n    Returns:\n        colors (np.ndarray): (num_colors, 3) array of colors, in RGB, [0, 1]\n    '''\n    colors = []\n    for i in range(num_colors):",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "show_mask",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "show_points",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \ndef show_box(box, ax, label=None):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n    if label is not None:",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "show_box",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def show_box(box, ax, label=None):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n    if label is not None:\n        ax.text(x0, y0, label)\ndef vis_result_fast_on_depth(\n    depth_image: np.ndarray, \n    detections: sv.Detections, \n    classes: list[str], ",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast_on_depth",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def vis_result_fast_on_depth(\n    depth_image: np.ndarray, \n    detections: sv.Detections, \n    classes: list[str], \n    color: Color | ColorPalette = ColorPalette.DEFAULT, \n    instance_random_color: bool = False,\n    draw_bbox: bool = True,\n) -> np.ndarray:\n    '''\n    Annotate the image with the detection results. ",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "old_filter_detections",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def old_filter_detections(\n    detections: sv.Detections, \n    classes: list[str], \n    top_x_detections: Optional[int] = None, \n    confidence_threshold: float = 0.0,\n    given_labels: Optional[list[str]] = None\n) -> tuple[sv.Detections, list[str]]:\n    '''\n    Filter detections based on confidence threshold and top X detections.\n    Returns a tuple of filtered detections and labels.",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_for_vlm",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def vis_result_for_vlm(\n    image: np.ndarray, \n    detections: sv.Detections, \n    labels: list[str], \n    color: Color | ColorPalette = ColorPalette.DEFAULT, \n    draw_bbox: bool = True,\n    thickness: int = 2,\n    text_scale: float = 0.3,\n    text_thickness: int = 1,\n    text_padding: int = 2,",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def vis_result_fast(\n    image: np.ndarray, \n    detections: sv.Detections, \n    classes: list[str], \n    color: Color | ColorPalette = ColorPalette.DEFAULT, \n    instance_random_color: bool = False,\n    draw_bbox: bool = True,\n) -> np.ndarray:\n    '''\n    Annotate the image with the detection results. ",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_slow_caption",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def vis_result_slow_caption(image, masks, boxes_filt, pred_phrases, caption, text_prompt):\n    '''\n    Annotate the image with detection results, together with captions and text prompts.\n    This function is very slow, but the output is more readable.\n    '''\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)\n    for mask in masks:\n        show_mask(mask, plt.gca(), random_color=True)\n    for box, label in zip(boxes_filt, pred_phrases):",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_sam_mask",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def vis_sam_mask(anns):\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n    img[:,:,3] = 0\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        color_mask = np.concatenate([np.random.random(3), [0.35]])\n        img[m] = color_mask\n    return img\ndef poses2lineset(poses, color=[0, 0, 1]):",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "poses2lineset",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def poses2lineset(poses, color=[0, 0, 1]):\n    '''\n    Create a open3d line set from a batch of poses\n    poses: (N, 4, 4)\n    color: (3,)\n    '''\n    N = poses.shape[0]\n    lineset = o3d.geometry.LineSet()\n    lineset.points = o3d.utility.Vector3dVector(poses[:, :3, 3])\n    lineset.lines = o3d.utility.Vector2iVector(",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "create_camera_frustum",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def create_camera_frustum(\n    camera_pose, width=1, height=1, z_near=0.5, z_far=1, color=[0, 0, 1]\n):\n    K = np.array([[z_near, 0, 0], [0, z_near, 0], [0, 0, z_near + z_far]])\n    points = np.array(\n        [\n            [-width / 2, -height / 2, z_near],\n            [width / 2, -height / 2, z_near],\n            [width / 2, height / 2, z_near],\n            [-width / 2, height / 2, z_near],",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "better_camera_frustum",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def better_camera_frustum(camera_pose, img_h, img_w, scale=3.0, color=[0, 0, 1]):\n    # Convert camera pose tensor to numpy array\n    if isinstance(camera_pose, torch.Tensor):\n        camera_pose = camera_pose.numpy()\n    # Define near and far distance (adjust these as needed)\n    near = scale * 0.1\n    far = scale * 1.0\n    # Define frustum dimensions at the near plane (replace with appropriate values)\n    frustum_h = near\n    frustum_w = frustum_h * img_w / img_h  # Set frustum width based on its height and the image aspect ratio",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "align_vector_to_another",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    \"\"\"\n    Aligns vector a to vector b with axis angle rotation\n    \"\"\"\n    if np.array_equal(a, b):\n        return None, None\n    axis_ = np.cross(a, b)\n    axis_ = axis_ / np.linalg.norm(axis_)\n    angle = np.arccos(np.dot(a, b))\n    return axis_, angle",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "normalized",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def normalized(a, axis=-1, order=2):\n    \"\"\"Normalizes a numpy array of points\"\"\"\n    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n    l2[l2 == 0] = 1\n    return a / np.expand_dims(l2, axis), l2\ndef save_video_detections(exp_out_path, save_path=None, fps=30):\n    '''\n    Save the detections in the folder as a video\n    '''\n    if save_path is None:",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_detections",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def save_video_detections(exp_out_path, save_path=None, fps=30):\n    '''\n    Save the detections in the folder as a video\n    '''\n    if save_path is None:\n        save_path = exp_out_path / \"vis_video.mp4\"\n    # Get the list of images\n    image_files = list((exp_out_path / \"vis\").glob(\"*.jpg\"))\n    image_files.sort()\n    # Read the first image to get the size",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_from_frames",
        "kind": 2,
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "peekOfCode": "def save_video_from_frames(frames, save_path, fps=30):\n    \"\"\"\n    Save a video from an array of frames.\n    Args:\n    - frames: An array of frames, each frame being an image array.\n    - save_path: Path where the video should be saved.\n    - fps: Frames per second for the output video.\n    \"\"\"\n    # Ensure frames are in uint8\n    frames = np.asarray(frames).astype(np.uint8)",
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "get_openai_client",
        "kind": 2,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "def get_openai_client():\n    client = OpenAI(\n        api_key=os.getenv('OPENAI_API_KEY')\n    )\n    return client\n# Function to encode the image as base64\ndef encode_image_for_openai(image_path: str, resize = False, target_size: int=512):\n    print(f\"Checking if image exists at path: {image_path}\")\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image file not found: {image_path}\")",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "encode_image_for_openai",
        "kind": 2,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "def encode_image_for_openai(image_path: str, resize = False, target_size: int=512):\n    print(f\"Checking if image exists at path: {image_path}\")\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n    if not resize:\n        # Open the image\n        print(f\"Opening image from path: {image_path}\")\n        with open(image_path, \"rb\") as img_file:\n            encoded_image = base64.b64encode(img_file.read()).decode('utf-8')\n            print(\"Image encoded in base64 format.\")",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "consolidate_captions",
        "kind": 2,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "def consolidate_captions(client: OpenAI, captions: list):\n    # Formatting the captions into a single string prompt\n    captions_text = \"\\n\".join([f\"{cap['caption']}\" for cap in captions if cap['caption'] is not None])\n    user_query = f\"Here are several captions for the same object:\\n{captions_text}\\n\\nPlease consolidate these into a single, clear caption that accurately describes the object.\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt_consolidate_captions\n        },\n        {",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "extract_list_of_tuples",
        "kind": 2,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "def extract_list_of_tuples(text: str):\n    # Pattern to match a list of tuples, considering a list that starts with '[' and ends with ']'\n    # and contains any characters in between, including nested lists/tuples.\n    text = text.replace('\\n', ' ')\n    pattern = r'\\[.*?\\]'\n    # Search for the pattern in the text\n    match = re.search(pattern, text)\n    if match:\n        # Extract the matched string\n        list_str = match.group(0)",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "vlm_extract_object_captions",
        "kind": 2,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "def vlm_extract_object_captions(text: str):\n    # Replace newlines with spaces for uniformity\n    text = text.replace('\\n', ' ')\n    # Pattern to match the list of objects\n    pattern = r'\\[(.*?)\\]'\n    # Search for the pattern in the text\n    match = re.search(pattern, text)\n    if match:\n        # Extract the matched string\n        list_str = match.group(0)",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "get_obj_rel_from_image_gpt4v",
        "kind": 2,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "def get_obj_rel_from_image_gpt4v(client: OpenAI, image_path: str, label_list: list):\n    # Getting the base64 string\n    base64_image = encode_image_for_openai(image_path)\n    global system_prompt\n    global gpt_model\n    user_query = f\"Here is the list of labels for the annotations of the objects in the image: {label_list}. Please describe the spatial relationships between the objects in the image.\"\n    vlm_answer = []\n    try:\n        response = client.chat.completions.create(\n            model=f\"{gpt_model}\",",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "get_obj_captions_from_image_gpt4v",
        "kind": 2,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "def get_obj_captions_from_image_gpt4v(client: OpenAI, image_path: str, label_list: list):\n    # Getting the base64 string\n    base64_image = encode_image_for_openai(image_path)\n    global system_prompt\n    user_query = f\"Here is the list of labels for the annotations of the objects in the image: {label_list}. Please accurately caption the objects in the image.\"\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt_captions\n        },",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "system_prompt_1",
        "kind": 5,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "system_prompt_1 = '''\nYou are an agent specialized in describing the spatial relationships between objects in an annotated image.\nYou will be provided with an annotated image and a list of labels for the annotations. Your task is to determine the spatial relationships between the annotated objects in the image, and return a list of these relationships in the correct list of tuples format as follows:\n[(\"object1\", \"spatial relationship\", \"object2\"), (\"object3\", \"spatial relationship\", \"object4\"), ...]\nYour options for the spatial relationship are \"on top of\" and \"next to\".\nFor example, you may get an annotated image and a list such as \n[\"cup 3\", \"book 4\", \"clock 5\", \"table 2\", \"candle 7\", \"music stand 6\", \"lamp 8\"]\nYour response should be a description of the spatial relationships between the objects in the image. \nAn example to illustrate the response format:\n[(\"book 4\", \"on top of\", \"table 2\"), (\"cup 3\", \"next to\", \"book 4\"), (\"lamp 8\", \"on top of\", \"music stand 6\")]",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "system_prompt_only_top",
        "kind": 5,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "system_prompt_only_top = '''\nYou are an agent specializing in identifying the physical and spatial relationships in annotated images for 3D mapping.\nIn the images, each object is annotated with a bright numeric id (i.e. a number) and a corresponding colored contour outline. Your task is to analyze the images and output a list of tuples describing the physical relationships between objects. Format your response as follows: [(\"1\", \"relation type\", \"2\"), ...]. When uncertain, return an empty list.\nNote that you are describing the **physical relationships** between the **objects inside** the image.\nYou will also be given a text list of the numeric ids of the objects in the image. The list will be in the format: [\"1: name1\", \"2: name2\", \"3: name3\" ...], only output the physical relationships between the objects in the list.\nThe relation types you must report are:\n- phyically placed on top of: (\"object x\", \"on top of\", \"object y\") \n- phyically placed underneath: (\"object x\", \"under\", \"object y\") \nAn illustrative example of the expected response format might look like this:\n[(\"object 1\", \"on top of\", \"object 2\"), (\"object 3\", \"under\", \"object 2\"), (\"object 4\", \"on top of\", \"object 3\")]. Do not put the names of the objects in your response, only the numeric ids.",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "system_prompt_captions",
        "kind": 5,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "system_prompt_captions = '''\nYou are an agent specializing in accurate captioning objects in an image.\nIn the images, each object is annotated with a bright numeric id (i.e. a number) and a corresponding colored contour outline. Your task is to analyze the images and output in a structured format, the captions for the objects.\nYou will also be given a text list of the numeric ids and names of the objects in the image. The list will be in the format: [\"1: name1\", \"2: name2\", \"3: name3\" ...]\nThe names were obtained from a simple object detection system and may be inaacurate.\nYour response should be in the format of a list of dictionaries, where each dictionary contains the id, name, and caption of an object. Your response will be evaluated as a python list of dictionaries, so make sure to format it correctly. An example of the expected response format is as follows:\n[\n    {\"id\": \"1\", \"name\": \"object1\", \"caption\": \"concise description of the object1 in the image\"},\n    {\"id\": \"2\", \"name\": \"object2\", \"caption\": \"concise description of the object2 in the image\"},\n    {\"id\": \"3\", \"name\": \"object3\", \"caption\": \"concise description of the object3 in the image\"}",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "system_prompt_consolidate_captions",
        "kind": 5,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "system_prompt_consolidate_captions = '''\nYou are an agent specializing in consolidating multiple captions for the same object into a single, clear, and accurate caption.\nYou will be provided with several captions describing the same object. Your task is to analyze these captions, identify the common elements, remove any noise or outliers, and consolidate them into a single, coherent caption that accurately describes the object.\nEnsure the consolidated caption is clear, concise, and captures the essential details from the provided captions.\nHere is an example of the input format:\n[\n    {\"id\": \"3\", \"name\": \"cigar box\", \"caption\": \"rectangular cigar box on the side cabinet\"},\n    {\"id\": \"9\", \"name\": \"cigar box\", \"caption\": \"A small cigar box placed on the side cabinet.\"},\n    {\"id\": \"7\", \"name\": \"cigar box\", \"caption\": \"A small cigar box is on the side cabinet.\"},\n    {\"id\": \"8\", \"name\": \"cigar box\", \"caption\": \"Box on top of the dresser\"},",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "system_prompt",
        "kind": 5,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "system_prompt = system_prompt_only_top\n# gpt_model = \"gpt-4-vision-preview\"\ngpt_model = \"gpt-4o-2024-05-13\"\ndef get_openai_client():\n    client = OpenAI(\n        api_key=os.getenv('OPENAI_API_KEY')\n    )\n    return client\n# Function to encode the image as base64\ndef encode_image_for_openai(image_path: str, resize = False, target_size: int=512):",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "gpt_model",
        "kind": 5,
        "importPath": "conceptgraph.utils.vlm",
        "description": "conceptgraph.utils.vlm",
        "peekOfCode": "gpt_model = \"gpt-4o-2024-05-13\"\ndef get_openai_client():\n    client = OpenAI(\n        api_key=os.getenv('OPENAI_API_KEY')\n    )\n    return client\n# Function to encode the image as base64\ndef encode_image_for_openai(image_path: str, resize = False, target_size: int=512):\n    print(f\"Checking if image exists at path: {image_path}\")\n    if not os.path.exists(image_path):",
        "detail": "conceptgraph.utils.vlm",
        "documentation": {}
    },
    {
        "label": "detections_path",
        "kind": 5,
        "importPath": "testbench",
        "description": "testbench",
        "peekOfCode": "detections_path = \"/home/kev/datasets/Replica/room0/gsa_detections_none/frame000000.pkl.gz\"\nwith gzip.open(detections_path, \"rb\") as f:\n    gobs = pickle.load(f)\nprint(gobs.keys())\nprint(gobs['xyxy'])",
        "detail": "testbench",
        "documentation": {}
    }
]