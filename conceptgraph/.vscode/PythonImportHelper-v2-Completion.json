[
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "Controller",
        "importPath": "ai2thor.controller",
        "description": "ai2thor.controller",
        "isExtraImport": true,
        "detail": "ai2thor.controller",
        "documentation": {}
    },
    {
        "label": "Controller",
        "importPath": "ai2thor.controller",
        "description": "ai2thor.controller",
        "isExtraImport": true,
        "detail": "ai2thor.controller",
        "documentation": {}
    },
    {
        "label": "Controller",
        "importPath": "ai2thor.controller",
        "description": "ai2thor.controller",
        "isExtraImport": true,
        "detail": "ai2thor.controller",
        "documentation": {}
    },
    {
        "label": "Controller",
        "importPath": "ai2thor.controller",
        "description": "ai2thor.controller",
        "isExtraImport": true,
        "detail": "ai2thor.controller",
        "documentation": {}
    },
    {
        "label": "parse_object_receptacle_mapping",
        "importPath": "conceptgraph.ai2thor.utils",
        "description": "conceptgraph.ai2thor.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.ai2thor.utils",
        "documentation": {}
    },
    {
        "label": "compute_pose",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_posrot",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_agent_pose_from_event",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_camera_pose_from_event",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_top_down_frame",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_intrinsics",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_pose",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_scene",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "sample_pose_uniform",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "sample_pose_random",
        "importPath": "conceptgraph.utils.ai2thor",
        "description": "conceptgraph.utils.ai2thor",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "abc",
        "description": "abc",
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "imageio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imageio",
        "description": "imageio",
        "detail": "imageio",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "natsort",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "natsort",
        "description": "natsort",
        "detail": "natsort",
        "documentation": {}
    },
    {
        "label": "natsorted",
        "importPath": "natsort",
        "description": "natsort",
        "isExtraImport": true,
        "detail": "natsort",
        "documentation": {}
    },
    {
        "label": "natsorted",
        "importPath": "natsort",
        "description": "natsort",
        "isExtraImport": true,
        "detail": "natsort",
        "documentation": {}
    },
    {
        "label": "datautils",
        "importPath": "gradslam.datasets",
        "description": "gradslam.datasets",
        "isExtraImport": true,
        "detail": "gradslam.datasets",
        "documentation": {}
    },
    {
        "label": "relative_transformation",
        "importPath": "gradslam.geometry.geometryutils",
        "description": "gradslam.geometry.geometryutils",
        "isExtraImport": true,
        "detail": "gradslam.geometry.geometryutils",
        "documentation": {}
    },
    {
        "label": "PointFusion",
        "importPath": "gradslam.slam.pointfusion",
        "description": "gradslam.slam.pointfusion",
        "isExtraImport": true,
        "detail": "gradslam.slam.pointfusion",
        "documentation": {}
    },
    {
        "label": "PointFusion",
        "importPath": "gradslam.slam.pointfusion",
        "description": "gradslam.slam.pointfusion",
        "isExtraImport": true,
        "detail": "gradslam.slam.pointfusion",
        "documentation": {}
    },
    {
        "label": "RGBDImages",
        "importPath": "gradslam.structures.rgbdimages",
        "description": "gradslam.structures.rgbdimages",
        "isExtraImport": true,
        "detail": "gradslam.structures.rgbdimages",
        "documentation": {}
    },
    {
        "label": "RGBDImages",
        "importPath": "gradslam.structures.rgbdimages",
        "description": "gradslam.structures.rgbdimages",
        "isExtraImport": true,
        "detail": "gradslam.structures.rgbdimages",
        "documentation": {}
    },
    {
        "label": "to_scalar",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "prjson",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "prjson",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_hydra_config",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "cfg_to_dict",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "prjson",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_tensor",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_tensor",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_tensor",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_tensor",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "importPath": "conceptgraph.utils.general_utils",
        "description": "conceptgraph.utils.general_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.general_utils",
        "documentation": {}
    },
    {
        "label": "dataclasses",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dataclasses",
        "description": "dataclasses",
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "liblzfse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "liblzfse",
        "description": "liblzfse",
        "detail": "liblzfse",
        "documentation": {}
    },
    {
        "label": "png",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "png",
        "description": "png",
        "detail": "png",
        "documentation": {}
    },
    {
        "label": "tyro",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tyro",
        "description": "tyro",
        "detail": "tyro",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "Record3DStream",
        "importPath": "record3d",
        "description": "record3d",
        "isExtraImport": true,
        "detail": "record3d",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "llava.conversation",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "llava.conversation",
        "description": "llava.conversation",
        "detail": "llava.conversation",
        "documentation": {}
    },
    {
        "label": "conv_templates",
        "importPath": "llava.conversation",
        "description": "llava.conversation",
        "isExtraImport": true,
        "detail": "llava.conversation",
        "documentation": {}
    },
    {
        "label": "SeparatorStyle",
        "importPath": "llava.conversation",
        "description": "llava.conversation",
        "isExtraImport": true,
        "detail": "llava.conversation",
        "documentation": {}
    },
    {
        "label": "SeparatorStyle",
        "importPath": "llava.conversation",
        "description": "llava.conversation",
        "isExtraImport": true,
        "detail": "llava.conversation",
        "documentation": {}
    },
    {
        "label": "conv_templates",
        "importPath": "llava.conversation",
        "description": "llava.conversation",
        "isExtraImport": true,
        "detail": "llava.conversation",
        "documentation": {}
    },
    {
        "label": "conv_templates",
        "importPath": "llava.conversation",
        "description": "llava.conversation",
        "isExtraImport": true,
        "detail": "llava.conversation",
        "documentation": {}
    },
    {
        "label": "SeparatorStyle",
        "importPath": "llava.conversation",
        "description": "llava.conversation",
        "isExtraImport": true,
        "detail": "llava.conversation",
        "documentation": {}
    },
    {
        "label": "load_pretrained_model",
        "importPath": "llava.model.builder",
        "description": "llava.model.builder",
        "isExtraImport": true,
        "detail": "llava.model.builder",
        "documentation": {}
    },
    {
        "label": "load_pretrained_model",
        "importPath": "llava.model.builder",
        "description": "llava.model.builder",
        "isExtraImport": true,
        "detail": "llava.model.builder",
        "documentation": {}
    },
    {
        "label": "get_model_name_from_path",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "process_images",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "tokenizer_image_token",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "KeywordsStoppingCriteria",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "get_model_name_from_path",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "process_images",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "tokenizer_image_token",
        "importPath": "llava.mm_utils",
        "description": "llava.mm_utils",
        "isExtraImport": true,
        "detail": "llava.mm_utils",
        "documentation": {}
    },
    {
        "label": "IMAGE_TOKEN_INDEX",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_TOKEN",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IM_START_TOKEN",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IM_END_TOKEN",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "IGNORE_INDEX",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "IMAGE_TOKEN_INDEX",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_TOKEN",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IM_START_TOKEN",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IM_END_TOKEN",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "IGNORE_INDEX",
        "importPath": "llava.constants",
        "description": "llava.constants",
        "isExtraImport": true,
        "detail": "llava.constants",
        "documentation": {}
    },
    {
        "label": "disable_torch_init",
        "importPath": "llava.utils",
        "description": "llava.utils",
        "isExtraImport": true,
        "detail": "llava.utils",
        "documentation": {}
    },
    {
        "label": "disable_torch_init",
        "importPath": "llava.utils",
        "description": "llava.utils",
        "isExtraImport": true,
        "detail": "llava.utils",
        "documentation": {}
    },
    {
        "label": "disable_torch_init",
        "importPath": "llava.utils",
        "description": "llava.utils",
        "isExtraImport": true,
        "detail": "llava.utils",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "matplotlib.patches",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.patches",
        "description": "matplotlib.patches",
        "detail": "matplotlib.patches",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "make_axes_locatable",
        "importPath": "mpl_toolkits.axes_grid1",
        "description": "mpl_toolkits.axes_grid1",
        "isExtraImport": true,
        "detail": "mpl_toolkits.axes_grid1",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPImageProcessor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPVisionModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BaseModelOutputWithPast",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "CausalLMOutputWithPast",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "gc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gc",
        "description": "gc",
        "detail": "gc",
        "documentation": {}
    },
    {
        "label": "gzip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gzip",
        "description": "gzip",
        "detail": "gzip",
        "documentation": {}
    },
    {
        "label": "SimpleNamespace",
        "importPath": "types",
        "description": "types",
        "isExtraImport": true,
        "detail": "types",
        "documentation": {}
    },
    {
        "label": "SimpleNamespace",
        "importPath": "types",
        "description": "types",
        "isExtraImport": true,
        "detail": "types",
        "documentation": {}
    },
    {
        "label": "wrap",
        "importPath": "textwrap",
        "description": "textwrap",
        "isExtraImport": true,
        "detail": "textwrap",
        "documentation": {}
    },
    {
        "label": "wrap",
        "importPath": "textwrap",
        "description": "textwrap",
        "isExtraImport": true,
        "detail": "textwrap",
        "documentation": {}
    },
    {
        "label": "rich",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "rich",
        "description": "rich",
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "csr_matrix",
        "importPath": "scipy.sparse",
        "description": "scipy.sparse",
        "isExtraImport": true,
        "detail": "scipy.sparse",
        "documentation": {}
    },
    {
        "label": "csr_matrix",
        "importPath": "scipy.sparse",
        "description": "scipy.sparse",
        "isExtraImport": true,
        "detail": "scipy.sparse",
        "documentation": {}
    },
    {
        "label": "connected_components",
        "importPath": "scipy.sparse.csgraph",
        "description": "scipy.sparse.csgraph",
        "isExtraImport": true,
        "detail": "scipy.sparse.csgraph",
        "documentation": {}
    },
    {
        "label": "minimum_spanning_tree",
        "importPath": "scipy.sparse.csgraph",
        "description": "scipy.sparse.csgraph",
        "isExtraImport": true,
        "detail": "scipy.sparse.csgraph",
        "documentation": {}
    },
    {
        "label": "connected_components",
        "importPath": "scipy.sparse.csgraph",
        "description": "scipy.sparse.csgraph",
        "isExtraImport": true,
        "detail": "scipy.sparse.csgraph",
        "documentation": {}
    },
    {
        "label": "minimum_spanning_tree",
        "importPath": "scipy.sparse.csgraph",
        "description": "scipy.sparse.csgraph",
        "isExtraImport": true,
        "detail": "scipy.sparse.csgraph",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "DetectionList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "DetectionList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "DetectionList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "DetectionList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "importPath": "conceptgraph.slam.slam_classes",
        "description": "conceptgraph.slam.slam_classes",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_gobs",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "create_or_load_colors",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_obj2_into_obj1",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "denoise_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "gobs_to_detection_list",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_obj2_into_obj1",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "compute_overlap_matrix_2set",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "create_or_load_colors",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_obj2_into_obj1",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "denoise_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "gobs_to_detection_list",
        "importPath": "conceptgraph.slam.utils",
        "description": "conceptgraph.slam.utils",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.utils",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "zlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "zlib",
        "description": "zlib",
        "detail": "zlib",
        "documentation": {}
    },
    {
        "label": "SensorData",
        "importPath": "SensorData",
        "description": "SensorData",
        "isExtraImport": true,
        "detail": "SensorData",
        "documentation": {}
    },
    {
        "label": "gzip,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gzip.",
        "description": "gzip.",
        "detail": "gzip.",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "open3d",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "open3d",
        "description": "open3d",
        "detail": "open3d",
        "documentation": {}
    },
    {
        "label": "better_camera_frustum",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "LineMesh",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_slow_caption",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "get_random_colors",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_slow_caption",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_slow_caption",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_detections",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "LineMesh",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "OnlineObjectRenderer",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "OnlineObjectRenderer",
        "importPath": "conceptgraph.utils.vis",
        "description": "conceptgraph.utils.vis",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.vis",
        "documentation": {}
    },
    {
        "label": "distinctipy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "distinctipy",
        "description": "distinctipy",
        "detail": "distinctipy",
        "documentation": {}
    },
    {
        "label": "Detections",
        "importPath": "supervision.detection.core",
        "description": "supervision.detection.core",
        "isExtraImport": true,
        "detail": "supervision.detection.core",
        "documentation": {}
    },
    {
        "label": "Color",
        "importPath": "supervision.draw.color",
        "description": "supervision.draw.color",
        "isExtraImport": true,
        "detail": "supervision.draw.color",
        "documentation": {}
    },
    {
        "label": "ColorPalette",
        "importPath": "supervision.draw.color",
        "description": "supervision.draw.color",
        "isExtraImport": true,
        "detail": "supervision.draw.color",
        "documentation": {}
    },
    {
        "label": "Color",
        "importPath": "supervision.draw.color",
        "description": "supervision.draw.color",
        "isExtraImport": true,
        "detail": "supervision.draw.color",
        "documentation": {}
    },
    {
        "label": "ColorPalette",
        "importPath": "supervision.draw.color",
        "description": "supervision.draw.color",
        "isExtraImport": true,
        "detail": "supervision.draw.color",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "from_intrinsics_matrix",
        "importPath": "conceptgraph.dataset.datasets_common",
        "description": "conceptgraph.dataset.datasets_common",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "open_clip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "open_clip",
        "description": "open_clip",
        "detail": "open_clip",
        "documentation": {}
    },
    {
        "label": "knn_points",
        "importPath": "chamferdist.chamfer",
        "description": "chamferdist.chamfer",
        "isExtraImport": true,
        "detail": "chamferdist.chamfer",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "gradslam.structures.pointclouds",
        "description": "gradslam.structures.pointclouds",
        "isExtraImport": true,
        "detail": "gradslam.structures.pointclouds",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "gradslam.structures.pointclouds",
        "description": "gradslam.structures.pointclouds",
        "isExtraImport": true,
        "detail": "gradslam.structures.pointclouds",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "gradslam.structures.pointclouds",
        "description": "gradslam.structures.pointclouds",
        "isExtraImport": true,
        "detail": "gradslam.structures.pointclouds",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "gradslam.structures.pointclouds",
        "description": "gradslam.structures.pointclouds",
        "isExtraImport": true,
        "detail": "gradslam.structures.pointclouds",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "gradslam.structures.pointclouds",
        "description": "gradslam.structures.pointclouds",
        "isExtraImport": true,
        "detail": "gradslam.structures.pointclouds",
        "documentation": {}
    },
    {
        "label": "REPLICA_EXISTING_CLASSES",
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_CLASSES",
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_SCENE_IDS",
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_SCENE_IDS_",
        "importPath": "conceptgraph.dataset.replica_constants",
        "description": "conceptgraph.dataset.replica_constants",
        "isExtraImport": true,
        "detail": "conceptgraph.dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "compute_confmatrix",
        "importPath": "conceptgraph.utils.eval",
        "description": "conceptgraph.utils.eval",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.eval",
        "documentation": {}
    },
    {
        "label": "compute_pred_gt_associations",
        "importPath": "conceptgraph.utils.eval",
        "description": "conceptgraph.utils.eval",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.eval",
        "documentation": {}
    },
    {
        "label": "compute_metrics",
        "importPath": "conceptgraph.utils.eval",
        "description": "conceptgraph.utils.eval",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.eval",
        "documentation": {}
    },
    {
        "label": "CloudRendering",
        "importPath": "ai2thor.platform",
        "description": "ai2thor.platform",
        "isExtraImport": true,
        "detail": "ai2thor.platform",
        "documentation": {}
    },
    {
        "label": "rearrange_objects",
        "importPath": "conceptgraph.ai2thor.rearrange",
        "description": "conceptgraph.ai2thor.rearrange",
        "isExtraImport": true,
        "detail": "conceptgraph.ai2thor.rearrange",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "supervision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "supervision",
        "description": "supervision",
        "detail": "supervision",
        "documentation": {}
    },
    {
        "label": "compute_clip_features",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_clip_features_batched",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_ft_vector_closeness_statistics",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "get_sam_predictor",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "get_sam_segmentation_from_xyxy_batched",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "get_sam_segmentation_from_xyxy",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_clip_features",
        "importPath": "conceptgraph.utils.model_utils",
        "description": "conceptgraph.utils.model_utils",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.model_utils",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "get_sam_predictor",
        "importPath": "conceptgraph.scripts.generate_gsa_results",
        "description": "conceptgraph.scripts.generate_gsa_results",
        "isExtraImport": true,
        "detail": "conceptgraph.scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "hydra",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hydra",
        "description": "hydra",
        "detail": "hydra",
        "documentation": {}
    },
    {
        "label": "profile",
        "importPath": "line_profiler",
        "description": "line_profiler",
        "isExtraImport": true,
        "detail": "line_profiler",
        "documentation": {}
    },
    {
        "label": "omegaconf",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "omegaconf",
        "description": "omegaconf",
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "YOLO",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "SAM",
        "importPath": "ultralytics",
        "description": "ultralytics",
        "isExtraImport": true,
        "detail": "ultralytics",
        "documentation": {}
    },
    {
        "label": "sam_model_registry",
        "importPath": "segment_anything",
        "description": "segment_anything",
        "isExtraImport": true,
        "detail": "segment_anything",
        "documentation": {}
    },
    {
        "label": "SamPredictor",
        "importPath": "segment_anything",
        "description": "segment_anything",
        "isExtraImport": true,
        "detail": "segment_anything",
        "documentation": {}
    },
    {
        "label": "SamAutomaticMaskGenerator",
        "importPath": "segment_anything",
        "description": "segment_anything",
        "isExtraImport": true,
        "detail": "segment_anything",
        "documentation": {}
    },
    {
        "label": "LLaVaChat",
        "importPath": "conceptgraph.llava.llava_model",
        "description": "conceptgraph.llava.llava_model",
        "isExtraImport": true,
        "detail": "conceptgraph.llava.llava_model",
        "documentation": {}
    },
    {
        "label": "crop_image_pil",
        "importPath": "conceptgraph.utils.image",
        "description": "conceptgraph.utils.image",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.image",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "compute_2d_box_contained_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_iou_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_giou_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_iou_accuracte_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_giou_accurate_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_2d_box_contained_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_iou",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_iou_accuracte_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "mask_subtract_contained",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_iou_batch",
        "importPath": "conceptgraph.utils.ious",
        "description": "conceptgraph.utils.ious",
        "isExtraImport": true,
        "detail": "conceptgraph.utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_spatial_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_visual_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "aggregate_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "merge_detections_to_objects",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_spatial_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_visual_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "aggregate_similarities",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "merge_detections_to_objects",
        "importPath": "conceptgraph.slam.mapping",
        "description": "conceptgraph.slam.mapping",
        "isExtraImport": true,
        "detail": "conceptgraph.slam.mapping",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "faiss",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "faiss",
        "description": "faiss",
        "detail": "faiss",
        "documentation": {}
    },
    {
        "label": "prior",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "prior",
        "description": "prior",
        "detail": "prior",
        "documentation": {}
    },
    {
        "label": "sam_model_registry",
        "importPath": "segment_anything.segment_anything",
        "description": "segment_anything.segment_anything",
        "isExtraImport": true,
        "detail": "segment_anything.segment_anything",
        "documentation": {}
    },
    {
        "label": "SamPredictor",
        "importPath": "segment_anything.segment_anything",
        "description": "segment_anything.segment_anything",
        "isExtraImport": true,
        "detail": "segment_anything.segment_anything",
        "documentation": {}
    },
    {
        "label": "SamAutomaticMaskGenerator",
        "importPath": "segment_anything.segment_anything",
        "description": "segment_anything.segment_anything",
        "isExtraImport": true,
        "detail": "segment_anything.segment_anything",
        "documentation": {}
    },
    {
        "label": "cosine",
        "importPath": "scipy.spatial.distance",
        "description": "scipy.spatial.distance",
        "isExtraImport": true,
        "detail": "scipy.spatial.distance",
        "documentation": {}
    },
    {
        "label": "rearrange_objects",
        "kind": 2,
        "importPath": "ai2thor.rearrange",
        "description": "ai2thor.rearrange",
        "peekOfCode": "def rearrange_objects(\n    controller: Controller,\n    pickupable_move_ratio: float,\n    moveable_move_ratio: float,\n    random_seed: int | None = None,\n    reset: bool = False,\n):\n    '''\n    Rearrange objects in the scene. \n    '''",
        "detail": "ai2thor.rearrange",
        "documentation": {}
    },
    {
        "label": "parse_object_receptacle_mapping",
        "kind": 2,
        "importPath": "ai2thor.utils",
        "description": "ai2thor.utils",
        "peekOfCode": "def parse_object_receptacle_mapping(controller: Controller):\n    # Parse the receptacle-object relationships\n    obj_list = controller.last_event.metadata[\"objects\"]\n    obj2receptacle = {}\n    receptacle2obj = {}\n    for obj in obj_list:\n        obj_id = obj['objectId']\n        parents = obj['parentReceptacles']\n        if parents is None:\n            continue",
        "detail": "ai2thor.utils",
        "documentation": {}
    },
    {
        "label": "compute_position_dist",
        "kind": 2,
        "importPath": "ai2thor.utils",
        "description": "ai2thor.utils",
        "peekOfCode": "def compute_position_dist(\n    p0: Mapping[str, Any],\n    p1: Mapping[str, Any],\n    ignore_y: bool = False,\n    l1_dist: bool = False,\n) -> float:\n    \"\"\"Distance between two points of the form {\"x\": x, \"y\":y, \"z\":z\"}.\"\"\"\n    if l1_dist:\n        return (\n            abs(p0[\"x\"] - p1[\"x\"])",
        "detail": "ai2thor.utils",
        "documentation": {}
    },
    {
        "label": "compute_rotation_dist",
        "kind": 2,
        "importPath": "ai2thor.utils",
        "description": "ai2thor.utils",
        "peekOfCode": "def compute_rotation_dist(a: Dict[str, float], b: Dict[str, float]):\n    \"\"\"Distance between rotations.\"\"\"\n    def deg_dist(d0: float, d1: float):\n        dist = (d0 - d1) % 360\n        return min(dist, 360 - dist)\n    return sum(deg_dist(a[k], b[k]) for k in [\"x\", \"y\", \"z\"])\ndef compute_angle_between_rotations(a: Dict[str, float], b: Dict[str, float]):\n    return np.abs(\n        (180 / (2 * math.pi))\n        * (",
        "detail": "ai2thor.utils",
        "documentation": {}
    },
    {
        "label": "compute_angle_between_rotations",
        "kind": 2,
        "importPath": "ai2thor.utils",
        "description": "ai2thor.utils",
        "peekOfCode": "def compute_angle_between_rotations(a: Dict[str, float], b: Dict[str, float]):\n    return np.abs(\n        (180 / (2 * math.pi))\n        * (\n            Rotation.from_euler(\"xyz\", [a[k] for k in \"xyz\"], degrees=True)\n            * Rotation.from_euler(\"xyz\", [b[k] for k in \"xyz\"], degrees=True).inv()\n        ).as_rotvec()\n    ).sum()",
        "detail": "ai2thor.utils",
        "documentation": {}
    },
    {
        "label": "GradSLAMDataset",
        "kind": 6,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "class GradSLAMDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        config_dict,\n        stride: Optional[int] = 1,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: int = 480,\n        desired_width: int = 640,\n        channels_first: bool = False,",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "ICLDataset",
        "kind": 6,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "class ICLDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict: Dict,\n        basedir: Union[Path, str],\n        sequence: Union[Path, str],\n        stride: Optional[int] = 1,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 480,",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "ReplicaDataset",
        "kind": 6,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "class ReplicaDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 480,",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "ScannetDataset",
        "kind": 6,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "class ScannetDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 968,",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "Ai2thorDataset",
        "kind": 6,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "class Ai2thorDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 968,",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "AzureKinectDataset",
        "kind": 6,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "class AzureKinectDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 480,",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "RealsenseDataset",
        "kind": 6,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "class RealsenseDataset(GradSLAMDataset):\n    \"\"\"\n    Dataset class to process depth images captured by realsense camera on the tabletop manipulator \n    \"\"\"\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "Record3DDataset",
        "kind": 6,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "class Record3DDataset(GradSLAMDataset):\n    \"\"\"\n    Dataset class to read in saved files from the structure created by our\n    `save_record3d_stream.py` script\n    \"\"\"\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "MultiscanDataset",
        "kind": 6,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "class MultiscanDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 480,",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "Hm3dDataset",
        "kind": 6,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "class Hm3dDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 480,",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "Hm3dOpeneqaDataset",
        "kind": 6,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "class Hm3dOpeneqaDataset(GradSLAMDataset):\n    def __init__(\n        self,\n        config_dict,\n        basedir,\n        sequence,\n        stride: Optional[int] = None,\n        start: Optional[int] = 0,\n        end: Optional[int] = -1,\n        desired_height: Optional[int] = 480,",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "as_intrinsics_matrix",
        "kind": 2,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "def as_intrinsics_matrix(intrinsics):\n    \"\"\"\n    Get matrix representation of intrinsics.\n    \"\"\"\n    K = np.eye(3)\n    K[0, 0] = intrinsics[0]\n    K[1, 1] = intrinsics[1]\n    K[0, 2] = intrinsics[2]\n    K[1, 2] = intrinsics[3]\n    return K",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "from_intrinsics_matrix",
        "kind": 2,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "def from_intrinsics_matrix(K: torch.Tensor) -> tuple[float, float, float, float]:\n    '''\n    Get fx, fy, cx, cy from the intrinsics matrix\n    return 4 scalars\n    '''\n    fx = to_scalar(K[0, 0])\n    fy = to_scalar(K[1, 1])\n    cx = to_scalar(K[0, 2])\n    cy = to_scalar(K[1, 2])\n    return fx, fy, cx, cy",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "readEXR_onlydepth",
        "kind": 2,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "def readEXR_onlydepth(filename):\n    \"\"\"\n    Read depth data from EXR image file.\n    Args:\n        filename (str): File path.\n    Returns:\n        Y (numpy.array): Depth buffer in float32 format.\n    \"\"\"\n    # move the import here since only CoFusion needs these package\n    # sometimes installation of openexr is hard, you can run all other datasets",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "load_dataset_config",
        "kind": 2,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "def load_dataset_config(path, default_path=None):\n    \"\"\"\n    Loads config file.\n    Args:\n        path (str): path to config file.\n        default_path (str, optional): whether to use default path. Defaults to None.\n    Returns:\n        cfg (dict): config dict.\n    \"\"\"\n    # load configuration from file itself",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "update_recursive",
        "kind": 2,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "def update_recursive(dict1, dict2):\n    \"\"\"\n    Update two config dictionaries recursively.\n    Args:\n        dict1 (dict): first dictionary to be updated.\n        dict2 (dict): second dictionary which entries should be used.\n    \"\"\"\n    for k, v in dict2.items():\n        if k not in dict1:\n            dict1[k] = dict()",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "common_dataset_to_batch",
        "kind": 2,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "def common_dataset_to_batch(dataset):\n    colors, depths, poses = [], [], []\n    intrinsics, embeddings = None, None\n    for idx in range(len(dataset)):\n        _color, _depth, intrinsics, _pose, _embedding = dataset[idx]\n        colors.append(_color)\n        depths.append(_depth)\n        poses.append(_pose)\n        if _embedding is not None:\n            if embeddings is None:",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "get_dataset",
        "kind": 2,
        "importPath": "dataset.datasets_common",
        "description": "dataset.datasets_common",
        "peekOfCode": "def get_dataset(dataconfig, basedir, sequence, **kwargs):\n    config_dict = load_dataset_config(dataconfig)\n    if config_dict[\"dataset_name\"].lower() in [\"icl\"]:\n        return ICLDataset(config_dict, basedir, sequence, **kwargs)\n    elif config_dict[\"dataset_name\"].lower() in [\"replica\"]:\n        return ReplicaDataset(config_dict, basedir, sequence, **kwargs)\n    elif config_dict[\"dataset_name\"].lower() in [\"azure\", \"azurekinect\"]:\n        return AzureKinectDataset(config_dict, basedir, sequence, **kwargs)\n    elif config_dict[\"dataset_name\"].lower() in [\"scannet\"]:\n        return ScannetDataset(config_dict, basedir, sequence, **kwargs)",
        "detail": "dataset.datasets_common",
        "documentation": {}
    },
    {
        "label": "ProgramArgs",
        "kind": 6,
        "importPath": "dataset.preprocess_r3d_file",
        "description": "dataset.preprocess_r3d_file",
        "peekOfCode": "class ProgramArgs:\n    datapath: str = \"/home/krishna/data/record3d/krishna-bcs-room\"\ndef load_depth(filepath):\n    with open(filepath, 'rb') as depth_fh:\n        raw_bytes = depth_fh.read()\n        decompressed_bytes = liblzfse.decompress(raw_bytes)\n        depth_img = np.frombuffer(decompressed_bytes, dtype=np.float32)\n    # depth_img = depth_img.reshape((640, 480))  # For a FaceID camera 3D Video\n    depth_img = depth_img.reshape((256, 192))  # For a LiDAR 3D Video\n    return depth_img",
        "detail": "dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "load_depth",
        "kind": 2,
        "importPath": "dataset.preprocess_r3d_file",
        "description": "dataset.preprocess_r3d_file",
        "peekOfCode": "def load_depth(filepath):\n    with open(filepath, 'rb') as depth_fh:\n        raw_bytes = depth_fh.read()\n        decompressed_bytes = liblzfse.decompress(raw_bytes)\n        depth_img = np.frombuffer(decompressed_bytes, dtype=np.float32)\n    # depth_img = depth_img.reshape((640, 480))  # For a FaceID camera 3D Video\n    depth_img = depth_img.reshape((256, 192))  # For a LiDAR 3D Video\n    return depth_img\ndef load_conf(filepath):\n    with open(filepath, 'rb') as conf_fh:",
        "detail": "dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "load_conf",
        "kind": 2,
        "importPath": "dataset.preprocess_r3d_file",
        "description": "dataset.preprocess_r3d_file",
        "peekOfCode": "def load_conf(filepath):\n    with open(filepath, 'rb') as conf_fh:\n        raw_bytes = conf_fh.read()\n        decompressed_bytes = liblzfse.decompress(raw_bytes)\n        conf_img = np.frombuffer(decompressed_bytes, dtype=np.uint8)\n    # depth_img = depth_img.reshape((640, 480))  # For a FaceID camera 3D Video\n    conf_img = conf_img.reshape((256, 192))  # For a LiDAR 3D Video\n    return conf_img\ndef load_color(filepath):\n    img = cv2.imread(filepath)",
        "detail": "dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "load_color",
        "kind": 2,
        "importPath": "dataset.preprocess_r3d_file",
        "description": "dataset.preprocess_r3d_file",
        "peekOfCode": "def load_color(filepath):\n    img = cv2.imread(filepath)\n    return cv2.resize(img, (192, 256))\ndef write_color(outpath, img):\n    cv2.imwrite(outpath, img)\ndef write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)",
        "detail": "dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "write_color",
        "kind": 2,
        "importPath": "dataset.preprocess_r3d_file",
        "description": "dataset.preprocess_r3d_file",
        "peekOfCode": "def write_color(outpath, img):\n    cv2.imwrite(outpath, img)\ndef write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)\ndef write_conf(outpath, conf):\n    np.save(outpath, conf)\ndef write_pose(outpath, pose):",
        "detail": "dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "write_depth",
        "kind": 2,
        "importPath": "dataset.preprocess_r3d_file",
        "description": "dataset.preprocess_r3d_file",
        "peekOfCode": "def write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)\ndef write_conf(outpath, conf):\n    np.save(outpath, conf)\ndef write_pose(outpath, pose):\n    np.save(outpath, pose.astype(np.float32))\ndef get_poses(metadata_dict: dict) -> int:",
        "detail": "dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "write_conf",
        "kind": 2,
        "importPath": "dataset.preprocess_r3d_file",
        "description": "dataset.preprocess_r3d_file",
        "peekOfCode": "def write_conf(outpath, conf):\n    np.save(outpath, conf)\ndef write_pose(outpath, pose):\n    np.save(outpath, pose.astype(np.float32))\ndef get_poses(metadata_dict: dict) -> int:\n    \"\"\"Converts Record3D's metadata dict into pose matrices needed by nerfstudio\n    Args:\n        metadata_dict: Dict containing Record3D metadata\n    Returns:\n        np.array of pose matrices for each image of shape: (num_images, 4, 4)",
        "detail": "dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "write_pose",
        "kind": 2,
        "importPath": "dataset.preprocess_r3d_file",
        "description": "dataset.preprocess_r3d_file",
        "peekOfCode": "def write_pose(outpath, pose):\n    np.save(outpath, pose.astype(np.float32))\ndef get_poses(metadata_dict: dict) -> int:\n    \"\"\"Converts Record3D's metadata dict into pose matrices needed by nerfstudio\n    Args:\n        metadata_dict: Dict containing Record3D metadata\n    Returns:\n        np.array of pose matrices for each image of shape: (num_images, 4, 4)\n    \"\"\"\n    poses_data = np.array(metadata_dict[\"poses\"])  # (N, 3, 4)",
        "detail": "dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "get_poses",
        "kind": 2,
        "importPath": "dataset.preprocess_r3d_file",
        "description": "dataset.preprocess_r3d_file",
        "peekOfCode": "def get_poses(metadata_dict: dict) -> int:\n    \"\"\"Converts Record3D's metadata dict into pose matrices needed by nerfstudio\n    Args:\n        metadata_dict: Dict containing Record3D metadata\n    Returns:\n        np.array of pose matrices for each image of shape: (num_images, 4, 4)\n    \"\"\"\n    poses_data = np.array(metadata_dict[\"poses\"])  # (N, 3, 4)\n    # NB: Record3D / scipy use \"scalar-last\" format quaternions (x y z w)\n    # https://fzheng.me/2017/11/12/quaternion_conventions_en/",
        "detail": "dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "get_intrinsics",
        "kind": 2,
        "importPath": "dataset.preprocess_r3d_file",
        "description": "dataset.preprocess_r3d_file",
        "peekOfCode": "def get_intrinsics(metadata_dict: dict, downscale_factor: float = 7.5) -> int:\n    \"\"\"Converts Record3D metadata dict into intrinsic info needed by nerfstudio\n    Args:\n        metadata_dict: Dict containing Record3D metadata\n        downscale_factor: factor to scale RGB image by (usually scale factor is\n            set to 7.5 for record3d 1.8 or higher -- this is the factor that downscales\n            RGB images to lidar)\n    Returns:\n        dict with camera intrinsics keys needed by nerfstudio\n    \"\"\"",
        "detail": "dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "dataset.preprocess_r3d_file",
        "description": "dataset.preprocess_r3d_file",
        "peekOfCode": "def main():\n    args = tyro.cli(ProgramArgs)\n    metadata = None\n    with open(os.path.join(args.datapath, \"metadata\"), \"r\") as f:\n        metadata = json.load(f)\n    # Keys in metadata dict\n    # h, w, K, fps, dw, dh, initPose, poses, cameraType, frameTimestamps\n    # print(metadata.keys())\n    poses = get_poses(metadata)\n    intrinsics_dict = get_intrinsics(metadata)",
        "detail": "dataset.preprocess_r3d_file",
        "documentation": {}
    },
    {
        "label": "REPLICA_CLASSES",
        "kind": 5,
        "importPath": "dataset.replica_constants",
        "description": "dataset.replica_constants",
        "peekOfCode": "REPLICA_CLASSES = [\n    \"other\",\n    \"backpack\",\n    \"base-cabinet\",\n    \"basket\",\n    \"bathtub\",\n    \"beam\",\n    \"beanbag\",\n    \"bed\",\n    \"bench\",",
        "detail": "dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_SCENE_IDS",
        "kind": 5,
        "importPath": "dataset.replica_constants",
        "description": "dataset.replica_constants",
        "peekOfCode": "REPLICA_SCENE_IDS = [\n    \"room0\",\n    \"room1\",\n    \"room2\",\n    \"office0\",\n    \"office1\",\n    \"office2\",\n    \"office3\",\n    \"office4\",\n]",
        "detail": "dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_SCENE_IDS_",
        "kind": 5,
        "importPath": "dataset.replica_constants",
        "description": "dataset.replica_constants",
        "peekOfCode": "REPLICA_SCENE_IDS_ = [\n    \"room_0\",\n    \"room_1\",\n    \"room_2\",\n    \"office_0\",\n    \"office_1\",\n    \"office_2\",\n    \"office_3\",\n    \"office_4\",\n]",
        "detail": "dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "REPLICA_EXISTING_CLASSES",
        "kind": 5,
        "importPath": "dataset.replica_constants",
        "description": "dataset.replica_constants",
        "peekOfCode": "REPLICA_EXISTING_CLASSES = [\n    0,\n    3,\n    7,\n    8,\n    10,\n    11,\n    12,\n    13,\n    14,",
        "detail": "dataset.replica_constants",
        "documentation": {}
    },
    {
        "label": "DemoApp",
        "kind": 6,
        "importPath": "dataset.save_record3d",
        "description": "dataset.save_record3d",
        "peekOfCode": "class DemoApp:\n    def __init__(self, savedir_name=\"saved-record3d\", seq_name=\"debug\"):\n        self.event = Event()\n        self.session = None\n        self.DEVICE_TYPE__TRUEDEPTH = 0\n        self.DEVICE_TYPE__LIDAR = 1\n        self.imgs = []\n        self.depths = []\n        self.intrinsics = None\n        self.poses = []",
        "detail": "dataset.save_record3d",
        "documentation": {}
    },
    {
        "label": "write_color",
        "kind": 2,
        "importPath": "dataset.save_record3d",
        "description": "dataset.save_record3d",
        "peekOfCode": "def write_color(outpath, img):\n    cv2.imwrite(outpath, img)\ndef write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)\ndef write_pose(outpath, pose):\n    # quat_trans: Quaternion + world position (accessible via camera_pose.[qx|qy|qz|qw|tx|ty|tz])\n    # NB: Record3D / scipy use \"scalar-last\" format quaternions (x y z w)",
        "detail": "dataset.save_record3d",
        "documentation": {}
    },
    {
        "label": "write_depth",
        "kind": 2,
        "importPath": "dataset.save_record3d",
        "description": "dataset.save_record3d",
        "peekOfCode": "def write_depth(outpath, depth):\n    depth = depth * 1000\n    depth = depth.astype(np.uint16)\n    depth = Image.fromarray(depth)\n    depth.save(outpath)\ndef write_pose(outpath, pose):\n    # quat_trans: Quaternion + world position (accessible via camera_pose.[qx|qy|qz|qw|tx|ty|tz])\n    # NB: Record3D / scipy use \"scalar-last\" format quaternions (x y z w)\n    # https://fzheng.me/2017/11/12/quaternion_conventions_en/\n    # pose = np.asarray([pose.qx, pose.qy, pose.qz, pose.qw, pose.tx, pose.ty, pose.tz])",
        "detail": "dataset.save_record3d",
        "documentation": {}
    },
    {
        "label": "write_pose",
        "kind": 2,
        "importPath": "dataset.save_record3d",
        "description": "dataset.save_record3d",
        "peekOfCode": "def write_pose(outpath, pose):\n    # quat_trans: Quaternion + world position (accessible via camera_pose.[qx|qy|qz|qw|tx|ty|tz])\n    # NB: Record3D / scipy use \"scalar-last\" format quaternions (x y z w)\n    # https://fzheng.me/2017/11/12/quaternion_conventions_en/\n    # pose = np.asarray([pose.qx, pose.qy, pose.qz, pose.qw, pose.tx, pose.ty, pose.tz])\n    c2w = np.zeros((4, 4))\n    c2w[3, 3] = 1.0\n    c2w[:3, :3] = Rotation.from_quat(pose[:4]).as_matrix()\n    c2w[:3, 3] = pose[4:]\n    np.save(outpath, c2w.astype(np.float32))",
        "detail": "dataset.save_record3d",
        "documentation": {}
    },
    {
        "label": "LLaVaNeXTChat",
        "kind": 6,
        "importPath": "llava.llava_model",
        "description": "llava.llava_model",
        "peekOfCode": "class LLaVaNeXTChat:\n    def __init__(\n        self,\n        model,\n        image_processor,\n        tokenizer,\n        max_length,\n        conv_template=\"llava_llama3\",\n    ):\n        self.model = model",
        "detail": "llava.llava_model",
        "documentation": {}
    },
    {
        "label": "LLAVA_PYTHON_PATH",
        "kind": 5,
        "importPath": "llava.llava_model",
        "description": "llava.llava_model",
        "peekOfCode": "LLAVA_PYTHON_PATH = \"/home/kev/packages/LLaVA-NeXT\"\nsys.path.append(LLAVA_PYTHON_PATH)\nimport llava.conversation\nwarnings.filterwarnings(\"ignore\")\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import (\n    get_model_name_from_path,\n    process_images,\n    tokenizer_image_token,\n)",
        "detail": "llava.llava_model",
        "documentation": {}
    },
    {
        "label": "LlavaConfig",
        "kind": 6,
        "importPath": "llava.llava_model_original",
        "description": "llava.llava_model_original",
        "peekOfCode": "class LlavaConfig(LlamaConfig):\n    model_type = \"llava\"\nclass LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        # Instantiate the multimodal vision tower\n        if hasattr(config, \"mm_vision_tower\"):\n            # HACK: for FSDP (fully-sharded data parallel); cast to a list\n            self.vision_tower = [",
        "detail": "llava.llava_model_original",
        "documentation": {}
    },
    {
        "label": "LlavaLlamaModelTweaked",
        "kind": 6,
        "importPath": "llava.llava_model_original",
        "description": "llava.llava_model_original",
        "peekOfCode": "class LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        # Instantiate the multimodal vision tower\n        if hasattr(config, \"mm_vision_tower\"):\n            # HACK: for FSDP (fully-sharded data parallel); cast to a list\n            self.vision_tower = [\n                CLIPVisionModel.from_pretrained(config.mm_vision_tower)\n            ]",
        "detail": "llava.llava_model_original",
        "documentation": {}
    },
    {
        "label": "LlavaLlamaForCausalLMTweaked",
        "kind": 6,
        "importPath": "llava.llava_model_original",
        "description": "llava.llava_model_original",
        "peekOfCode": "class LlavaLlamaForCausalLMTweaked(LlamaForCausalLM):\n    config_class = LlavaConfig\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlavaLlamaModelTweaked(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n    def get_model(self):\n        return self.model",
        "detail": "llava.llava_model_original",
        "documentation": {}
    },
    {
        "label": "LLaVaChat",
        "kind": 6,
        "importPath": "llava.llava_model_original",
        "description": "llava.llava_model_original",
        "peekOfCode": "class LLaVaChat(object):\n    def __init__(self, model_path, conv_mode=\"multimodal\", num_gpus=1):\n        self.model_path = model_path\n        self.conv_mode = conv_mode\n        self.num_gpus = num_gpus\n        # Handle multi-gpu config\n        if self.num_gpus == 1:\n            kwargs = {}\n        else:\n            kwargs = {",
        "detail": "llava.llava_model_original",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_TOKEN",
        "kind": 5,
        "importPath": "llava.llava_model_original",
        "description": "llava.llava_model_original",
        "peekOfCode": "DEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava\"\nclass LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)",
        "detail": "llava.llava_model_original",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_PATCH_TOKEN",
        "kind": 5,
        "importPath": "llava.llava_model_original",
        "description": "llava.llava_model_original",
        "peekOfCode": "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava\"\nclass LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        # Instantiate the multimodal vision tower",
        "detail": "llava.llava_model_original",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IM_START_TOKEN",
        "kind": 5,
        "importPath": "llava.llava_model_original",
        "description": "llava.llava_model_original",
        "peekOfCode": "DEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava\"\nclass LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        # Instantiate the multimodal vision tower\n        if hasattr(config, \"mm_vision_tower\"):",
        "detail": "llava.llava_model_original",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IM_END_TOKEN",
        "kind": 5,
        "importPath": "llava.llava_model_original",
        "description": "llava.llava_model_original",
        "peekOfCode": "DEFAULT_IM_END_TOKEN = \"<im_end>\"\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava\"\nclass LlavaLlamaModelTweaked(LlamaModel):\n    config_class = LlavaConfig\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        # Instantiate the multimodal vision tower\n        if hasattr(config, \"mm_vision_tower\"):\n            # HACK: for FSDP (fully-sharded data parallel); cast to a list",
        "detail": "llava.llava_model_original",
        "documentation": {}
    },
    {
        "label": "GPTPrompt",
        "kind": 6,
        "importPath": "scenegraph.GPTPrompt",
        "description": "scenegraph.GPTPrompt",
        "peekOfCode": "class GPTPrompt:\n    def __init__(self):\n        self.old_system_prompt = \"\"\"\n        The input is a list of JSONs describing multiple predictions of a single object. Each JSON has four fields: \n        1. id: a unique identifier for the object\n        2. bbox_extent: the 3D bounding box extents of the object \n        3. bbox_center: the 3D bounding box center of the object \n        4. caption: a caption predicted by an image captioning model referencing that object. This caption should be brief and in sparse prose. For example, the caption \"the object described appears to be described as a electric bicycle, it is sitting alongside a red suitcase which is nearby\" should be shortened to \"electric bike, near red suitcase\".\n        There may be upto 10 such bounding boxes and captions in each input.\n        The captions may not always be accurate or consistent (often, predictions may just be wrong). ",
        "detail": "scenegraph.GPTPrompt",
        "documentation": {}
    },
    {
        "label": "ProgramArgs",
        "kind": 6,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "class ProgramArgs:\n    mode: Literal[\n        \"extract-node-captions\",\n        \"refine-node-captions\",\n        \"build-scenegraph\",\n        \"generate-scenegraph-json\",\n        \"annotate-scenegraph\",\n    ]\n    # Path to cache directory\n    cachedir: str = \"saved/room0\"",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "load_scene_map",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def load_scene_map(args, scene_map):\n    \"\"\"\n    Loads a scene map from a gzip-compressed pickle file. This is a function because depending whether the mapfile was made using cfslam_pipeline_batch.py or merge_duplicate_objects.py, the file format is different (see below). So this function handles that case.\n    The function checks the structure of the deserialized object to determine\n    the correct way to load it into the `scene_map` object. There are two\n    expected formats:\n    1. A dictionary containing an \"objects\" key.\n    2. A list or a dictionary (replace with your expected type).\n    \"\"\"\n    with gzip.open(Path(args.mapfile), \"rb\") as f:",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "crop_image_pil",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def crop_image_pil(image: Image, x1: int, y1: int, x2: int, y2: int, padding: int = 0) -> Image:\n    \"\"\"\n    Crop the image with some padding\n    Args:\n        image: PIL image\n        x1, y1, x2, y2: bounding box coordinates\n        padding: padding around the bounding box\n    Returns:\n        image_crop: PIL image\n    Implementation from the CFSLAM repo",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "draw_red_outline",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def draw_red_outline(image, mask):\n    \"\"\" Draw a red outline around the object i nan image\"\"\"\n    # Convert PIL Image to numpy array\n    image_np = np.array(image)\n    red_outline = [255, 0, 0]\n    # Find contours in the binary mask\n    contours, _ = cv2.findContours(mask.astype(np.uint8) * 255, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    # Draw red outlines around the object. The last argument \"3\" indicates the thickness of the outline.\n    cv2.drawContours(image_np, contours, -1, red_outline, 3)\n    # Optionally, add padding around the object by dilating the drawn contours",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "crop_image_and_mask",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def crop_image_and_mask(image: Image, mask: np.ndarray, x1: int, y1: int, x2: int, y2: int, padding: int = 0):\n    \"\"\" Crop the image and mask with some padding. I made a single function that crops both the image and the mask at the same time because I was getting shape mismatches when I cropped them separately.This way I can check that they are the same shape.\"\"\"\n    image = np.array(image)\n    # Verify initial dimensions\n    if image.shape[:2] != mask.shape:\n        raise ValueError(\"Initial shape mismatch: Image shape {} != Mask shape {}\".format(image.shape, mask.shape))\n    # Define the cropping coordinates\n    x1 = max(0, x1 - padding)\n    y1 = max(0, y1 - padding)\n    x2 = min(image.shape[1], x2 + padding)",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "blackout_nonmasked_area",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def blackout_nonmasked_area(image_pil, mask):\n    \"\"\" Blackout the non-masked area of an image\"\"\"\n    # convert image to numpy array\n    image_np = np.array(image_pil)\n    # Create an all-black image of the same shape as the input image\n    black_image = np.zeros_like(image_np)\n    # Wherever the mask is True, replace the black image pixel with the original image pixel\n    black_image[mask] = image_np[mask]\n    # convert back to pil image\n    black_image = Image.fromarray(black_image)",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "plot_images_with_captions",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def plot_images_with_captions(images, captions, confidences, low_confidences, masks, savedir, idx_obj):\n    \"\"\" This is debug helper function that plots the images with the captions and masks overlaid and saves them to a directory. This way you can inspect exactly what the LLaVA model is captioning which image with the mask, and the mask confidence scores overlaid.\"\"\"\n    n = min(9, len(images))  # Only plot up to 9 images\n    nrows = int(np.ceil(n / 3))\n    ncols = 3 if n > 1 else 1\n    fig, axarr = plt.subplots(nrows, ncols, figsize=(10, 5 * nrows), squeeze=False)  # Adjusted figsize\n    for i in range(n):\n        row, col = divmod(i, 3)\n        ax = axarr[row][col]\n        ax.imshow(images[i])",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "extract_node_captions",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def extract_node_captions(args):\n    from conceptgraph.llava.llava_model import LLaVaNeXTChat\n    # NOTE: args.mapfile is in cfslam format\n    from conceptgraph.slam.slam_classes import MapObjectList\n    # Load the scene map\n    scene_map = MapObjectList()\n    load_scene_map(args, scene_map)\n    # Scene map is in CFSLAM format\n    # keys: 'image_idx', 'mask_idx', 'color_path', 'class_id', 'num_detections',\n    # 'mask', 'xyxy', 'conf', 'n_points', 'pixel_area', 'contain_number', 'clip_ft',",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "save_json_to_file",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def save_json_to_file(json_str, filename):\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        json.dump(json_str, f, indent=4, sort_keys=False)\ndef refine_node_captions(args):\n    # NOTE: args.mapfile is in cfslam format\n    from conceptgraph.slam.slam_classes import MapObjectList\n    from conceptgraph.scenegraph.GPTPrompt import GPTPrompt\n    # Load the captions for each segment\n    caption_file = Path(args.cachedir) / \"cfslam_llava_captions.json\"\n    captions = None",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "refine_node_captions",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def refine_node_captions(args):\n    # NOTE: args.mapfile is in cfslam format\n    from conceptgraph.slam.slam_classes import MapObjectList\n    from conceptgraph.scenegraph.GPTPrompt import GPTPrompt\n    # Load the captions for each segment\n    caption_file = Path(args.cachedir) / \"cfslam_llava_captions.json\"\n    captions = None\n    with open(caption_file, \"r\") as f:\n        captions = json.load(f)\n    # loaddir_captions = Path(args.cachedir) / \"cfslam_captions_llava\"",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "extract_object_tag_from_json_str",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def extract_object_tag_from_json_str(json_str):\n    start_str_found = False\n    is_object_tag = False\n    object_tag_complete = False\n    object_tag = \"\"\n    r = json_str.strip().split()\n    for _idx, _r in enumerate(r):\n        if not start_str_found:\n            # Searching for open parenthesis of JSON\n            if _r == \"{\":",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "build_scenegraph",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def build_scenegraph(args):\n    from conceptgraph.slam.slam_classes import MapObjectList\n    from conceptgraph.slam.utils import compute_overlap_matrix\n    # Load the scene map\n    scene_map = MapObjectList()\n    load_scene_map(args, scene_map)\n    response_dir = Path(args.cachedir) / \"cfslam_gpt-4_responses\"\n    responses = []\n    object_tags = []\n    also_indices_to_remove = [] # indices to remove if the json file does not exist",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "generate_scenegraph_json",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def generate_scenegraph_json(args):\n    from conceptgraph.slam.slam_classes import MapObjectList\n    # Generate the JSON file summarizing the scene, if it doesn't exist already\n    # or if the --recopmute_scenegraph_json flag is set\n    scene_desc = []\n    print(\"Generating scene graph JSON file...\")\n    # Load the pruned scene map\n    scene_map = MapObjectList()\n    with gzip.open(Path(args.cachedir) / \"map\" / \"scene_map_cfslam_pruned.pkl.gz\", \"rb\") as f:\n        scene_map.load_serializable(pkl.load(f))",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "display_images",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def display_images(image_list):\n    num_images = len(image_list)\n    cols = 2  # Number of columns for the subplots (you can change this as needed)\n    rows = (num_images + cols - 1) // cols\n    _, axes = plt.subplots(rows, cols, figsize=(10, 5))\n    for i, ax in enumerate(axes.flat):\n        if i < num_images:\n            img = image_list[i]\n            ax.imshow(img)\n            ax.axis(\"off\")",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "annotate_scenegraph",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def annotate_scenegraph(args):\n    from conceptgraph.slam.slam_classes import MapObjectList\n    # Load the pruned scene map\n    scene_map = MapObjectList()\n    with gzip.open(Path(args.cachedir) / \"map\" / \"scene_map_cfslam_pruned.pkl.gz\", \"rb\") as f:\n        scene_map.load_serializable(pkl.load(f))\n    annot_inds = None\n    if args.annot_inds is not None:\n        annot_inds = args.annot_inds\n    # If annot_inds is not None, we also need to load the annotation json file and only",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "def main():\n    # Process command-line args (if any)\n    args = tyro.cli(ProgramArgs)\n    # print using masking option\n    print(f\"args.masking_option: {args.masking_option}\")\n    if args.mode == \"extract-node-captions\":\n        extract_node_captions(args)\n    elif args.mode == \"refine-node-captions\":\n        refine_node_captions(args)\n    elif args.mode == \"build-scenegraph\":",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "LLAVA_PYTHON_PATH",
        "kind": 5,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "LLAVA_PYTHON_PATH = \"/home/kev/packages/LLaVA-NeXT\"\nLLAVA_CKPT_PATH = \"/home/kev/pretrained_models/llama3-llava-next-8b\"\nsys.path.append(LLAVA_PYTHON_PATH)\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import (\n    get_model_name_from_path,\n    process_images,\n    tokenizer_image_token,\n)\nfrom llava.constants import (",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "LLAVA_CKPT_PATH",
        "kind": 5,
        "importPath": "scenegraph.build_scenegraph_cfslam",
        "description": "scenegraph.build_scenegraph_cfslam",
        "peekOfCode": "LLAVA_CKPT_PATH = \"/home/kev/pretrained_models/llama3-llava-next-8b\"\nsys.path.append(LLAVA_PYTHON_PATH)\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import (\n    get_model_name_from_path,\n    process_images,\n    tokenizer_image_token,\n)\nfrom llava.constants import (\n    IMAGE_TOKEN_INDEX,",
        "detail": "scenegraph.build_scenegraph_cfslam",
        "documentation": {}
    },
    {
        "label": "ProgramArgs",
        "kind": 6,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "class ProgramArgs:\n    mode: Literal[\n        \"extract-node-captions\",\n        \"refine-node-captions\",\n        \"build-scenegraph\",\n        \"generate-scenegraph-json\",\n        \"annotate-scenegraph\",\n    ]\n    # Path to cache directory\n    cachedir: str = \"saved/room0\"",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "load_scene_map",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def load_scene_map(args, scene_map):\n    \"\"\"\n    Loads a scene map from a gzip-compressed pickle file. This is a function because depending whether the mapfile was made using cfslam_pipeline_batch.py or merge_duplicate_objects.py, the file format is different (see below). So this function handles that case.\n    The function checks the structure of the deserialized object to determine\n    the correct way to load it into the `scene_map` object. There are two\n    expected formats:\n    1. A dictionary containing an \"objects\" key.\n    2. A list or a dictionary (replace with your expected type).\n    \"\"\"\n    with gzip.open(Path(args.mapfile), \"rb\") as f:",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "crop_image_pil",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def crop_image_pil(image: Image, x1: int, y1: int, x2: int, y2: int, padding: int = 0) -> Image:\n    \"\"\"\n    Crop the image with some padding\n    Args:\n        image: PIL image\n        x1, y1, x2, y2: bounding box coordinates\n        padding: padding around the bounding box\n    Returns:\n        image_crop: PIL image\n    Implementation from the CFSLAM repo",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "draw_red_outline",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def draw_red_outline(image, mask):\n    \"\"\" Draw a red outline around the object i nan image\"\"\"\n    # Convert PIL Image to numpy array\n    image_np = np.array(image)\n    red_outline = [255, 0, 0]\n    # Find contours in the binary mask\n    contours, _ = cv2.findContours(mask.astype(np.uint8) * 255, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    # Draw red outlines around the object. The last argument \"3\" indicates the thickness of the outline.\n    cv2.drawContours(image_np, contours, -1, red_outline, 3)\n    # Optionally, add padding around the object by dilating the drawn contours",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "crop_image_and_mask",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def crop_image_and_mask(image: Image, mask: np.ndarray, x1: int, y1: int, x2: int, y2: int, padding: int = 0):\n    \"\"\" Crop the image and mask with some padding. I made a single function that crops both the image and the mask at the same time because I was getting shape mismatches when I cropped them separately.This way I can check that they are the same shape.\"\"\"\n    image = np.array(image)\n    # Verify initial dimensions\n    if image.shape[:2] != mask.shape:\n        raise ValueError(\"Initial shape mismatch: Image shape {} != Mask shape {}\".format(image.shape, mask.shape))\n    # Define the cropping coordinates\n    x1 = max(0, x1 - padding)\n    y1 = max(0, y1 - padding)\n    x2 = min(image.shape[1], x2 + padding)",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "blackout_nonmasked_area",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def blackout_nonmasked_area(image_pil, mask):\n    \"\"\" Blackout the non-masked area of an image\"\"\"\n    # convert image to numpy array\n    image_np = np.array(image_pil)\n    # Create an all-black image of the same shape as the input image\n    black_image = np.zeros_like(image_np)\n    # Wherever the mask is True, replace the black image pixel with the original image pixel\n    black_image[mask] = image_np[mask]\n    # convert back to pil image\n    black_image = Image.fromarray(black_image)",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "plot_images_with_captions",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def plot_images_with_captions(images, captions, confidences, low_confidences, masks, savedir, idx_obj):\n    \"\"\" This is debug helper function that plots the images with the captions and masks overlaid and saves them to a directory. This way you can inspect exactly what the LLaVA model is captioning which image with the mask, and the mask confidence scores overlaid.\"\"\"\n    n = min(9, len(images))  # Only plot up to 9 images\n    nrows = int(np.ceil(n / 3))\n    ncols = 3 if n > 1 else 1\n    fig, axarr = plt.subplots(nrows, ncols, figsize=(10, 5 * nrows), squeeze=False)  # Adjusted figsize\n    for i in range(n):\n        row, col = divmod(i, 3)\n        ax = axarr[row][col]\n        ax.imshow(images[i])",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "extract_node_captions",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def extract_node_captions(args):\n    from conceptgraph.llava.llava_model import LLaVaChat\n    # NOTE: args.mapfile is in cfslam format\n    from conceptgraph.slam.slam_classes import MapObjectList\n    # Load the scene map\n    scene_map = MapObjectList()\n    load_scene_map(args, scene_map)\n    # Scene map is in CFSLAM format\n    # keys: 'image_idx', 'mask_idx', 'color_path', 'class_id', 'num_detections',\n    # 'mask', 'xyxy', 'conf', 'n_points', 'pixel_area', 'contain_number', 'clip_ft',",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "save_json_to_file",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def save_json_to_file(json_str, filename):\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        json.dump(json_str, f, indent=4, sort_keys=False)\ndef refine_node_captions(args):\n    # NOTE: args.mapfile is in cfslam format\n    from conceptgraph.slam.slam_classes import MapObjectList\n    from conceptgraph.scenegraph.GPTPrompt import GPTPrompt\n    # Load the captions for each segment\n    caption_file = Path(args.cachedir) / \"cfslam_llava_captions.json\"\n    captions = None",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "refine_node_captions",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def refine_node_captions(args):\n    # NOTE: args.mapfile is in cfslam format\n    from conceptgraph.slam.slam_classes import MapObjectList\n    from conceptgraph.scenegraph.GPTPrompt import GPTPrompt\n    # Load the captions for each segment\n    caption_file = Path(args.cachedir) / \"cfslam_llava_captions.json\"\n    captions = None\n    with open(caption_file, \"r\") as f:\n        captions = json.load(f)\n    # loaddir_captions = Path(args.cachedir) / \"cfslam_captions_llava\"",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "extract_object_tag_from_json_str",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def extract_object_tag_from_json_str(json_str):\n    start_str_found = False\n    is_object_tag = False\n    object_tag_complete = False\n    object_tag = \"\"\n    r = json_str.strip().split()\n    for _idx, _r in enumerate(r):\n        if not start_str_found:\n            # Searching for open parenthesis of JSON\n            if _r == \"{\":",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "build_scenegraph",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def build_scenegraph(args):\n    from conceptgraph.slam.slam_classes import MapObjectList\n    from conceptgraph.slam.utils import compute_overlap_matrix\n    # Load the scene map\n    scene_map = MapObjectList()\n    load_scene_map(args, scene_map)\n    response_dir = Path(args.cachedir) / \"cfslam_gpt-4_responses\"\n    responses = []\n    object_tags = []\n    also_indices_to_remove = [] # indices to remove if the json file does not exist",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "generate_scenegraph_json",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def generate_scenegraph_json(args):\n    from conceptgraph.slam.slam_classes import MapObjectList\n    # Generate the JSON file summarizing the scene, if it doesn't exist already\n    # or if the --recopmute_scenegraph_json flag is set\n    scene_desc = []\n    print(\"Generating scene graph JSON file...\")\n    # Load the pruned scene map\n    scene_map = MapObjectList()\n    with gzip.open(Path(args.cachedir) / \"map\" / \"scene_map_cfslam_pruned.pkl.gz\", \"rb\") as f:\n        scene_map.load_serializable(pkl.load(f))",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "display_images",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def display_images(image_list):\n    num_images = len(image_list)\n    cols = 2  # Number of columns for the subplots (you can change this as needed)\n    rows = (num_images + cols - 1) // cols\n    _, axes = plt.subplots(rows, cols, figsize=(10, 5))\n    for i, ax in enumerate(axes.flat):\n        if i < num_images:\n            img = image_list[i]\n            ax.imshow(img)\n            ax.axis(\"off\")",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "annotate_scenegraph",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def annotate_scenegraph(args):\n    from conceptgraph.slam.slam_classes import MapObjectList\n    # Load the pruned scene map\n    scene_map = MapObjectList()\n    with gzip.open(Path(args.cachedir) / \"map\" / \"scene_map_cfslam_pruned.pkl.gz\", \"rb\") as f:\n        scene_map.load_serializable(pkl.load(f))\n    annot_inds = None\n    if args.annot_inds is not None:\n        annot_inds = args.annot_inds\n    # If annot_inds is not None, we also need to load the annotation json file and only",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "def main():\n    # Process command-line args (if any)\n    args = tyro.cli(ProgramArgs)\n    # print using masking option\n    print(f\"args.masking_option: {args.masking_option}\")\n    if args.mode == \"extract-node-captions\":\n        extract_node_captions(args)\n    elif args.mode == \"refine-node-captions\":\n        refine_node_captions(args)\n    elif args.mode == \"build-scenegraph\":",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "openai.api_key",
        "kind": 5,
        "importPath": "scenegraph.build_scenegraph_cfslam_original",
        "description": "scenegraph.build_scenegraph_cfslam_original",
        "peekOfCode": "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n@dataclass\nclass ProgramArgs:\n    mode: Literal[\n        \"extract-node-captions\",\n        \"refine-node-captions\",\n        \"build-scenegraph\",\n        \"generate-scenegraph-json\",\n        \"annotate-scenegraph\",\n    ]",
        "detail": "scenegraph.build_scenegraph_cfslam_original",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "scenegraph.merge_duplicate_objects",
        "description": "scenegraph.merge_duplicate_objects",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--result_path\", type=str, required=True)\n    parser.add_argument(\"--savefile\", type=str, required=True)\n    # To inspect the results of merge_overlap_objects\n    # This is mainly to quickly try out different thresholds\n    parser.add_argument(\"--merge_overlap_thresh\", type=float, default=-1)\n    parser.add_argument(\"--merge_visual_sim_thresh\", type=float, default=-1)\n    parser.add_argument(\"--merge_text_sim_thresh\", type=float, default=-1)\n    parser.add_argument(\"--obj_min_points\", type=int, default=0)",
        "detail": "scenegraph.merge_duplicate_objects",
        "documentation": {}
    },
    {
        "label": "RGBDFrame",
        "kind": 6,
        "importPath": "scripts.scannet_process.SensorData",
        "description": "scripts.scannet_process.SensorData",
        "peekOfCode": "class RGBDFrame():\n  def load(self, file_handle):\n    self.camera_to_world = np.asarray(struct.unpack('f'*16, file_handle.read(16*4)), dtype=np.float32).reshape(4, 4)\n    self.timestamp_color = struct.unpack('Q', file_handle.read(8))[0]\n    self.timestamp_depth = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.depth_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_data = b''.join(struct.unpack('c'*self.color_size_bytes, file_handle.read(self.color_size_bytes)))\n    self.depth_data = b''.join(struct.unpack('c'*self.depth_size_bytes, file_handle.read(self.depth_size_bytes)))\n  def decompress_depth(self, compression_type):",
        "detail": "scripts.scannet_process.SensorData",
        "documentation": {}
    },
    {
        "label": "SensorData",
        "kind": 6,
        "importPath": "scripts.scannet_process.SensorData",
        "description": "scripts.scannet_process.SensorData",
        "peekOfCode": "class SensorData:\n  def __init__(self, filename):\n    self.version = 4\n    self.load(filename)\n  def load(self, filename):\n    with open(filename, 'rb') as f:\n      version = struct.unpack('I', f.read(4))[0]\n      assert self.version == version\n      strlen = struct.unpack('Q', f.read(8))[0]\n      self.sensor_name = b''.join(struct.unpack('c'*strlen, f.read(strlen)))",
        "detail": "scripts.scannet_process.SensorData",
        "documentation": {}
    },
    {
        "label": "COMPRESSION_TYPE_COLOR",
        "kind": 5,
        "importPath": "scripts.scannet_process.SensorData",
        "description": "scripts.scannet_process.SensorData",
        "peekOfCode": "COMPRESSION_TYPE_COLOR = {-1:'unknown', 0:'raw', 1:'png', 2:'jpeg'}\nCOMPRESSION_TYPE_DEPTH = {-1:'unknown', 0:'raw_ushort', 1:'zlib_ushort', 2:'occi_ushort'}\nclass RGBDFrame():\n  def load(self, file_handle):\n    self.camera_to_world = np.asarray(struct.unpack('f'*16, file_handle.read(16*4)), dtype=np.float32).reshape(4, 4)\n    self.timestamp_color = struct.unpack('Q', file_handle.read(8))[0]\n    self.timestamp_depth = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.depth_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_data = b''.join(struct.unpack('c'*self.color_size_bytes, file_handle.read(self.color_size_bytes)))",
        "detail": "scripts.scannet_process.SensorData",
        "documentation": {}
    },
    {
        "label": "COMPRESSION_TYPE_DEPTH",
        "kind": 5,
        "importPath": "scripts.scannet_process.SensorData",
        "description": "scripts.scannet_process.SensorData",
        "peekOfCode": "COMPRESSION_TYPE_DEPTH = {-1:'unknown', 0:'raw_ushort', 1:'zlib_ushort', 2:'occi_ushort'}\nclass RGBDFrame():\n  def load(self, file_handle):\n    self.camera_to_world = np.asarray(struct.unpack('f'*16, file_handle.read(16*4)), dtype=np.float32).reshape(4, 4)\n    self.timestamp_color = struct.unpack('Q', file_handle.read(8))[0]\n    self.timestamp_depth = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.depth_size_bytes = struct.unpack('Q', file_handle.read(8))[0]\n    self.color_data = b''.join(struct.unpack('c'*self.color_size_bytes, file_handle.read(self.color_size_bytes)))\n    self.depth_data = b''.join(struct.unpack('c'*self.depth_size_bytes, file_handle.read(self.depth_size_bytes)))",
        "detail": "scripts.scannet_process.SensorData",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.scannet_process.reader",
        "description": "scripts.scannet_process.reader",
        "peekOfCode": "def main():\n  if not os.path.exists(opt.output_path):\n    os.makedirs(opt.output_path)\n  # load the data\n  sys.stdout.write('loading %s...' % opt.filename)\n  sd = SensorData(opt.filename)\n  sys.stdout.write('loaded!\\n')\n  if opt.export_depth_images:\n    sd.export_depth_images(os.path.join(opt.output_path, 'depth'))\n  if opt.export_color_images:",
        "detail": "scripts.scannet_process.reader",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "scripts.scannet_process.reader",
        "description": "scripts.scannet_process.reader",
        "peekOfCode": "parser = argparse.ArgumentParser()\n# data paths\nparser.add_argument('--filename', required=True, help='path to sens file to read')\nparser.add_argument('--output_path', required=True, help='path to output folder')\nparser.add_argument('--export_depth_images', dest='export_depth_images', action='store_true')\nparser.add_argument('--export_color_images', dest='export_color_images', action='store_true')\nparser.add_argument('--export_poses', dest='export_poses', action='store_true')\nparser.add_argument('--export_intrinsics', dest='export_intrinsics', action='store_true')\nparser.set_defaults(export_depth_images=False, export_color_images=False, export_poses=False, export_intrinsics=False)\nopt = parser.parse_args()",
        "detail": "scripts.scannet_process.reader",
        "documentation": {}
    },
    {
        "label": "opt",
        "kind": 5,
        "importPath": "scripts.scannet_process.reader",
        "description": "scripts.scannet_process.reader",
        "peekOfCode": "opt = parser.parse_args()\nprint(opt)\ndef main():\n  if not os.path.exists(opt.output_path):\n    os.makedirs(opt.output_path)\n  # load the data\n  sys.stdout.write('loading %s...' % opt.filename)\n  sd = SensorData(opt.filename)\n  sys.stdout.write('loaded!\\n')\n  if opt.export_depth_images:",
        "detail": "scripts.scannet_process.reader",
        "documentation": {}
    },
    {
        "label": "COLOR_MODE",
        "kind": 6,
        "importPath": "scripts.animate_mapping_interactive",
        "description": "scripts.animate_mapping_interactive",
        "peekOfCode": "class COLOR_MODE(Enum):\n    RGB = 0\n    CLASS = 1\n    INSTANCE = 2\ndef create_ball_mesh(center, radius, color=(0, 1, 0)):\n    \"\"\"\n    Create a colored mesh sphere.\n    Args:\n    - center (tuple): (x, y, z) coordinates for the center of the sphere.\n    - radius (float): Radius of the sphere.",
        "detail": "scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "create_ball_mesh",
        "kind": 2,
        "importPath": "scripts.animate_mapping_interactive",
        "description": "scripts.animate_mapping_interactive",
        "peekOfCode": "def create_ball_mesh(center, radius, color=(0, 1, 0)):\n    \"\"\"\n    Create a colored mesh sphere.\n    Args:\n    - center (tuple): (x, y, z) coordinates for the center of the sphere.\n    - radius (float): Radius of the sphere.\n    - color (tuple): RGB values in the range [0, 1] for the color of the sphere.\n    Returns:\n    - o3d.geometry.TriangleMesh: Colored mesh sphere.\n    \"\"\"",
        "detail": "scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "scripts.animate_mapping_interactive",
        "description": "scripts.animate_mapping_interactive",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser(description=\"Visualize a series of point clouds as an animation.\")\n    parser.add_argument(\"--input_folder\", type=str, required=True, help=\"Folder where the objects of the mapping process are stored.\")\n    parser.add_argument(\"--meta_folder\", type=str, default=None, help=\"Folder where the meta information is stored. Default the same as input_folder.\")\n    parser.add_argument(\"--edge_file\", type=str, default=None, help=\"Path to the scene graph relationship json file. If not provided, the edges will not be shown.\")\n    parser.add_argument(\"--sleep_time\", type=float, default=0.1, help=\"Time to sleep between each frame.\")\n    parser.add_argument(\"--follow_cam\", action=\"store_true\", help=\"If set, follow the camera pose.\")\n    parser.add_argument(\"--use_original_color\", action=\"store_true\", help=\"If set, will use color scheme from the CFSLAM pipeline.\")\n    parser.add_argument(\"--height_cutoff\", type=float, default=np.inf, help=\"Object nodes above this height will not be shown.\")\n    return parser",
        "detail": "scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "load_frame",
        "kind": 2,
        "importPath": "scripts.animate_mapping_interactive",
        "description": "scripts.animate_mapping_interactive",
        "peekOfCode": "def load_frame(path):\n    global cached_frame\n    if cached_frame is None:\n        with gzip.open(path, \"rb\") as f:\n            frame = pickle.load(f)\n    else:\n        frame = cached_frame\n    if isinstance(frame, dict):\n        camera_pose = frame[\"camera_pose\"]\n        objects = MapObjectList()",
        "detail": "scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.animate_mapping_interactive",
        "description": "scripts.animate_mapping_interactive",
        "peekOfCode": "def main(args):\n    # Load metadata\n    if args.meta_folder is None:\n        args.meta_folder = args.input_folder\n    meta_path = os.path.join(args.meta_folder, \"meta.pkl.gz\")\n    with gzip.open(meta_path, \"rb\") as f:\n        meta_info = pickle.load(f)\n    cfg = meta_info[\"cfg\"]\n    class_names = meta_info[\"class_names\"]\n    main.class_colors = meta_info[\"class_colors\"]",
        "detail": "scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "cached_frame",
        "kind": 5,
        "importPath": "scripts.animate_mapping_interactive",
        "description": "scripts.animate_mapping_interactive",
        "peekOfCode": "cached_frame = None\ndef load_frame(path):\n    global cached_frame\n    if cached_frame is None:\n        with gzip.open(path, \"rb\") as f:\n            frame = pickle.load(f)\n    else:\n        frame = cached_frame\n    if isinstance(frame, dict):\n        camera_pose = frame[\"camera_pose\"]",
        "detail": "scripts.animate_mapping_interactive",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "scripts.animate_mapping_save",
        "description": "scripts.animate_mapping_save",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser(description=\"Visualize a series of point clouds as an animation.\")\n    parser.add_argument(\"--input_folder\", type=str, help=\"Folder where the objects of the mapping process are stored.\")\n    return parser\ndef main(args):\n    meta_path = os.path.join(args.input_folder, \"meta.pkl.gz\")\n    frame_paths = glob.glob(os.path.join(args.input_folder, \"*.pkl.gz\"))\n    frame_paths = [path for path in frame_paths if path != meta_path]\n    frame_paths = natsort.natsorted(frame_paths)\n    with gzip.open(meta_path, \"rb\") as f:",
        "detail": "scripts.animate_mapping_save",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.animate_mapping_save",
        "description": "scripts.animate_mapping_save",
        "peekOfCode": "def main(args):\n    meta_path = os.path.join(args.input_folder, \"meta.pkl.gz\")\n    frame_paths = glob.glob(os.path.join(args.input_folder, \"*.pkl.gz\"))\n    frame_paths = [path for path in frame_paths if path != meta_path]\n    frame_paths = natsort.natsorted(frame_paths)\n    with gzip.open(meta_path, \"rb\") as f:\n        meta_info = pickle.load(f)\n    cfg = meta_info[\"cfg\"]\n    class_names = meta_info[\"class_names\"]\n    class_colors = meta_info[\"class_colors\"]",
        "detail": "scripts.animate_mapping_save",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "scripts.eval_replica_semseg",
        "description": "scripts.eval_replica_semseg",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--replica_root\", type=Path, default=Path(\"~/rdata/Replica/\").expanduser()\n    )\n    parser.add_argument(\n        \"--replica_semantic_root\",\n        type=Path,\n        default=Path(\"~/rdata/Replica-semantic/\").expanduser()\n    )",
        "detail": "scripts.eval_replica_semseg",
        "documentation": {}
    },
    {
        "label": "eval_replica",
        "kind": 2,
        "importPath": "scripts.eval_replica_semseg",
        "description": "scripts.eval_replica_semseg",
        "peekOfCode": "def eval_replica(\n    scene_id: str,\n    scene_id_: str,\n    class_names: list[str],\n    class_feats: torch.Tensor,\n    args: argparse.Namespace,\n    class_all2existing: torch.Tensor,\n    ignore_index=[],\n    gt_class_only: bool = True, # only compute the conf matrix for the GT classes\n):",
        "detail": "scripts.eval_replica_semseg",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.eval_replica_semseg",
        "description": "scripts.eval_replica_semseg",
        "peekOfCode": "def main(args: argparse.Namespace):\n    # map REPLICA_CLASSES to REPLICA_EXISTING_CLASSES\n    class_all2existing = torch.ones(len(REPLICA_CLASSES)).long() * -1\n    for i, c in enumerate(REPLICA_EXISTING_CLASSES):\n        class_all2existing[c] = i\n    class_names = [REPLICA_CLASSES[i] for i in REPLICA_EXISTING_CLASSES]\n    if args.n_exclude == 1:\n        exclude_class = [class_names.index(c) for c in [\n            \"other\"\n        ]]",
        "detail": "scripts.eval_replica_semseg",
        "documentation": {}
    },
    {
        "label": "generate_obs_from_poses",
        "kind": 2,
        "importPath": "scripts.generate_ai2thor_dataset",
        "description": "scripts.generate_ai2thor_dataset",
        "peekOfCode": "def generate_obs_from_poses(\n    controller,\n    K,\n    sampled_poses,\n    save_root,\n    depth_scale=1000.0,\n):\n    color_path_temp = save_root + \"/color/{:06d}.png\"\n    depth_path_temp = save_root + \"/depth/{:06d}.png\"\n    instance_path_temp = save_root + \"/instance/{:06d}.png\"",
        "detail": "scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "sample_pose_from_file",
        "kind": 2,
        "importPath": "scripts.generate_ai2thor_dataset",
        "description": "scripts.generate_ai2thor_dataset",
        "peekOfCode": "def sample_pose_from_file(traj_file):\n    # Load the trajectory file (json)\n    with open(traj_file, \"r\") as f:\n        traj = json.load(f)\n    sampled_poses = []\n    for log in traj[\"agent_logs\"]:\n        sampled_poses.append(\n            {\n                \"position\": log[\"position\"],\n                \"rotation\": log[\"rotation\"],",
        "detail": "scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "is_removeable",
        "kind": 2,
        "importPath": "scripts.generate_ai2thor_dataset",
        "description": "scripts.generate_ai2thor_dataset",
        "peekOfCode": "def is_removeable(obj, level: int):\n    if level == 1: # all objects except those in NOT_TO_REMOVE\n        return obj['objectType'] not in NOT_TO_REMOVE\n    elif level == 2: # objects that are pickupable or moveable\n        return obj['pickupable'] or obj['moveable']\n    elif level == 3: # objects that are pickupable\n        return obj['pickupable']\ndef randomize_scene(args, controller) -> list[str]|None:\n    '''\n    Since we want to keep track of which objects are removed, but it is not done in ai2thor",
        "detail": "scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "randomize_scene",
        "kind": 2,
        "importPath": "scripts.generate_ai2thor_dataset",
        "description": "scripts.generate_ai2thor_dataset",
        "peekOfCode": "def randomize_scene(args, controller) -> list[str]|None:\n    '''\n    Since we want to keep track of which objects are removed, but it is not done in ai2thor\n    So we will keep of a list of object ids that are kept in the scene. \n    if no object is removed from the scene, then return None.\n    '''\n    if args.randomize_lighting:\n        controller.step(\n            action=\"RandomizeLighting\",\n            brightness=(0.5, 1.5),",
        "detail": "scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "randomize_scene_from_log",
        "kind": 2,
        "importPath": "scripts.generate_ai2thor_dataset",
        "description": "scripts.generate_ai2thor_dataset",
        "peekOfCode": "def randomize_scene_from_log(controller, randomization_log):\n    if randomization_log['randomize_lighting']:\n        warnings.warn(\"randomize_lighting from log file is not implemented yet\")\n    if randomization_log['randomize_material']:\n        warnings.warn(\"randomize_material from log file is not implemented yet\")\n    # Remove some objects\n    removed_object_ids = randomization_log['removed_object_ids']\n    if len(removed_object_ids) > 0:\n        for obj_id in removed_object_ids:\n            event = controller.step(",
        "detail": "scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "load_or_randomize_scene",
        "kind": 2,
        "importPath": "scripts.generate_ai2thor_dataset",
        "description": "scripts.generate_ai2thor_dataset",
        "peekOfCode": "def load_or_randomize_scene(args, controller):\n    randomization_file_path = args.save_root + \"/randomization.json\"\n    if os.path.exists(randomization_file_path):\n        with open(randomization_file_path, \"r\") as f:\n            randomization_log = json.load(f)\n        randomize_scene_from_log(controller, randomization_log)\n        print(\"Loaded Randomization from {}\".format(randomization_file_path))\n    else:\n        randomization_log = randomize_scene(args, controller)\n        with open(randomization_file_path, \"w\") as f:",
        "detail": "scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.generate_ai2thor_dataset",
        "description": "scripts.generate_ai2thor_dataset",
        "peekOfCode": "def main(args: argparse.Namespace):\n    save_folder_name = (\n        args.scene_name\n        if args.save_suffix is None\n        else args.scene_name + \"_\" + args.save_suffix\n    )\n    save_root = args.dataset_root + \"/\" + save_folder_name + \"/\"\n    os.makedirs(save_root, exist_ok=True)\n    args.save_folder_name = save_folder_name\n    args.save_root = save_root",
        "detail": "scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "main_interact",
        "kind": 2,
        "importPath": "scripts.generate_ai2thor_dataset",
        "description": "scripts.generate_ai2thor_dataset",
        "peekOfCode": "def main_interact(args: argparse.Namespace):\n    '''\n    Interact with the AI2Thor simulator, navigating the robot. \n    The agent trajectory will be saved to a file as a file. \n    Note that this saves the agent pose but not the camera pose. \n    '''\n    save_folder_name = (\n        args.scene_name + \"_interact\"\n        if args.save_suffix is None\n        else args.scene_name + \"_\" + args.save_suffix",
        "detail": "scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "scripts.generate_ai2thor_dataset",
        "description": "scripts.generate_ai2thor_dataset",
        "peekOfCode": "def get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser(description=\"Program Arguments\")\n    parser.add_argument(\n        \"--dataset_root\",\n        default=str(Path(\"~/ldata/ai2thor/\").expanduser()),\n        help=\"The root path to the dataset.\",\n    )\n    parser.add_argument(\n        \"--grid_size\",\n        default=0.5,",
        "detail": "scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "NOT_TO_REMOVE",
        "kind": 5,
        "importPath": "scripts.generate_ai2thor_dataset",
        "description": "scripts.generate_ai2thor_dataset",
        "peekOfCode": "NOT_TO_REMOVE = [\n    \"Wall\",\n    \"Floor\",\n    \"Window\",\n    \"Doorway\",\n    \"Room\",\n]\ndef generate_obs_from_poses(\n    controller,\n    K,",
        "detail": "scripts.generate_ai2thor_dataset",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "def get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dataset_root\", type=Path, required=True,\n    )\n    parser.add_argument(\n        \"--dataset_config\", type=str, required=True,\n        help=\"This path may need to be changed depending on where you run this script. \"\n    )\n    parser.add_argument(\"--scene_id\", type=str, default=\"train_3\")",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "get_sam_segmentation_from_xyxy",
        "kind": 2,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "def get_sam_segmentation_from_xyxy(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n    sam_predictor.set_image(image)\n    result_masks = []\n    for box in xyxy:\n        masks, scores, logits = sam_predictor.predict(\n            box=box,\n            multimask_output=True\n        )\n        index = np.argmax(scores)\n        result_masks.append(masks[index])",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "get_sam_predictor",
        "kind": 2,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "def get_sam_predictor(variant: str, device: str | int) -> SamPredictor:\n    if variant == \"sam\":\n        sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH)\n        sam.to(device)\n        sam_predictor = SamPredictor(sam)\n        return sam_predictor\n    if variant == \"mobilesam\":\n        from MobileSAM.setup_mobile_sam import setup_model\n        MOBILE_SAM_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./EfficientSAM/mobile_sam.pt\")\n        checkpoint = torch.load(MOBILE_SAM_CHECKPOINT_PATH)",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "get_sam_segmentation_dense",
        "kind": 2,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "def get_sam_segmentation_dense(\n    variant:str, model: Any, image: np.ndarray\n) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n    '''\n    The SAM based on automatic mask generation, without bbox prompting\n    Args:\n        model: The mask generator or the YOLO model\n        image: )H, W, 3), in RGB color space, in range [0, 255]\n    Returns:\n        mask: (N, H, W)",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "get_sam_mask_generator",
        "kind": 2,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "def get_sam_mask_generator(variant:str, device: str | int) -> SamAutomaticMaskGenerator:\n    if variant == \"sam\":\n        sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH)\n        sam.to(device)\n        mask_generator = SamAutomaticMaskGenerator(\n            model=sam,\n            points_per_side=12,\n            points_per_batch=144,\n            pred_iou_thresh=0.88,\n            stability_score_thresh=0.95,",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "process_tag_classes",
        "kind": 2,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "def process_tag_classes(text_prompt:str, add_classes:List[str]=[], remove_classes:List[str]=[]) -> list[str]:\n    '''\n    Convert a text prompt from Tag2Text to a list of classes. \n    '''\n    classes = text_prompt.split(',')\n    classes = [obj_class.strip() for obj_class in classes]\n    classes = [obj_class for obj_class in classes if obj_class != '']\n    for c in add_classes:\n        if c not in classes:\n            classes.append(c)",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "process_ai2thor_classes",
        "kind": 2,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "def process_ai2thor_classes(classes: List[str], add_classes:List[str]=[], remove_classes:List[str]=[]) -> List[str]:\n    '''\n    Some pre-processing on AI2Thor objectTypes in a scene\n    '''\n    classes = list(set(classes))\n    for c in add_classes:\n        classes.append(c)\n    for c in remove_classes:\n        classes = [obj_class for obj_class in classes if c not in obj_class.lower()]\n    # Split the element in classes by captical letters",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "def main(args: argparse.Namespace):\n    ### Initialize the Grounding DINO model ###\n    grounding_dino_model = Model(\n        model_config_path=GROUNDING_DINO_CONFIG_PATH, \n        model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH, \n        device=args.device\n    )\n    ### Initialize the SAM model ###\n    if args.class_set == \"none\":\n        mask_generator = get_sam_mask_generator(args.sam_variant, args.device)",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "TAG2TEXT_PATH",
        "kind": 5,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "TAG2TEXT_PATH = os.path.join(GSA_PATH, \"\")\nEFFICIENTSAM_PATH = os.path.join(GSA_PATH, \"EfficientSAM\")\nsys.path.append(GSA_PATH) # This is needed for the following imports in this file\nsys.path.append(TAG2TEXT_PATH) # This is needed for some imports in the Tag2Text files\nsys.path.append(EFFICIENTSAM_PATH)\nsys.path.append(\"/home/kev/packages/Grounded-Segment-Anything/recognize-anything\")\nimport torchvision.transforms as TS\ntry:\n    from ram.models import ram\n    from ram.models import tag2text",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "EFFICIENTSAM_PATH",
        "kind": 5,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "EFFICIENTSAM_PATH = os.path.join(GSA_PATH, \"EfficientSAM\")\nsys.path.append(GSA_PATH) # This is needed for the following imports in this file\nsys.path.append(TAG2TEXT_PATH) # This is needed for some imports in the Tag2Text files\nsys.path.append(EFFICIENTSAM_PATH)\nsys.path.append(\"/home/kev/packages/Grounded-Segment-Anything/recognize-anything\")\nimport torchvision.transforms as TS\ntry:\n    from ram.models import ram\n    from ram.models import tag2text\n    from ram import inference_tag2text, inference_ram",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "GROUNDING_DINO_CONFIG_PATH",
        "kind": 5,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "GROUNDING_DINO_CONFIG_PATH = os.path.join(GSA_PATH, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\nGROUNDING_DINO_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./groundingdino_swint_ogc.pth\")\n# Segment-Anything checkpoint\nSAM_ENCODER_VERSION = \"vit_h\"\nSAM_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./sam_vit_h_4b8939.pth\")\n# Tag2Text checkpoint\nTAG2TEXT_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./tag2text_swin_14m.pth\")\nRAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "GROUNDING_DINO_CHECKPOINT_PATH",
        "kind": 5,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./groundingdino_swint_ogc.pth\")\n# Segment-Anything checkpoint\nSAM_ENCODER_VERSION = \"vit_h\"\nSAM_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./sam_vit_h_4b8939.pth\")\n# Tag2Text checkpoint\nTAG2TEXT_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./tag2text_swin_14m.pth\")\nRAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "SAM_ENCODER_VERSION",
        "kind": 5,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "SAM_ENCODER_VERSION = \"vit_h\"\nSAM_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./sam_vit_h_4b8939.pth\")\n# Tag2Text checkpoint\nTAG2TEXT_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./tag2text_swin_14m.pth\")\nRAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]\nFOREGROUND_MINIMAL_CLASSES = [\n    \"item\"",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "SAM_CHECKPOINT_PATH",
        "kind": 5,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "SAM_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./sam_vit_h_4b8939.pth\")\n# Tag2Text checkpoint\nTAG2TEXT_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./tag2text_swin_14m.pth\")\nRAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]\nFOREGROUND_MINIMAL_CLASSES = [\n    \"item\"\n]",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "TAG2TEXT_CHECKPOINT_PATH",
        "kind": 5,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "TAG2TEXT_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./tag2text_swin_14m.pth\")\nRAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]\nFOREGROUND_MINIMAL_CLASSES = [\n    \"item\"\n]\ndef get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "RAM_CHECKPOINT_PATH",
        "kind": 5,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "RAM_CHECKPOINT_PATH = os.path.join(TAG2TEXT_PATH, \"./ram_swin_large_14m.pth\")\nFOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]\nFOREGROUND_MINIMAL_CLASSES = [\n    \"item\"\n]\ndef get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "FOREGROUND_GENERIC_CLASSES",
        "kind": 5,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "FOREGROUND_GENERIC_CLASSES = [\n    \"item\", \"furniture\", \"object\", \"electronics\", \"wall decoration\", \"door\"\n]\nFOREGROUND_MINIMAL_CLASSES = [\n    \"item\"\n]\ndef get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dataset_root\", type=Path, required=True,",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "FOREGROUND_MINIMAL_CLASSES",
        "kind": 5,
        "importPath": "scripts.generate_gsa_results",
        "description": "scripts.generate_gsa_results",
        "peekOfCode": "FOREGROUND_MINIMAL_CLASSES = [\n    \"item\"\n]\ndef get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dataset_root\", type=Path, required=True,\n    )\n    parser.add_argument(\n        \"--dataset_config\", type=str, required=True,",
        "detail": "scripts.generate_gsa_results",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "scripts.run_post_filter_merge",
        "description": "scripts.run_post_filter_merge",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--result_path\", type=str, required=True)\n    parser.add_argument(\"--rgb_pcd_path\", type=str, default=None)\n    # To inspect the results of merge_overlap_objects\n    # This is mainly to quickly try out different thresholds\n    parser.add_argument(\"--merge_overlap_thresh\", type=float, default=-1)\n    parser.add_argument(\"--merge_visual_sim_thresh\", type=float, default=-1)\n    parser.add_argument(\"--merge_text_sim_thresh\", type=float, default=-1)\n    parser.add_argument(\"--obj_min_points\", type=int, default=0)",
        "detail": "scripts.run_post_filter_merge",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "scripts.run_slam_rgb",
        "description": "scripts.run_slam_rgb",
        "peekOfCode": "def get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dataset_root\", type=Path, required=True,\n    )\n    parser.add_argument(\n        \"--dataset_config\", type=str, required=True,\n        help=\"This path may need to be changed depending on where you run this script. \"\n    )\n    parser.add_argument(\"--scene_id\", type=str, default=\"train_3\")",
        "detail": "scripts.run_slam_rgb",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.run_slam_rgb",
        "description": "scripts.run_slam_rgb",
        "peekOfCode": "def main(args: argparse.Namespace):\n    if args.load_semseg:\n        load_embeddings = True\n        embedding_dir = \"embed_semseg\"\n        semseg_classes = json.load(open(\n            args.dataset_root / args.scene_id / \"embed_semseg_classes.json\", \"r\"\n        ))\n        embedding_dim = len(semseg_classes)\n    else:\n        load_embeddings = False",
        "detail": "scripts.run_slam_rgb",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.streamlined_detections",
        "description": "scripts.streamlined_detections",
        "peekOfCode": "def main(cfg: DictConfig):\n    # Initialize the dataset\n    dataset = get_dataset(\n        dataconfig=cfg.dataset_config,\n        start=cfg.start,\n        end=cfg.end,\n        stride=cfg.stride,\n        basedir=cfg.dataset_root,\n        sequence=cfg.scene_id,\n        desired_height=cfg.desired_height,",
        "detail": "scripts.streamlined_detections",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "scripts.visualize_cfslam_interact_llava",
        "description": "scripts.visualize_cfslam_interact_llava",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--result_path\", type=str, required=True)\n    parser.add_argument(\"--rgb_pcd_path\", type=str, default=None)\n    return parser\nif __name__ == \"__main__\":\n    parser = get_parser()\n    args = parser.parse_args()\n    # rich console for pretty printing\n    # console = rich.console.Console()",
        "detail": "scripts.visualize_cfslam_interact_llava",
        "documentation": {}
    },
    {
        "label": "create_ball_mesh",
        "kind": 2,
        "importPath": "scripts.visualize_cfslam_results",
        "description": "scripts.visualize_cfslam_results",
        "peekOfCode": "def create_ball_mesh(center, radius, color=(0, 1, 0)):\n    \"\"\"\n    Create a colored mesh sphere.\n    Args:\n    - center (tuple): (x, y, z) coordinates for the center of the sphere.\n    - radius (float): Radius of the sphere.\n    - color (tuple): RGB values in the range [0, 1] for the color of the sphere.\n    Returns:\n    - o3d.geometry.TriangleMesh: Colored mesh sphere.\n    \"\"\"",
        "detail": "scripts.visualize_cfslam_results",
        "documentation": {}
    },
    {
        "label": "get_parser",
        "kind": 2,
        "importPath": "scripts.visualize_cfslam_results",
        "description": "scripts.visualize_cfslam_results",
        "peekOfCode": "def get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--result_path\", type=str, default=None)\n    parser.add_argument(\"--rgb_pcd_path\", type=str, default=None)\n    parser.add_argument(\"--edge_file\", type=str, default=None)\n    parser.add_argument(\"--no_clip\", action=\"store_true\", \n                        help=\"If set, the CLIP model will not init for fast debugging.\")\n    # To inspect the results of merge_overlap_objects\n    # This is mainly to quickly try out different thresholds\n    parser.add_argument(\"--merge_overlap_thresh\", type=float, default=-1)",
        "detail": "scripts.visualize_cfslam_results",
        "documentation": {}
    },
    {
        "label": "load_result",
        "kind": 2,
        "importPath": "scripts.visualize_cfslam_results",
        "description": "scripts.visualize_cfslam_results",
        "peekOfCode": "def load_result(result_path):\n    with gzip.open(result_path, \"rb\") as f:\n        results = pickle.load(f)\n    if isinstance(results, dict):\n        objects = MapObjectList()\n        objects.load_serializable(results[\"objects\"])\n        if results['bg_objects'] is None:\n            bg_objects = None\n        else:\n            bg_objects = MapObjectList()",
        "detail": "scripts.visualize_cfslam_results",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.visualize_cfslam_results",
        "description": "scripts.visualize_cfslam_results",
        "peekOfCode": "def main(args):\n    result_path = args.result_path\n    rgb_pcd_path = args.rgb_pcd_path\n    assert not (result_path is None and rgb_pcd_path is None), \\\n        \"Either result_path or rgb_pcd_path must be provided.\"\n    if rgb_pcd_path is not None:        \n        pointclouds = Pointclouds.load_pointcloud_from_h5(rgb_pcd_path)\n        global_pcd = pointclouds.open3d(0, include_colors=True)\n        if result_path is None:\n            print(\"Only visualizing the pointcloud...\")",
        "detail": "scripts.visualize_cfslam_results",
        "documentation": {}
    },
    {
        "label": "compute_match_batch",
        "kind": 2,
        "importPath": "slam.cfslam_pipeline_batch",
        "description": "slam.cfslam_pipeline_batch",
        "peekOfCode": "def compute_match_batch(cfg, spatial_sim: torch.Tensor, visual_sim: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute object association based on spatial and visual similarities\n    Args:\n        spatial_sim: a MxN tensor of spatial similarities\n        visual_sim: a MxN tensor of visual similarities\n    Returns:\n        A MxN tensor of binary values, indicating whether a detection is associate with an object. \n        Each row has at most one 1, indicating one detection can be associated with at most one existing object.\n        One existing object can receive multiple new detections",
        "detail": "slam.cfslam_pipeline_batch",
        "documentation": {}
    },
    {
        "label": "prepare_objects_save_vis",
        "kind": 2,
        "importPath": "slam.cfslam_pipeline_batch",
        "description": "slam.cfslam_pipeline_batch",
        "peekOfCode": "def prepare_objects_save_vis(objects: MapObjectList, downsample_size: float=0.025):\n    objects_to_save = copy.deepcopy(objects)\n    # Downsample the point cloud\n    for i in range(len(objects_to_save)):\n        objects_to_save[i]['pcd'] = objects_to_save[i]['pcd'].voxel_down_sample(downsample_size)\n    # Remove unnecessary keys\n    for i in range(len(objects_to_save)):\n        for k in list(objects_to_save[i].keys()):\n            if k not in [\n                'pcd', 'bbox', 'clip_ft', 'text_ft', 'class_id', 'num_detections', 'inst_color'",
        "detail": "slam.cfslam_pipeline_batch",
        "documentation": {}
    },
    {
        "label": "process_cfg",
        "kind": 2,
        "importPath": "slam.cfslam_pipeline_batch",
        "description": "slam.cfslam_pipeline_batch",
        "peekOfCode": "def process_cfg(cfg: DictConfig):\n    cfg.dataset_root = Path(cfg.dataset_root)\n    cfg.dataset_config = Path(cfg.dataset_config)\n    if cfg.dataset_config.name != \"multiscan.yaml\":\n        # For datasets whose depth and RGB have the same resolution\n        # Set the desired image heights and width from the dataset config\n        dataset_cfg = omegaconf.OmegaConf.load(cfg.dataset_config)\n        if cfg.image_height is None:\n            cfg.image_height = dataset_cfg.camera_params.image_height\n        if cfg.image_width is None:",
        "detail": "slam.cfslam_pipeline_batch",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "slam.cfslam_pipeline_batch",
        "description": "slam.cfslam_pipeline_batch",
        "peekOfCode": "def main(cfg : DictConfig):\n    cfg = process_cfg(cfg)\n    # Initialize the dataset\n    dataset = get_dataset(\n        dataconfig=cfg.dataset_config,\n        start=cfg.start,\n        end=cfg.end,\n        stride=cfg.stride,\n        basedir=cfg.dataset_root,\n        sequence=cfg.scene_id,",
        "detail": "slam.cfslam_pipeline_batch",
        "documentation": {}
    },
    {
        "label": "BG_CLASSES",
        "kind": 5,
        "importPath": "slam.cfslam_pipeline_batch",
        "description": "slam.cfslam_pipeline_batch",
        "peekOfCode": "BG_CLASSES = [\"wall\", \"floor\", \"ceiling\"]\n# Disable torch gradient computation\ntorch.set_grad_enabled(False)\ndef compute_match_batch(cfg, spatial_sim: torch.Tensor, visual_sim: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute object association based on spatial and visual similarities\n    Args:\n        spatial_sim: a MxN tensor of spatial similarities\n        visual_sim: a MxN tensor of visual similarities\n    Returns:",
        "detail": "slam.cfslam_pipeline_batch",
        "documentation": {}
    },
    {
        "label": "compute_spatial_similarities",
        "kind": 2,
        "importPath": "slam.mapping",
        "description": "slam.mapping",
        "peekOfCode": "def compute_spatial_similarities(cfg, detection_list: DetectionList, objects: MapObjectList) -> torch.Tensor:\n    '''\n    Compute the spatial similarities between the detections and the objects\n    Args:\n        detection_list: a list of M detections\n        objects: a list of N objects in the map\n    Returns:\n        A MxN tensor of spatial similarities\n    '''\n    det_bboxes = detection_list.get_stacked_values_torch('bbox')",
        "detail": "slam.mapping",
        "documentation": {}
    },
    {
        "label": "compute_visual_similarities",
        "kind": 2,
        "importPath": "slam.mapping",
        "description": "slam.mapping",
        "peekOfCode": "def compute_visual_similarities(cfg, detection_list: DetectionList, objects: MapObjectList) -> torch.Tensor:\n    '''\n    Compute the visual similarities between the detections and the objects\n    Args:\n        detection_list: a list of M detections\n        objects: a list of N objects in the map\n    Returns:\n        A MxN tensor of visual similarities\n    '''\n    det_fts = detection_list.get_stacked_values_torch('clip_ft') # (M, D)",
        "detail": "slam.mapping",
        "documentation": {}
    },
    {
        "label": "aggregate_similarities",
        "kind": 2,
        "importPath": "slam.mapping",
        "description": "slam.mapping",
        "peekOfCode": "def aggregate_similarities(cfg, spatial_sim: torch.Tensor, visual_sim: torch.Tensor) -> torch.Tensor:\n    '''\n    Aggregate spatial and visual similarities into a single similarity score\n    Args:\n        spatial_sim: a MxN tensor of spatial similarities\n        visual_sim: a MxN tensor of visual similarities\n    Returns:\n        A MxN tensor of aggregated similarities\n    '''\n    if cfg.match_method == \"sim_sum\":",
        "detail": "slam.mapping",
        "documentation": {}
    },
    {
        "label": "merge_detections_to_objects",
        "kind": 2,
        "importPath": "slam.mapping",
        "description": "slam.mapping",
        "peekOfCode": "def merge_detections_to_objects(\n    cfg, \n    detection_list: DetectionList, \n    objects: MapObjectList, \n    agg_sim: torch.Tensor\n) -> MapObjectList:\n    # Iterate through all detections and merge them into objects\n    for i in range(agg_sim.shape[0]):\n        # If not matched to any object, add it as a new object\n        if agg_sim[i].max() == float('-inf'):",
        "detail": "slam.mapping",
        "documentation": {}
    },
    {
        "label": "DetectionList",
        "kind": 6,
        "importPath": "slam.slam_classes",
        "description": "slam.slam_classes",
        "peekOfCode": "class DetectionList(list):\n    def get_values(self, key, idx:int=None):\n        if idx is None:\n            return [detection[key] for detection in self]\n        else:\n            return [detection[key][idx] for detection in self]\n    def get_stacked_values_torch(self, key, idx:int=None):\n        values = []\n        for detection in self:\n            v = detection[key]",
        "detail": "slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "MapObjectList",
        "kind": 6,
        "importPath": "slam.slam_classes",
        "description": "slam.slam_classes",
        "peekOfCode": "class MapObjectList(DetectionList):\n    def compute_similarities(self, new_clip_ft):\n        '''\n        The input feature should be of shape (D, ), a one-row vector\n        This is mostly for backward compatibility\n        '''\n        # if it is a numpy array, make it a tensor \n        new_clip_ft = to_tensor(new_clip_ft)\n        # assuming cosine similarity for features\n        clip_fts = self.get_stacked_values_torch('clip_ft')",
        "detail": "slam.slam_classes",
        "documentation": {}
    },
    {
        "label": "compute_match_batch",
        "kind": 2,
        "importPath": "slam.streamlined_mapping",
        "description": "slam.streamlined_mapping",
        "peekOfCode": "def compute_match_batch(cfg, spatial_sim: torch.Tensor, visual_sim: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute object association based on spatial and visual similarities\n    Args:\n        spatial_sim: a MxN tensor of spatial similarities\n        visual_sim: a MxN tensor of visual similarities\n    Returns:\n        A MxN tensor of binary values, indicating whether a detection is associate with an object. \n        Each row has at most one 1, indicating one detection can be associated with at most one existing object.\n        One existing object can receive multiple new detections",
        "detail": "slam.streamlined_mapping",
        "documentation": {}
    },
    {
        "label": "prepare_objects_save_vis",
        "kind": 2,
        "importPath": "slam.streamlined_mapping",
        "description": "slam.streamlined_mapping",
        "peekOfCode": "def prepare_objects_save_vis(objects: MapObjectList, downsample_size: float=0.025):\n    objects_to_save = copy.deepcopy(objects)\n    # Downsample the point cloud\n    for i in range(len(objects_to_save)):\n        objects_to_save[i]['pcd'] = objects_to_save[i]['pcd'].voxel_down_sample(downsample_size)\n    # Remove unnecessary keys\n    for i in range(len(objects_to_save)):\n        for k in list(objects_to_save[i].keys()):\n            if k not in [\n                'pcd', 'bbox', 'clip_ft', 'text_ft', 'class_id', 'num_detections', 'inst_color'",
        "detail": "slam.streamlined_mapping",
        "documentation": {}
    },
    {
        "label": "process_cfg",
        "kind": 2,
        "importPath": "slam.streamlined_mapping",
        "description": "slam.streamlined_mapping",
        "peekOfCode": "def process_cfg(cfg: DictConfig):\n    cfg.dataset_root = Path(cfg.dataset_root)\n    cfg.dataset_config = Path(cfg.dataset_config)\n    if cfg.dataset_config.name != \"multiscan.yaml\":\n        # For datasets whose depth and RGB have the same resolution\n        # Set the desired image heights and width from the dataset config\n        dataset_cfg = omegaconf.OmegaConf.load(cfg.dataset_config)\n        if cfg.image_height is None:\n            cfg.image_height = dataset_cfg.camera_params.image_height\n        if cfg.image_width is None:",
        "detail": "slam.streamlined_mapping",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "slam.streamlined_mapping",
        "description": "slam.streamlined_mapping",
        "peekOfCode": "def main(cfg : DictConfig):\n    cfg = process_cfg(cfg)\n    # Initialize the dataset\n    dataset = get_dataset(\n        dataconfig=cfg.dataset_config,\n        start=cfg.start,\n        end=cfg.end,\n        stride=cfg.stride,\n        basedir=cfg.dataset_root,\n        sequence=cfg.scene_id,",
        "detail": "slam.streamlined_mapping",
        "documentation": {}
    },
    {
        "label": "BG_CLASSES",
        "kind": 5,
        "importPath": "slam.streamlined_mapping",
        "description": "slam.streamlined_mapping",
        "peekOfCode": "BG_CLASSES = [\"wall\", \"floor\", \"ceiling\"]\n# Disable torch gradient computation\ntorch.set_grad_enabled(False)\ndef compute_match_batch(cfg, spatial_sim: torch.Tensor, visual_sim: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute object association based on spatial and visual similarities\n    Args:\n        spatial_sim: a MxN tensor of spatial similarities\n        visual_sim: a MxN tensor of visual similarities\n    Returns:",
        "detail": "slam.streamlined_mapping",
        "documentation": {}
    },
    {
        "label": "get_classes_colors",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def get_classes_colors(classes):\n    class_colors = {}\n    # Generate a random color for each class\n    for class_idx, class_name in enumerate(classes):\n        # Generate random RGB values between 0 and 255\n        r = np.random.randint(0, 256)/255.0\n        g = np.random.randint(0, 256)/255.0\n        b = np.random.randint(0, 256)/255.0\n        # Assign the RGB values as a tuple to the class in the dictionary\n        class_colors[class_idx] = (r, g, b)",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "create_or_load_colors",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def create_or_load_colors(cfg, filename=\"gsa_classes_tag2text\"):\n    # get the classes, should be saved when making the dataset\n    classes_fp = cfg['dataset_root'] / cfg['scene_id'] / f\"{filename}.json\"\n    classes  = None\n    with open(classes_fp, \"r\") as f:\n        classes = json.load(f)\n    # create the class colors, or load them if they exist\n    class_colors  = None\n    class_colors_fp = cfg['dataset_root'] / cfg['scene_id'] / f\"{filename}_colors.json\"\n    if class_colors_fp.exists():",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "create_object_pcd",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def create_object_pcd(depth_array, mask, cam_K, image, obj_color=None) -> o3d.geometry.PointCloud:\n    fx, fy, cx, cy = from_intrinsics_matrix(cam_K)\n    # Also remove points with invalid depth values\n    mask = np.logical_and(mask, depth_array > 0)\n    if mask.sum() == 0:\n        pcd = o3d.geometry.PointCloud()\n        return pcd\n    height, width = depth_array.shape\n    x = np.arange(0, width, 1.0)\n    y = np.arange(0, height, 1.0)",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "pcd_denoise_dbscan",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def pcd_denoise_dbscan(pcd: o3d.geometry.PointCloud, eps=0.02, min_points=10) -> o3d.geometry.PointCloud:\n    ### Remove noise via clustering\n    pcd_clusters = pcd.cluster_dbscan(\n        eps=eps,\n        min_points=min_points,\n    )\n    # Convert to numpy arrays\n    obj_points = np.asarray(pcd.points)\n    obj_colors = np.asarray(pcd.colors)\n    pcd_clusters = np.array(pcd_clusters)",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "process_pcd",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def process_pcd(pcd, cfg, run_dbscan=True):\n    pcd = pcd.voxel_down_sample(voxel_size=cfg.downsample_voxel_size)\n    if cfg.dbscan_remove_noise and run_dbscan:\n        # print(\"Before dbscan:\", len(pcd.points))\n        pcd = pcd_denoise_dbscan(\n            pcd, \n            eps=cfg.dbscan_eps, \n            min_points=cfg.dbscan_min_points\n        )\n        # print(\"After dbscan:\", len(pcd.points))",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "get_bounding_box",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def get_bounding_box(cfg, pcd):\n    if (\"accurate\" in cfg.spatial_sim_type or \"overlap\" in cfg.spatial_sim_type) and len(pcd.points) >= 4:\n        try:\n            return pcd.get_oriented_bounding_box(robust=True)\n        except RuntimeError as e:\n            print(f\"Met {e}, use axis aligned bounding box instead\")\n            return pcd.get_axis_aligned_bounding_box()\n    else:\n        return pcd.get_axis_aligned_bounding_box()\ndef merge_obj2_into_obj1(cfg, obj1, obj2, run_dbscan=True):",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_obj2_into_obj1",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def merge_obj2_into_obj1(cfg, obj1, obj2, run_dbscan=True):\n    '''\n    Merge the new object to the old object\n    This operation is done in-place\n    '''\n    n_obj1_det = obj1['num_detections']\n    n_obj2_det = obj2['num_detections']\n    for k in obj1.keys():\n        if k in ['caption']:\n            # Here we need to merge two dictionaries and adjust the key of the second one",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "compute_overlap_matrix",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def compute_overlap_matrix(cfg, objects: MapObjectList):\n    '''\n    compute pairwise overlapping between objects in terms of point nearest neighbor. \n    Suppose we have a list of n point cloud, each of which is a o3d.geometry.PointCloud object. \n    Now we want to construct a matrix of size n x n, where the (i, j) entry is the ratio of points in point cloud i \n    that are within a distance threshold of any point in point cloud j. \n    '''\n    n = len(objects)\n    overlap_matrix = np.zeros((n, n))\n    # Convert the point clouds into numpy arrays and then into FAISS indices for efficient search",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "compute_overlap_matrix_2set",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def compute_overlap_matrix_2set(cfg, objects_map: MapObjectList, objects_new: DetectionList) -> np.ndarray:\n    '''\n    compute pairwise overlapping between two set of objects in terms of point nearest neighbor. \n    objects_map is the existing objects in the map, objects_new is the new objects to be added to the map\n    Suppose len(objects_map) = m, len(objects_new) = n\n    Then we want to construct a matrix of size m x n, where the (i, j) entry is the ratio of points \n    in point cloud i that are within a distance threshold of any point in point cloud j.\n    '''\n    m = len(objects_map)\n    n = len(objects_new)",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_overlap_objects",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def merge_overlap_objects(cfg, objects: MapObjectList, overlap_matrix: np.ndarray):\n    x, y = overlap_matrix.nonzero()\n    overlap_ratio = overlap_matrix[x, y]\n    sort = np.argsort(overlap_ratio)[::-1]\n    x = x[sort]\n    y = y[sort]\n    overlap_ratio = overlap_ratio[sort]\n    kept_objects = np.ones(len(objects), dtype=bool)\n    for i, j, ratio in zip(x, y, overlap_ratio):\n        visual_sim = F.cosine_similarity(",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "denoise_objects",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def denoise_objects(cfg, objects: MapObjectList):\n    for i in range(len(objects)):\n        og_object_pcd = objects[i]['pcd']\n        objects[i]['pcd'] = process_pcd(objects[i]['pcd'], cfg, run_dbscan=True)\n        if len(objects[i]['pcd'].points) < 4:\n            objects[i]['pcd'] = og_object_pcd\n            continue\n        objects[i]['bbox'] = get_bounding_box(cfg, objects[i]['pcd'])\n        objects[i]['bbox'].color = [0,1,0]\n    return objects",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_objects",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def filter_objects(cfg, objects: MapObjectList):\n    # Remove the object that has very few points or viewed too few times\n    print(\"Before filtering:\", len(objects))\n    objects_to_keep = []\n    for obj in objects:\n        if len(obj['pcd'].points) >= cfg.obj_min_points and obj['num_detections'] >= cfg.obj_min_detections:\n            objects_to_keep.append(obj)\n    objects = MapObjectList(objects_to_keep)\n    print(\"After filtering:\", len(objects))\n    return objects",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "merge_objects",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def merge_objects(cfg, objects: MapObjectList):\n    if cfg.merge_overlap_thresh > 0:\n        # Merge one object into another if the former is contained in the latter\n        overlap_matrix = compute_overlap_matrix(cfg, objects)\n        print(\"Before merging:\", len(objects))\n        objects = merge_overlap_objects(cfg, objects, overlap_matrix)\n        print(\"After merging:\", len(objects))\n    return objects\ndef filter_gobs(\n    cfg: DictConfig,",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "filter_gobs",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def filter_gobs(\n    cfg: DictConfig,\n    gobs: dict,\n    image: np.ndarray,\n    BG_CLASSES = [\"wall\", \"floor\", \"ceiling\"],\n):\n    # If no detection at all\n    if len(gobs['xyxy']) == 0:\n        return gobs\n    # Filter out the objects based on various criteria",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "resize_gobs",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def resize_gobs(\n    gobs,\n    image\n):\n    n_masks = len(gobs['xyxy'])\n    new_mask = []\n    for mask_idx in range(n_masks):\n        # TODO: rewrite using interpolation/resize in numpy or torch rather than cv2\n        mask = gobs['mask'][mask_idx]\n        if mask.shape != image.shape[:2]:",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "gobs_to_detection_list",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def gobs_to_detection_list(\n    cfg, \n    image, \n    depth_array,\n    cam_K, \n    idx, \n    gobs, \n    trans_pose = None,\n    class_names = None,\n    BG_CLASSES = [\"wall\", \"floor\", \"ceiling\"],",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "transform_detection_list",
        "kind": 2,
        "importPath": "slam.utils",
        "description": "slam.utils",
        "peekOfCode": "def transform_detection_list(\n    detection_list: DetectionList,\n    transform: torch.Tensor,\n    deepcopy = False,\n):\n    '''\n    Transform the detection list by the given transform\n    Args:\n        detection_list: DetectionList\n        transform: 4x4 torch.Tensor",
        "detail": "slam.utils",
        "documentation": {}
    },
    {
        "label": "get_top_down_frame",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def get_top_down_frame(controller):\n    # Setup the top-down camera\n    event = controller.step(action=\"GetMapViewCameraProperties\", raise_for_failure=True)\n    pose = copy.deepcopy(event.metadata[\"actionReturn\"])\n    bounds = event.metadata[\"sceneBounds\"][\"size\"]\n    max_bound = max(bounds[\"x\"], bounds[\"z\"])\n    pose[\"position\"][\"y\"] += 1.1 * max_bound\n    pose[\"farClippingPlane\"] = 50\n    # pose[\"fieldOfView\"] = 50\n    # pose[\"orthographic\"] = False",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "adjust_ai2thor_pose",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def adjust_ai2thor_pose(pose):\n    '''\n    Adjust the camera pose from the one used in Unity to that in Open3D.\n    '''\n    # Transformation matrix to flip Y-axis\n    flip_y = np.array([\n        [1, 0, 0, 0],\n        [0, -1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "adjust_ai2thor_pose_batch",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def adjust_ai2thor_pose_batch(poses):\n    '''\n    Adjust the camera poses from the one used in Unity to that in Open3D.\n    '''\n    N = poses.shape[0]\n    # Transformation matrix to flip Y-axis\n    flip_y = np.array([\n        [1, 0, 0, 0],\n        [0, -1, 0, 0],\n        [0, 0, 1, 0],",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "adjust_ai2thor_batch_torch",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def adjust_ai2thor_batch_torch(poses):\n    '''\n    Adjust the camera poses from the one used in Unity to that in Open3D.\n    Args:\n        poses: torch.Tensor, shape (N, 4, 4)\n    Returns:\n        adjusted_pose: torch.Tensor, shape (N, 4, 4)\n    '''\n    N = poses.shape[0]\n    # Transformation matrix to flip Y-axis",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "depth2xyz",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def depth2xyz(depth: np.ndarray, K: np.ndarray) -> np.ndarray:\n    '''\n    Convert depth image to 3D XYZ image in the camera coordinate frame\n    Args:\n        depth: depth image, shape (H, W), in meters\n        K: camera intrinsics matrix, shape (3, 3)\n    Returns:\n        xyz_camera: 3D XYZ image in the camera coordinate frame, shape (H, W, 3)\n    '''\n    frame_size = depth.shape[:2]",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "transform_xyz",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def transform_xyz(xyz: np.ndarray, pose: np.ndarray) -> np.ndarray:\n    '''\n    Transform the 3D XYZ image using the input pose matrix\n    Args:\n        xyz: 3D XYZ image, shape (H, W, 3)\n        pose: 4x4 pose matrix, shape (4, 4)\n    Returns:\n        xyz_transformed: transformed 3D XYZ image, shape (H, W, 3)\n    '''\n    xyz_flatten = xyz.reshape(-1, 3)",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_scene",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def get_scene(scene_name):\n    # By default, use scene from AI2THOR\n    # If the scene name starts with train, val, or test, use the scene from ProcTHOR\n    scene = scene_name\n    if (\n        scene_name.startswith(\"train\")\n        or scene_name.startswith(\"val\")\n        or scene_name.startswith(\"test\")\n    ):\n        dataset = prior.load_dataset(\"procthor-10k\")",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_intrinsics",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def compute_intrinsics(vfov, height, width):\n    \"\"\"\n    Compute the camera intrinsics matrix K from the\n    vertical field of view (in degree), height, and width.\n    \"\"\"\n    # For Unity, the field view is the vertical field of view.\n    f = height / (2 * np.tan(np.deg2rad(vfov) / 2))\n    return np.array([[f, 0, width / 2], [0, f, height / 2], [0, 0, 1]])\ndef compute_pose(position: dict, rotation: dict) -> np.ndarray:\n    \"\"\"",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_pose",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def compute_pose(position: dict, rotation: dict) -> np.ndarray:\n    \"\"\"\n    Compute the camera extrinsics matrix from the position and rotation.\n    Note that in Unity, XYZ follows the left-hand rule, with Y pointing up.\n    See: https://docs.unity3d.com/560/Documentation/Manual/Transforms.html\n    In the camera coordinate, Z is the viewing direction, X is right, and Y is up. \n    See: https://library.vuforia.com/device-tracking/spatial-frame-reference\n    Euler angles are in degrees and in Rotation is done in the ZXY order.\n    See: https://docs.unity3d.com/ScriptReference/Transform-eulerAngles.html\n    \"\"\"",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_posrot",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def compute_posrot(T: np.ndarray) -> Tuple[dict, dict]:\n    \"\"\"\n    Decompose the camera extrinsics matrix into position and rotation.\n    This function reverses the operation performed in `compute_pose`, considering\n    Unity's conventions, left-hand coordinate system, and ZXY Euler angle rotation order.\n    \"\"\"\n    # Extract the rotation matrix from the first 3x3 elements\n    R = T[:3, :3]\n    # Extract the translation vector from the last column\n    t = T[:3, 3]",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_agent_pose_from_event",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def get_agent_pose_from_event(event) -> np.ndarray:\n    '''\n    Compute the 4x4 agent pose matrix from the event\n    '''\n    position = event.metadata[\"agent\"][\"position\"]\n    rotation = event.metadata[\"agent\"][\"rotation\"]\n    # Compute the agent pose (position and rotation of agent's body in global 3D space)\n    agent_pose = compute_pose(position, rotation)\n    return agent_pose\ndef get_camera_pose_from_event(event) -> np.ndarray:",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "get_camera_pose_from_event",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def get_camera_pose_from_event(event) -> np.ndarray:\n    '''\n    Compute the 4x4 camera pose matrix from the event\n    This is different from the agent pose!\n    '''\n    camera_position = event.metadata['cameraPosition']\n    camera_rotation = copy.deepcopy(event.metadata[\"agent\"][\"rotation\"])\n    camera_rotation['x'] = event.metadata['agent']['cameraHorizon']\n    camera_pose = compute_pose(camera_position, camera_rotation)\n    return camera_pose",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "sample_pose_random",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def sample_pose_random(controller: Controller, n_poses: int):\n    reachable_positions = controller.step(action=\"GetReachablePositions\").metadata[\n        \"actionReturn\"\n    ]\n    # Convert the positions to numpy array\n    reachable_np = np.array([[p[\"x\"], p[\"y\"], p[\"z\"]] for p in reachable_positions])\n    print(reachable_np)\n    # Generate a list of poses for taking pictures\n    sampled_poses = []\n    for i in trange(n_poses):",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "sample_pose_uniform",
        "kind": 2,
        "importPath": "utils.ai2thor",
        "description": "utils.ai2thor",
        "peekOfCode": "def sample_pose_uniform(controller: Controller, n_positions: int):\n    \"\"\"\n    Uniformly sample n_positions from the reachable positions\n    for each position, uniformly sample 8 rotations (0, 45, 90, 135, 180, 225, 270, 315)\n    \"\"\"\n    reachable_positions = controller.step(action=\"GetReachablePositions\").metadata[\n        \"actionReturn\"\n    ]\n    # Convert the positions to numpy array\n    reachable_np = np.array([[p[\"x\"], p[\"y\"], p[\"z\"]] for p in reachable_positions])",
        "detail": "utils.ai2thor",
        "documentation": {}
    },
    {
        "label": "compute_pred_gt_associations",
        "kind": 2,
        "importPath": "utils.eval",
        "description": "utils.eval",
        "peekOfCode": "def compute_pred_gt_associations(pred, gt):\n    # pred: predicted pointcloud\n    # gt: GT pointcloud\n    from chamferdist.chamfer import knn_points\n    # pred = pointclouds.points_padded.cuda().contiguous()\n    # gt = pts_gt.unsqueeze(0).cuda().contiguous()\n    b, l, d = pred.shape\n    lengths_src = torch.ones(b, dtype=torch.long, device=pred.device) * l\n    b, l, d = gt.shape\n    lengths_tgt = torch.ones(b, dtype=torch.long, device=pred.device) * l",
        "detail": "utils.eval",
        "documentation": {}
    },
    {
        "label": "compute_confmatrix",
        "kind": 2,
        "importPath": "utils.eval",
        "description": "utils.eval",
        "peekOfCode": "def compute_confmatrix(\n    labels_pred, labels_gt, idx_pred_to_gt, idx_gt_to_pred, class_names\n):\n    labels_gt = labels_gt[idx_pred_to_gt]\n    # num_classes = labels_gt.max().item() + 1\n    # print(num_classes)\n    num_classes = len(class_names)\n    print(num_classes)\n    confmatrix = torch.zeros(num_classes, num_classes, device=labels_pred.device)\n    for class_gt_int in range(num_classes):",
        "detail": "utils.eval",
        "documentation": {}
    },
    {
        "label": "compute_metrics",
        "kind": 2,
        "importPath": "utils.eval",
        "description": "utils.eval",
        "peekOfCode": "def compute_metrics(confmatrix, class_names):\n    if isinstance(confmatrix, torch.Tensor):\n        confmatrix = confmatrix.cpu().numpy()\n    num_classes = len(class_names)\n    ious = np.zeros((num_classes))\n    precision = np.zeros((num_classes))\n    recall = np.zeros((num_classes))\n    f1score = np.zeros((num_classes))\n    for _idx in range(num_classes):\n        ious[_idx] = confmatrix[_idx, _idx] / (",
        "detail": "utils.eval",
        "documentation": {}
    },
    {
        "label": "Timer",
        "kind": 6,
        "importPath": "utils.general_utils",
        "description": "utils.general_utils",
        "peekOfCode": "class Timer:\n    def __init__(self, heading = \"\", verbose = True):\n        self.verbose = verbose\n        if not self.verbose:\n            return\n        self.heading = heading\n    def __enter__(self):\n        if not self.verbose:\n            return self\n        self.start = time.time()",
        "detail": "utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "kind": 2,
        "importPath": "utils.general_utils",
        "description": "utils.general_utils",
        "peekOfCode": "def to_numpy(tensor):\n    if isinstance(tensor, np.ndarray):\n        return tensor\n    return tensor.detach().cpu().numpy()\ndef to_tensor(numpy_array, device=None):\n    if isinstance(numpy_array, torch.Tensor):\n        return numpy_array\n    if device is None:\n        return torch.from_numpy(numpy_array)\n    else:",
        "detail": "utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_tensor",
        "kind": 2,
        "importPath": "utils.general_utils",
        "description": "utils.general_utils",
        "peekOfCode": "def to_tensor(numpy_array, device=None):\n    if isinstance(numpy_array, torch.Tensor):\n        return numpy_array\n    if device is None:\n        return torch.from_numpy(numpy_array)\n    else:\n        return torch.from_numpy(numpy_array).to(device)\ndef to_scalar(d: np.ndarray | torch.Tensor | float) -> int | float:\n    '''\n    Convert the d to a scalar",
        "detail": "utils.general_utils",
        "documentation": {}
    },
    {
        "label": "to_scalar",
        "kind": 2,
        "importPath": "utils.general_utils",
        "description": "utils.general_utils",
        "peekOfCode": "def to_scalar(d: np.ndarray | torch.Tensor | float) -> int | float:\n    '''\n    Convert the d to a scalar\n    '''\n    if isinstance(d, float):\n        return d\n    elif \"numpy\" in str(type(d)):\n        assert d.size == 1\n        return d.item()\n    elif isinstance(d, torch.Tensor):",
        "detail": "utils.general_utils",
        "documentation": {}
    },
    {
        "label": "prjson",
        "kind": 2,
        "importPath": "utils.general_utils",
        "description": "utils.general_utils",
        "peekOfCode": "def prjson(input_json, indent=0):\n    \"\"\" Pretty print a json object \"\"\"\n    if not isinstance(input_json, list):\n        input_json = [input_json]\n    print(\"[\")\n    for i, entry in enumerate(input_json):\n        print(\"  {\")\n        for j, (key, value) in enumerate(entry.items()):\n            terminator = \",\" if j < len(entry) - 1 else \"\"\n            if isinstance(value, str):",
        "detail": "utils.general_utils",
        "documentation": {}
    },
    {
        "label": "cfg_to_dict",
        "kind": 2,
        "importPath": "utils.general_utils",
        "description": "utils.general_utils",
        "peekOfCode": "def cfg_to_dict(input_cfg):\n    \"\"\" Convert a json object to a dictionary representation \"\"\"\n    # Ensure input is a list for uniform processing\n    if not isinstance(input_cfg, list):\n        input_cfg = [input_cfg]\n    result = []  # Initialize the result list to hold our dictionaries\n    for entry in input_cfg:\n        entry_dict = {}  # Dictionary to store current entry's data\n        for key, value in entry.items():\n            # Replace escaped newline and tab characters in strings",
        "detail": "utils.general_utils",
        "documentation": {}
    },
    {
        "label": "measure_time",
        "kind": 2,
        "importPath": "utils.general_utils",
        "description": "utils.general_utils",
        "peekOfCode": "def measure_time(func):\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        # print(f\"Starting {func.__name__}...\")\n        result = func(*args, **kwargs)  # Call the function with any arguments it was called with\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        print(f\"Done! Execution time of {func.__name__} function: {elapsed_time:.2f} seconds\")\n        return result  # Return the result of the function call\n    return wrapper",
        "detail": "utils.general_utils",
        "documentation": {}
    },
    {
        "label": "save_hydra_config",
        "kind": 2,
        "importPath": "utils.general_utils",
        "description": "utils.general_utils",
        "peekOfCode": "def save_hydra_config(hydra_cfg, exp_out_path):\n    with open(exp_out_path / \"config_params.json\", \"w\") as f:\n        json.dump(cfg_to_dict(hydra_cfg), f, indent=2)",
        "detail": "utils.general_utils",
        "documentation": {}
    },
    {
        "label": "transform_points_batch",
        "kind": 2,
        "importPath": "utils.geometry",
        "description": "utils.geometry",
        "peekOfCode": "def transform_points_batch(poses: torch.Tensor, points: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    poses: (M, 4, 4)\n    points: (N, 3)\n    return: (M, N, 3)\n    \"\"\"\n    N = points.shape[0]\n    M = poses.shape[0]\n    # Convert points to homogeneous coordinates (N, 4)\n    points_homogeneous = torch.cat((points, torch.ones(N, 1).to(points.device)), dim=-1) # (N, 4)",
        "detail": "utils.geometry",
        "documentation": {}
    },
    {
        "label": "project_points_camera2plane",
        "kind": 2,
        "importPath": "utils.geometry",
        "description": "utils.geometry",
        "peekOfCode": "def project_points_camera2plane(points: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Project a set of points (in camera coordinates) to the image plane according to the camera intrinsics\n    points: (N, 3)\n    K: (3, 3)\n    return: \n        points_proj: (N, 2) the projected points in the image plane\n        points_depth: (N,) the depth of the points in the camera coordinates\n    \"\"\"\n    # multiply points by the camera intrinsics",
        "detail": "utils.geometry",
        "documentation": {}
    },
    {
        "label": "project_points_world2plane",
        "kind": 2,
        "importPath": "utils.geometry",
        "description": "utils.geometry",
        "peekOfCode": "def project_points_world2plane(points: torch.Tensor, poses: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n    \"\"\"Project a set of points (in the world coordinates) to the image plane according to the camera pose and intrinsics\n    Args:\n        points (torch.Tensor): (N, 3)\n        poses (torch.Tensor): (M, 4, 4) the camera poses in the world coordinates\n        K (torch.Tensor): (3, 3)\n    Returns:\n        points_coord (torch.Tensor): (M, N, 2) the projected points in the image plane\n        points_depth (torch.Tensor): (M, N) the depth of the points in the camera coordinates\n    \"\"\"",
        "detail": "utils.geometry",
        "documentation": {}
    },
    {
        "label": "check_proj_points",
        "kind": 2,
        "importPath": "utils.geometry",
        "description": "utils.geometry",
        "peekOfCode": "def check_proj_points(\n    points: torch.Tensor, \n    depth_tensor: torch.Tensor,\n    K: torch.Tensor, \n    pose: torch.Tensor,\n    depth_margin: float = 0.05,\n) -> torch.Tensor:\n    '''\n    Project points to the image plane and perform visibility checks\n    Args:",
        "detail": "utils.geometry",
        "documentation": {}
    },
    {
        "label": "crop_image_pil",
        "kind": 2,
        "importPath": "utils.image",
        "description": "utils.image",
        "peekOfCode": "def crop_image_pil(image: Image, x1:int, y1:int, x2:int, y2:int, padding:int=0) -> Image:\n    '''\n    Crop the image with some padding\n    Args:\n        image: PIL image\n        x1, y1, x2, y2: bounding box coordinates\n        padding: padding around the bounding box\n    Returns:\n        image_crop: PIL image\n    '''",
        "detail": "utils.image",
        "documentation": {}
    },
    {
        "label": "compute_3d_iou",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_3d_iou(bbox1, bbox2, padding=0, use_iou=True):\n    # Get the coordinates of the first bounding box\n    bbox1_min = np.asarray(bbox1.get_min_bound()) - padding\n    bbox1_max = np.asarray(bbox1.get_max_bound()) + padding\n    # Get the coordinates of the second bounding box\n    bbox2_min = np.asarray(bbox2.get_min_bound()) - padding\n    bbox2_max = np.asarray(bbox2.get_max_bound()) + padding\n    # Compute the overlap between the two bounding boxes\n    overlap_min = np.maximum(bbox1_min, bbox2_min)\n    overlap_max = np.minimum(bbox1_max, bbox2_max)",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_iou_batch",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_iou_batch(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute IoU between two sets of axis-aligned 3D bounding boxes.\n    bbox1: (M, V, D), e.g. (M, 8, 3)\n    bbox2: (N, V, D), e.g. (N, 8, 3)\n    returns: (M, N)\n    '''\n    # Compute min and max for each box\n    bbox1_min, _ = bbox1.min(dim=1) # Shape: (M, 3)\n    bbox1_max, _ = bbox1.max(dim=1) # Shape: (M, 3)",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_giou",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_3d_giou(bbox1, bbox2):\n    # Get the coordinates of the first bounding box\n    bbox1_min = np.asarray(bbox1.get_min_bound())\n    bbox1_max = np.asarray(bbox1.get_max_bound())\n    # Get the coordinates of the second bounding box\n    bbox2_min = np.asarray(bbox2.get_min_bound())\n    bbox2_max = np.asarray(bbox2.get_max_bound())\n    # Intersection\n    intersec_min = np.maximum(bbox1_min, bbox2_min)\n    intersec_max = np.minimum(bbox1_max, bbox2_max)",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_giou_batch",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_giou_batch(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute the generalized IoU between two sets of axis-aligned 3D bounding boxes.\n    bbox1: (M, V, D), e.g. (M, 8, 3)\n    bbox2: (N, V, D), e.g. (N, 8, 3)\n    returns: (M, N)\n    '''\n    # Compute min and max for each box\n    bbox1_min, _ = bbox1.min(dim=1) # Shape: (M, D)\n    bbox1_max, _ = bbox1.max(dim=1) # Shape: (M, D)",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_iou_accuracte_batch",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_3d_iou_accuracte_batch(bbox1, bbox2):\n    '''\n    Compute IoU between two sets of oriented (or axis-aligned) 3D bounding boxes.\n    bbox1: (M, 8, D), e.g. (M, 8, 3)\n    bbox2: (N, 8, D), e.g. (N, 8, 3)\n    returns: (M, N)\n    '''\n    # Must expend the box beforehand, otherwise it may results overestimated results\n    bbox1 = expand_3d_box(bbox1, 0.02)\n    bbox2 = expand_3d_box(bbox2, 0.02)",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_giou_accurate",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_3d_giou_accurate(obj1, obj2):\n    '''\n    Compute the 3D GIoU in a more accurate way. \n    '''\n    import pytorch3d.ops as ops\n    # This is too slow\n    # bbox1 = pcd1.get_minimal_oriented_bounding_box()\n    # bbox2 = pcd2.get_minimal_oriented_bounding_box()\n    # This is still slow ... \n    # Moved it outside of this function so that it is computed less times",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_box_volume_batch",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_3d_box_volume_batch(bbox: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute the volume of a set of rectangular boxes.\n    This assumes bbox corner order follows the open3d convention, which is:\n    ---, +--, -+-, --+, +++, -++, +-+, ++-\n    See https://github.com/isl-org/Open3D/blob/47f4ee936841ae9f8a9c4ce5d9162bd5b3e0279f/cpp/open3d/geometry/BoundingVolume.cpp#L92\n    bbox: (N, 8, D)\n    returns: (N,)\n    '''\n    a = torch.linalg.vector_norm(bbox[:, 0, :] - bbox[:, 1, :], ord=2, dim=1)",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "expand_3d_box",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def expand_3d_box(bbox: torch.Tensor, eps=0.02) -> torch.Tensor:\n    '''\n    Expand the side of 3D boxes such that each side has at least eps length.\n    Assumes the bbox cornder order in open3d convention. \n    bbox: (N, 8, D)\n    returns: (N, 8, D)\n    '''\n    center = bbox.mean(dim=1)  # shape: (N, D)\n    va = bbox[:, 1, :] - bbox[:, 0, :]  # shape: (N, D)\n    vb = bbox[:, 2, :] - bbox[:, 0, :]  # shape: (N, D)",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_enclosing_vol",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_enclosing_vol(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute the enclosing volume between every pair of boxes in bbox1 and bbox2.\n    This is an accurate but slow version using convex hull\n    bbox1: (M, 8, D)\n    bbox2: (N, 8, D)\n    returns: (M, N)\n    '''\n    M = bbox1.shape[0]\n    N = bbox2.shape[0]",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_enclosing_vol_fast",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_enclosing_vol_fast(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute the enclosing volume between every pair of boxes in bbox1 and bbox2.\n    This is fast but approximate version using axis-aligned bounding box\n    bbox1: (M, 8, 3)\n    bbox2: (N, 8, 3)\n    returns: (M, N)\n    '''\n    M = bbox1.shape[0]\n    N = bbox2.shape[0]",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_giou_accurate_batch",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_3d_giou_accurate_batch(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute Generalized IoU between two sets of oriented (or axis-aligned) 3D bounding boxes.\n    bbox1: (M, 8, D), e.g. (M, 8, 3)\n    bbox2: (N, 8, D), e.g. (N, 8, 3)\n    returns: (M, N)\n    '''\n    # Must expend the box beforehand, otherwise it may results overestimated results\n    bbox1 = expand_3d_box(bbox1, 0.02)\n    bbox2 = expand_3d_box(bbox2, 0.02)",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_3d_contain_ratio_accurate_batch",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_3d_contain_ratio_accurate_batch(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n    '''\n    Compute for i-th box in bbox1, how much of it is contained in j-th box in bbox2.\n    bbox1: (M, 8, D), e.g. (M, 8, 3)\n    bbox2: (N, 8, D), e.g. (N, 8, 3)\n    returns: (M, N)\n    '''\n    # Must expend the box beforehand, otherwise it may results overestimated results\n    bbox1 = expand_3d_box(bbox1)\n    bbox2 = expand_3d_box(bbox2)",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "compute_2d_box_contained_batch",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def compute_2d_box_contained_batch(bbox: torch.Tensor, thresh:float=0.95) -> torch.Tensor:\n    '''\n    For each bbox, compute how many other bboxes are containing it. \n    First compute the area of the intersection between each pair of bboxes. \n    Then for each bbox, count how many bboxes have the intersection area larger than thresh of its own area.\n    bbox: (N, 4), in (x1, y1, x2, y2) format\n    returns: (N,)\n    '''\n    N = bbox.shape[0]\n    # Get areas of each bbox",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "mask_subtract_contained",
        "kind": 2,
        "importPath": "utils.ious",
        "description": "utils.ious",
        "peekOfCode": "def mask_subtract_contained(xyxy: np.ndarray, mask: np.ndarray, th1=0.8, th2=0.7):\n    '''\n    Compute the containing relationship between all pair of bounding boxes.\n    For each mask, subtract the mask of bounding boxes that are contained by it.\n    Args:\n        xyxy: (N, 4), in (x1, y1, x2, y2) format\n        mask: (N, H, W), binary mask\n        th1: float, threshold for computing intersection over box1\n        th2: float, threshold for computing intersection over box2\n    Returns:",
        "detail": "utils.ious",
        "documentation": {}
    },
    {
        "label": "get_sam_predictor",
        "kind": 2,
        "importPath": "utils.model_utils",
        "description": "utils.model_utils",
        "peekOfCode": "def get_sam_predictor(cfg) -> SamPredictor:\n    if cfg.sam_variant == \"sam\":\n        sam = sam_model_registry[cfg.sam_encoder_version](checkpoint=cfg.sam_checkpoint_path)\n        sam.to(cfg.device)\n        sam_predictor = SamPredictor(sam)\n        return sam_predictor\n    if cfg.sam_variant == \"mobilesam\":\n        from MobileSAM.setup_mobile_sam import setup_model\n        # MOBILE_SAM_CHECKPOINT_PATH = os.path.join(GSA_PATH, \"./EfficientSAM/mobile_sam.pt\")\n        # checkpoint = torch.load(MOBILE_SAM_CHECKPOINT_PATH)",
        "detail": "utils.model_utils",
        "documentation": {}
    },
    {
        "label": "get_sam_segmentation_from_xyxy_batched",
        "kind": 2,
        "importPath": "utils.model_utils",
        "description": "utils.model_utils",
        "peekOfCode": "def get_sam_segmentation_from_xyxy_batched(sam_predictor: SamPredictor, image: np.ndarray, xyxy_tensor: torch.Tensor) -> torch.Tensor:\n    sam_predictor.set_image(image)\n    transformed_boxes = sam_predictor.transform.apply_boxes_torch(xyxy_tensor, image.shape[:2])\n    masks, _, _ = sam_predictor.predict_torch(\n        point_coords=None,\n        point_labels=None,\n        boxes=transformed_boxes,\n        multimask_output=False,\n    )\n    return masks.squeeze()",
        "detail": "utils.model_utils",
        "documentation": {}
    },
    {
        "label": "get_sam_segmentation_from_xyxy",
        "kind": 2,
        "importPath": "utils.model_utils",
        "description": "utils.model_utils",
        "peekOfCode": "def get_sam_segmentation_from_xyxy(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n    sam_predictor.set_image(image)\n    result_masks = []\n    for box in xyxy:\n        masks, scores, logits = sam_predictor.predict(\n            box=box,\n            multimask_output=True\n        )\n        index = np.argmax(scores)\n        result_masks.append(masks[index])",
        "detail": "utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_clip_features",
        "kind": 2,
        "importPath": "utils.model_utils",
        "description": "utils.model_utils",
        "peekOfCode": "def compute_clip_features(image, detections, clip_model, clip_preprocess, clip_tokenizer, classes, device):\n    backup_image = image.copy()\n    image = Image.fromarray(image)\n    # padding = args.clip_padding  # Adjust the padding amount as needed\n    padding = 20  # Adjust the padding amount as needed\n    image_crops = []\n    image_feats = []\n    text_feats = []\n    for idx in range(len(detections.xyxy)):\n        # Get the crop of the mask with padding",
        "detail": "utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_clip_features_batched",
        "kind": 2,
        "importPath": "utils.model_utils",
        "description": "utils.model_utils",
        "peekOfCode": "def compute_clip_features_batched(image, detections, clip_model, clip_preprocess, clip_tokenizer, classes, device):\n    image = Image.fromarray(image)\n    padding = 20  # Adjust the padding amount as needed\n    image_crops = []\n    preprocessed_images = []\n    text_tokens = []\n    # Prepare data for batch processing\n    for idx in range(len(detections.xyxy)):\n        x_min, y_min, x_max, y_max = detections.xyxy[idx]\n        image_width, image_height = image.size",
        "detail": "utils.model_utils",
        "documentation": {}
    },
    {
        "label": "compute_ft_vector_closeness_statistics",
        "kind": 2,
        "importPath": "utils.model_utils",
        "description": "utils.model_utils",
        "peekOfCode": "def compute_ft_vector_closeness_statistics(unbatched, batched):\n    # Initialize lists to store statistics\n    mad = []  # Mean Absolute Difference\n    max_diff = []  # Maximum Absolute Difference\n    mrd = []  # Mean Relative Difference\n    cosine_sim = []  # Cosine Similarity\n    for i in range(len(unbatched)):\n        diff = np.abs(unbatched[i] - batched[i])\n        mad.append(np.mean(diff))\n        max_diff.append(np.max(diff))",
        "detail": "utils.model_utils",
        "documentation": {}
    },
    {
        "label": "OnlineObjectRenderer",
        "kind": 6,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "class OnlineObjectRenderer():\n    '''\n    Refactor of the open3d visualization code to make it more modular\n    '''\n    def __init__(\n        self, \n        view_param: str | dict,\n        base_objects: MapObjectList | None = None,\n        gray_map: bool = False\n    ) -> None:",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "LineMesh",
        "kind": 6,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "class LineMesh(object):\n    def __init__(self, points, lines=None, colors=[0, 1, 0], radius=0.15):\n        \"\"\"Creates a line represented as sequence of cylinder triangular meshes\n        Arguments:\n            points {ndarray} -- Numpy array of ponts Nx3.\n        Keyword Arguments:\n            lines {list[list] or None} -- List of point index pairs denoting line segments. If None, implicit lines from ordered pairwise points. (default: {None})\n            colors {list} -- list of colors, or single color of the line (default: {[0, 1, 0]})\n            radius {float} -- radius of cylinder (default: {0.15})\n        \"\"\"",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "get_random_colors",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def get_random_colors(num_colors):\n    '''\n    Generate random colors for visualization\n    Args:\n        num_colors (int): number of colors to generate\n    Returns:\n        colors (np.ndarray): (num_colors, 3) array of colors, in RGB, [0, 1]\n    '''\n    colors = []\n    for i in range(num_colors):",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "show_mask",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "show_points",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \ndef show_box(box, ax, label=None):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n    if label is not None:",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "show_box",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def show_box(box, ax, label=None):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n    if label is not None:\n        ax.text(x0, y0, label)\ndef vis_result_fast(\n    image: np.ndarray, \n    detections: sv.Detections, \n    classes: list[str], ",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_fast",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def vis_result_fast(\n    image: np.ndarray, \n    detections: sv.Detections, \n    classes: list[str], \n    color: Color | ColorPalette = ColorPalette.DEFAULT, \n    instance_random_color: bool = False,\n    draw_bbox: bool = True,\n) -> np.ndarray:\n    '''\n    Annotate the image with the detection results. ",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_result_slow_caption",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def vis_result_slow_caption(image, masks, boxes_filt, pred_phrases, caption, text_prompt):\n    '''\n    Annotate the image with detection results, together with captions and text prompts.\n    This function is very slow, but the output is more readable.\n    '''\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)\n    for mask in masks:\n        show_mask(mask, plt.gca(), random_color=True)\n    for box, label in zip(boxes_filt, pred_phrases):",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "vis_sam_mask",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def vis_sam_mask(anns):\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n    img[:,:,3] = 0\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        color_mask = np.concatenate([np.random.random(3), [0.35]])\n        img[m] = color_mask\n    return img\ndef poses2lineset(poses, color=[0, 0, 1]):",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "poses2lineset",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def poses2lineset(poses, color=[0, 0, 1]):\n    '''\n    Create a open3d line set from a batch of poses\n    poses: (N, 4, 4)\n    color: (3,)\n    '''\n    N = poses.shape[0]\n    lineset = o3d.geometry.LineSet()\n    lineset.points = o3d.utility.Vector3dVector(poses[:, :3, 3])\n    lineset.lines = o3d.utility.Vector2iVector(",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "create_camera_frustum",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def create_camera_frustum(\n    camera_pose, width=1, height=1, z_near=0.5, z_far=1, color=[0, 0, 1]\n):\n    K = np.array([[z_near, 0, 0], [0, z_near, 0], [0, 0, z_near + z_far]])\n    points = np.array(\n        [\n            [-width / 2, -height / 2, z_near],\n            [width / 2, -height / 2, z_near],\n            [width / 2, height / 2, z_near],\n            [-width / 2, height / 2, z_near],",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "better_camera_frustum",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def better_camera_frustum(camera_pose, img_h, img_w, scale=3.0, color=[0, 0, 1]):\n    # Convert camera pose tensor to numpy array\n    if isinstance(camera_pose, torch.Tensor):\n        camera_pose = camera_pose.numpy()\n    # Define near and far distance (adjust these as needed)\n    near = scale * 0.1\n    far = scale * 1.0\n    # Define frustum dimensions at the near plane (replace with appropriate values)\n    frustum_h = near\n    frustum_w = frustum_h * img_w / img_h  # Set frustum width based on its height and the image aspect ratio",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "align_vector_to_another",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def align_vector_to_another(a=np.array([0, 0, 1]), b=np.array([1, 0, 0])):\n    \"\"\"\n    Aligns vector a to vector b with axis angle rotation\n    \"\"\"\n    if np.array_equal(a, b):\n        return None, None\n    axis_ = np.cross(a, b)\n    axis_ = axis_ / np.linalg.norm(axis_)\n    angle = np.arccos(np.dot(a, b))\n    return axis_, angle",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "normalized",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def normalized(a, axis=-1, order=2):\n    \"\"\"Normalizes a numpy array of points\"\"\"\n    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n    l2[l2 == 0] = 1\n    return a / np.expand_dims(l2, axis), l2\ndef save_video_detections(exp_out_path, save_path=None, fps=30):\n    '''\n    Save the detections in the folder as a video\n    '''\n    if save_path is None:",
        "detail": "utils.vis",
        "documentation": {}
    },
    {
        "label": "save_video_detections",
        "kind": 2,
        "importPath": "utils.vis",
        "description": "utils.vis",
        "peekOfCode": "def save_video_detections(exp_out_path, save_path=None, fps=30):\n    '''\n    Save the detections in the folder as a video\n    '''\n    if save_path is None:\n        save_path = exp_out_path / \"vis_video.mp4\"\n    # Get the list of images\n    image_files = list((exp_out_path / \"vis\").glob(\"*.jpg\"))\n    image_files.sort()\n    # Read the first image to get the size",
        "detail": "utils.vis",
        "documentation": {}
    }
]