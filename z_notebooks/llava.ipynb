{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'conceptgraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CLIPVisionModel\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconceptgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLaVaNeXTChat\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'conceptgraph'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LLAVA_PYTHON_PATH = \"/home/kev/packages/LLaVA-NeXT\"\n",
    "sys.path.append(LLAVA_PYTHON_PATH)\n",
    "\n",
    "CG_PYTHON_PATH = \"/home/kev/repos/concept-graphs\"\n",
    "sys.path.append(CG_PYTHON_PATH)\n",
    "\n",
    "import llava.conversation \n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import (\n",
    "get_model_name_from_path,\n",
    "process_images,\n",
    "tokenizer_image_token,\n",
    ")\n",
    "from llava.constants import (\n",
    "IMAGE_TOKEN_INDEX,\n",
    "DEFAULT_IMAGE_TOKEN,\n",
    "DEFAULT_IM_START_TOKEN,\n",
    "DEFAULT_IM_END_TOKEN,\n",
    "IGNORE_INDEX,\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "from transformers import CLIPVisionModel\n",
    "\n",
    "from conceptgraph.llava.llava_model import LLaVaNeXTChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLaVA model: /home/kev/pretrained_models/llama3-llava-next-8b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision tower: openai/clip-vit-large-patch14-336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f23d5a0895a46a092a8f0b24ac3a047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Class: LlavaLlamaForCausalLM\n",
      "LLaVA chat initialized...\n"
     ]
    }
   ],
   "source": [
    "pretrained = \"/home/kev/pretrained_models/llama3-llava-next-8b\"\n",
    "model_name = \"llava_llama3\"\n",
    "device = \"cuda\"\n",
    "conv_template = (\n",
    "    \"llava_llama_3\"  # Make sure you use correct chat template for different models\n",
    ")\n",
    "# device_map = \"auto\"\n",
    "device_map = 0\n",
    "\n",
    "disable_torch_init()\n",
    "tokenizer, model, image_processor, max_length = load_pretrained_model(\n",
    "    pretrained,\n",
    "    None,\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    load_8bit=True,\n",
    "    attn_implementation=None,\n",
    ")  # Add any other thing you want to pass in llava_model_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA chat initialized...\n"
     ]
    }
   ],
   "source": [
    "chat = LLaVaNeXTChat(\n",
    "    model,\n",
    "    image_processor=image_processor,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    conv_template=conv_template,\n",
    ")\n",
    "chat.model.eval()\n",
    "chat.model.tie_weights()\n",
    "\n",
    "print(\"LLaVA chat initialized...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 336, 336])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LLaVaNeXTChat' object has no attribute 'compute_image_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39mimage_processor\u001b[38;5;241m.\u001b[39mpreprocess(image, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m feat \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_image_features\u001b[49m(image_tensor)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LLaVaNeXTChat' object has no attribute 'compute_image_features'"
     ]
    }
   ],
   "source": [
    "\n",
    "image = Image.open(\"/home/kev/repos/concept-graphs/scannet.jpeg\").convert(\"RGB\")\n",
    "image_tensor = chat.image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "print(image_tensor.shape)\n",
    "\n",
    "feat = chat.compute_image_features(image_tensor)\n",
    "\n",
    "# prompt = \"Tell me a joke\"\n",
    "# output = chat(prompt=prompt, img=None)\n",
    "# print(output)\n",
    "\n",
    "# prompt = \"Explain it to me in more detail\"\n",
    "# output = chat(prompt=prompt, img=None)\n",
    "# print(output)\n",
    "\n",
    "# prompt = \"What did you say?\"\n",
    "# output = chat(prompt=prompt, img=None)\n",
    "# print(output)\n",
    "\n",
    "# prompt = \"What did I ask you before and what did you reply? Summarize without giving me the exact words\"\n",
    "# output = chat(prompt=prompt, img=None)\n",
    "# print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
